{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOc2S6iP/83/n4CkIzooJS2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmk4444/System_engineering/blob/main/System_eng_Part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!touch requirements.txt\n",
        "!echo python-dotenv >> requirements.txt\n",
        "!echo openai >> requirements.txt\n",
        "!echo streamlit >> requirements.txt\n",
        "!echo google-generativeai >> requirements.txt\n",
        "!echo anthropic >> requirements.txt\n",
        "!echo cohere >> requirements.txt\n",
        "!echo replicate >> requirements.txt"
      ],
      "metadata": {
        "id": "3dONcerXb_fE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "jFBFK_h2cBQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install bitsandbytes\n",
        "!pip install accelerate\n",
        "!pip install torch"
      ],
      "metadata": {
        "id": "MwJzUJ-latTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import pipeline\n",
        "\n",
        "login(token = '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')"
      ],
      "metadata": {
        "id": "uxTWpKOUauh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app_v1.py\n",
        "import streamlit as st\n",
        "import replicate\n",
        "import os\n",
        "\n",
        "# App title\n",
        "st.set_page_config(page_title=\"ü¶ôüí¨ Sistem M√ºhendisliƒüi Giri≈ü Chatbot\")\n",
        "\n",
        "# Replicate Credentials\n",
        "with st.sidebar:\n",
        "\n",
        "    st.title('ü¶ôüí¨ Sistem M√ºhendisliƒüi Giri≈ü')\n",
        "    \"\"\"\n",
        "    if 'REPLICATE_API_TOKEN' in st.secrets:\n",
        "        st.success('API key already provided!', icon='‚úÖ')\n",
        "        replicate_api = st.secrets['REPLICATE_API_TOKEN']\n",
        "    else:\n",
        "        replicate_api = st.text_input('Enter Replicate API token:', type='password')\n",
        "        if not (replicate_api.startswith('r8_') and len(replicate_api)==40):\n",
        "            st.warning('Please enter your credentials!', icon='‚ö†Ô∏è')\n",
        "        else:\n",
        "            st.success('Proceed to entering your prompt message!', icon='üëâ')\n",
        "    os.environ['REPLICATE_API_TOKEN'] = replicate_api\n",
        "    \"\"\"\n",
        "\n",
        "    st.subheader('Models and parameters')\n",
        "    \"\"\"\n",
        "    selected_model = st.sidebar.selectbox('Choose a Llama2 model', ['Llama2-7B', 'Llama2-13B'], key='selected_model')\n",
        "    if selected_model == 'Llama2-7B':\n",
        "        llm = 'a16z-infra/llama7b-v2-chat:4f0a4744c7295c024a1de15e1a63c880d3da035fa1f49bfd344fe076074c8eea'\n",
        "    elif selected_model == 'Llama2-13B':\n",
        "        llm = 'a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5'\n",
        "    \"\"\"\n",
        "    temperature = st.sidebar.slider('temperature', min_value=0.01, max_value=5.0, value=0.1, step=0.01)\n",
        "    top_p = st.sidebar.slider('top_p', min_value=0.01, max_value=1.0, value=0.9, step=0.01)\n",
        "    max_length = st.sidebar.slider('max_length', min_value=32, max_value=128, value=120, step=8)\n",
        "    st.markdown('üìñ Learn how to build this app in this [blog](https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/)!')\n",
        "\n",
        "# Store LLM generated responses\n",
        "if \"messages\" not in st.session_state.keys():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "\n",
        "# Display or clear chat messages\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.write(message[\"content\"])\n",
        "\n",
        "def clear_chat_history():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "st.sidebar.button('Clear Chat History', on_click=clear_chat_history)\n",
        "\n",
        "# Function for generating LLaMA2 response. Refactored from https://github.com/a16z-infra/llama2-chatbot\n",
        "def generate_llama2_response(prompt_input):\n",
        "    string_dialogue = \"You are a helpful assistant. You do not respond as 'User' or pretend to be 'User'. You only respond once as 'Assistant'.\"\n",
        "    for dict_message in st.session_state.messages:\n",
        "        if dict_message[\"role\"] == \"user\":\n",
        "            string_dialogue += \"User: \" + dict_message[\"content\"] + \"\\n\\n\"\n",
        "        else:\n",
        "            string_dialogue += \"Assistant: \" + dict_message[\"content\"] + \"\\n\\n\"\n",
        "    output = replicate.run('a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5',\n",
        "                           input={\"prompt\": f\"{string_dialogue} {prompt_input} Assistant: \",\n",
        "                                  \"temperature\":temperature, \"top_p\":top_p, \"max_length\":max_length, \"repetition_penalty\":1})\n",
        "    return output\n",
        "\n",
        "# User-provided prompt\n",
        "if prompt := st.chat_input(\"Mesajƒ±nƒ±zƒ± Giriniz:\"):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.write(prompt)\n",
        "\n",
        "# Generate a new response if last message is not from assistant\n",
        "if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            response = generate_llama2_response(prompt)\n",
        "            placeholder = st.empty()\n",
        "            full_response = ''\n",
        "            for item in response:\n",
        "                full_response += item\n",
        "                placeholder.markdown(full_response)\n",
        "            placeholder.markdown(full_response)\n",
        "    message = {\"role\": \"assistant\", \"content\": full_response}\n",
        "    st.session_state.messages.append(message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tJwH45tavAV",
        "outputId": "72d19657-af56-4d37-971e-05f976927143"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!streamlit run /content/app_v1.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "lidd5YYqcJrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app_v2.py\n",
        "import streamlit as st\n",
        "import replicate\n",
        "import os\n",
        "\n",
        "\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    # The quantization line\n",
        "    model_kwargs={\"torch_dtype\": torch.bfloat16}\n",
        ")\n",
        "\n",
        "\n",
        "# App title\n",
        "st.set_page_config(page_title=\"ü¶ôüí¨ Sistem M√ºhendisliƒüi Giri≈ü Chatbot\")\n",
        "\n",
        "# Replicate Credentials\n",
        "with st.sidebar:\n",
        "\n",
        "    st.title('ü¶ôüí¨ Sistem M√ºhendisliƒüi Giri≈ü')\n",
        "    \"\"\"\n",
        "    if 'REPLICATE_API_TOKEN' in st.secrets:\n",
        "        st.success('API key already provided!', icon='‚úÖ')\n",
        "        replicate_api = st.secrets['REPLICATE_API_TOKEN']\n",
        "    else:\n",
        "        replicate_api = st.text_input('Enter Replicate API token:', type='password')\n",
        "        if not (replicate_api.startswith('r8_') and len(replicate_api)==40):\n",
        "            st.warning('Please enter your credentials!', icon='‚ö†Ô∏è')\n",
        "        else:\n",
        "            st.success('Proceed to entering your prompt message!', icon='üëâ')\n",
        "    os.environ['REPLICATE_API_TOKEN'] = replicate_api\n",
        "    \"\"\"\n",
        "\n",
        "    st.subheader('Models and parameters')\n",
        "    \"\"\"\n",
        "    selected_model = st.sidebar.selectbox('Choose a Llama2 model', ['Llama2-7B', 'Llama2-13B'], key='selected_model')\n",
        "    if selected_model == 'Llama2-7B':\n",
        "        llm = 'a16z-infra/llama7b-v2-chat:4f0a4744c7295c024a1de15e1a63c880d3da035fa1f49bfd344fe076074c8eea'\n",
        "    elif selected_model == 'Llama2-13B':\n",
        "        llm = 'a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5'\n",
        "    \"\"\"\n",
        "    temperature = st.sidebar.slider('temperature', min_value=0.01, max_value=5.0, value=0.1, step=0.01)\n",
        "    top_p = st.sidebar.slider('top_p', min_value=0.01, max_value=1.0, value=0.9, step=0.01)\n",
        "    max_length = st.sidebar.slider('max_length', min_value=32, max_value=128, value=120, step=8)\n",
        "    st.markdown('üìñ Learn how to build this app in this [blog](https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/)!')\n",
        "\n",
        "# Store LLM generated responses\n",
        "if \"messages\" not in st.session_state.keys():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "\n",
        "# Display or clear chat messages\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.write(message[\"content\"])\n",
        "\n",
        "def clear_chat_history():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "st.sidebar.button('Clear Chat History', on_click=clear_chat_history)\n",
        "\n",
        "# Function for generating LLaMA2 response. Refactored from https://github.com/a16z-infra/llama2-chatbot\n",
        "def generate_llama2_response(prompt_input):\n",
        "    string_dialogue = \"You are a helpful assistant. You do not respond as 'User' or pretend to be 'User'. You only respond once as 'Assistant'.\"\n",
        "    for dict_message in st.session_state.messages:\n",
        "        if dict_message[\"role\"] == \"user\":\n",
        "            string_dialogue += \"User: \" + dict_message[\"content\"] + \"\\n\\n\"\n",
        "        else:\n",
        "            string_dialogue += \"Assistant: \" + dict_message[\"content\"] + \"\\n\\n\"\n",
        "\n",
        "    messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Answer questions\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "    ]\n",
        "\n",
        "    prompt = pipeline.tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    terminators = [\n",
        "    pipeline.tokenizer.eos_token_id,\n",
        "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "    ]\n",
        "\n",
        "    outputs = pipeline(\n",
        "    prompt,\n",
        "    max_new_tokens=256,\n",
        "    eos_token_id=terminators,\n",
        "    do_sample=True,\n",
        "    temperature=temperature,\n",
        "    top_p=top_p,\n",
        "    )\n",
        "\n",
        "    return outputs[0][\"generated_text\"]\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    output = replicate.run('a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5',\n",
        "                           input={\"prompt\": f\"{string_dialogue} {prompt_input} Assistant: \",\n",
        "                                  \"temperature\":temperature, \"top_p\":top_p, \"max_length\":max_length, \"repetition_penalty\":1})\n",
        "    \"\"\"\n",
        "\n",
        "# User-provided prompt\n",
        "if prompt := st.chat_input(\"Mesajƒ±nƒ±zƒ± Giriniz:\"):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.write(prompt)\n",
        "\n",
        "# Generate a new response if last message is not from assistant\n",
        "if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            response = generate_llama2_response(prompt)\n",
        "            placeholder = st.empty()\n",
        "            full_response = ''\n",
        "            for item in response:\n",
        "                full_response += item\n",
        "                placeholder.markdown(full_response)\n",
        "            placeholder.markdown(full_response)\n",
        "    message = {\"role\": \"assistant\", \"content\": full_response}\n",
        "    st.session_state.messages.append(message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IJZMHNwqe0R",
        "outputId": "de4dcbfe-0dae-4f11-c55e-92ec8764b857"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app_v2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!streamlit run /content/app_v2.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "aZBo8xgnqjfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app_v3.py\n",
        "import streamlit as st\n",
        "import replicate\n",
        "import os\n",
        "\n",
        "\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    # The quantization line\n",
        "    model_kwargs={\"torch_dtype\": torch.bfloat16}\n",
        ")\n",
        "\n",
        "\n",
        "# App title\n",
        "st.set_page_config(page_title=\"ü¶ôüí¨ Sistem M√ºhendisliƒüi Giri≈ü Chatbot\")\n",
        "\n",
        "# Replicate Credentials\n",
        "with st.sidebar:\n",
        "\n",
        "    st.title('ü¶ôüí¨ Sistem M√ºhendisliƒüi Giri≈ü')\n",
        "\n",
        "    st.subheader('Models and parameters')\n",
        "\n",
        "    temperature = st.sidebar.slider('temperature', min_value=0.01, max_value=5.0, value=0.1, step=0.01)\n",
        "    top_p = st.sidebar.slider('top_p', min_value=0.01, max_value=1.0, value=0.9, step=0.01)\n",
        "    max_length = st.sidebar.slider('max_length', min_value=32, max_value=128, value=120, step=8)\n",
        "    st.markdown('üìñ Learn how to build this app in this [blog](https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/)!')\n",
        "\n",
        "# Store LLM generated responses\n",
        "if \"messages\" not in st.session_state.keys():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "\n",
        "# Display or clear chat messages\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.write(message[\"content\"])\n",
        "\n",
        "def clear_chat_history():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "st.sidebar.button('Clear Chat History', on_click=clear_chat_history)\n",
        "\n",
        "# Function for generating LLaMA2 response. Refactored from https://github.com/a16z-infra/llama2-chatbot\n",
        "def generate_llama2_response(prompt_input):\n",
        "    string_dialogue = \"You are a helpful assistant. You do not respond as 'User' or pretend to be 'User'. You only respond once as 'Assistant'.\"\n",
        "    for dict_message in st.session_state.messages:\n",
        "        if dict_message[\"role\"] == \"user\":\n",
        "            string_dialogue += \"User: \" + dict_message[\"content\"] + \"\\n\\n\"\n",
        "        else:\n",
        "            string_dialogue += \"Assistant: \" + dict_message[\"content\"] + \"\\n\\n\"\n",
        "\n",
        "    messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Answer questions\"},\n",
        "    {\"role\": \"user\", \"content\": string_dialogue},\n",
        "    ]\n",
        "\n",
        "    prompt = pipeline.tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    terminators = [\n",
        "    pipeline.tokenizer.eos_token_id,\n",
        "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "    ]\n",
        "\n",
        "    outputs = pipeline(\n",
        "    prompt,\n",
        "    max_new_tokens=256,\n",
        "    eos_token_id=terminators,\n",
        "    do_sample=True,\n",
        "    temperature=temperature,\n",
        "    top_p=top_p,\n",
        "    )\n",
        "\n",
        "    return outputs[0][\"generated_text\"]\n",
        "\n",
        "\n",
        "\n",
        "# User-provided prompt\n",
        "if prompt := st.chat_input(\"Mesajƒ±nƒ±zƒ± Giriniz:\"):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.write(prompt)\n",
        "\n",
        "# Generate a new response if last message is not from assistant\n",
        "if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            response = generate_llama2_response(prompt)\n",
        "            placeholder = st.empty()\n",
        "            full_response = ''\n",
        "            for item in response:\n",
        "                full_response += item\n",
        "                placeholder.markdown(full_response)\n",
        "            placeholder.markdown(full_response)\n",
        "    message = {\"role\": \"assistant\", \"content\": full_response}\n",
        "    st.session_state.messages.append(message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88NDPMPg9-zZ",
        "outputId": "da504f44-fa8d-4c78-ef0e-b0639260d1f4"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app_v3.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!streamlit run /content/app_v3.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "-Kas6vmO-Byp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app_v4.py\n",
        "import streamlit as st\n",
        "import replicate\n",
        "import os\n",
        "\n",
        "\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    # The quantization line\n",
        "    model_kwargs={\"torch_dtype\": torch.bfloat16}\n",
        ")\n",
        "\n",
        "\n",
        "# App title\n",
        "st.set_page_config(page_title=\"ü¶ôüí¨ Sistem M√ºhendisliƒüi Giri≈ü Chatbot\")\n",
        "\n",
        "# Replicate Credentials\n",
        "with st.sidebar:\n",
        "\n",
        "    st.title('ü¶ôüí¨ Sistem M√ºhendisliƒüi Giri≈ü')\n",
        "\n",
        "    st.subheader('Models and parameters')\n",
        "\n",
        "    temperature = st.sidebar.slider('temperature', min_value=0.01, max_value=5.0, value=0.1, step=0.01)\n",
        "    top_p = st.sidebar.slider('top_p', min_value=0.01, max_value=1.0, value=0.9, step=0.01)\n",
        "    max_length = st.sidebar.slider('max_length', min_value=32, max_value=128, value=120, step=8)\n",
        "    st.markdown('üìñ Learn how to build this app in this [blog](https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/)!')\n",
        "\n",
        "# Store LLM generated responses\n",
        "if \"messages\" not in st.session_state.keys():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "\n",
        "# Display or clear chat messages\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.write(message[\"content\"])\n",
        "\n",
        "def clear_chat_history():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "st.sidebar.button('Clear Chat History', on_click=clear_chat_history)\n",
        "\n",
        "# Function for generating LLaMA2 response. Refactored from https://github.com/a16z-infra/llama2-chatbot\n",
        "def generate_llama2_response(prompt_input):\n",
        "    string_dialogue = \"You are a helpful assistant. You do not respond as 'User' or pretend to be 'User'. You only respond once as 'Assistant'.\"\n",
        "    for dict_message in st.session_state.messages:\n",
        "        if dict_message[\"role\"] == \"user\":\n",
        "            string_dialogue += \"User: \" + dict_message[\"content\"] + \"\\n\\n\"\n",
        "        else:\n",
        "            string_dialogue += \"Assistant: \" + dict_message[\"content\"] + \"\\n\\n\"\n",
        "\n",
        "    messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Past conservation is here: \" + string_dialogue + \"When you answer user's question, you should consider old conversation and prompt.\" },\n",
        "    {\"role\": \"user\", \"content\": prompt_input},\n",
        "    ]\n",
        "\n",
        "    prompt = pipeline.tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    terminators = [\n",
        "    pipeline.tokenizer.eos_token_id,\n",
        "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "    ]\n",
        "\n",
        "    outputs = pipeline(\n",
        "    prompt,\n",
        "    max_new_tokens=256,\n",
        "    eos_token_id=terminators,\n",
        "    do_sample=True,\n",
        "    temperature=temperature,\n",
        "    top_p=top_p,\n",
        "    )\n",
        "\n",
        "    return outputs[0][\"generated_text\"]\n",
        "\n",
        "\n",
        "\n",
        "# User-provided prompt\n",
        "if prompt := st.chat_input(\"Mesajƒ±nƒ±zƒ± Giriniz:\"):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.write(prompt)\n",
        "\n",
        "# Generate a new response if last message is not from assistant\n",
        "if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            response = generate_llama2_response(prompt)\n",
        "            placeholder = st.empty()\n",
        "            full_response = ''\n",
        "            for item in response:\n",
        "                full_response += item\n",
        "                placeholder.markdown(full_response)\n",
        "            placeholder.markdown(full_response)\n",
        "    message = {\"role\": \"assistant\", \"content\": full_response}\n",
        "    st.session_state.messages.append(message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfZTrxy9EROZ",
        "outputId": "2c33a2e0-01e8-4c97-9d32-8028ddcf041e"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app_v4.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!streamlit run /content/app_v4.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "VDxsGOAfEUEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app_v5.py\n",
        "import streamlit as st\n",
        "import replicate\n",
        "import os\n",
        "\n",
        "\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    # The quantization line\n",
        "    model_kwargs={\"torch_dtype\": torch.bfloat16}\n",
        ")\n",
        "\n",
        "\n",
        "# App title\n",
        "st.set_page_config(page_title=\"ü¶ôüí¨ Sistem M√ºhendisliƒüi Giri≈ü Chatbot\")\n",
        "\n",
        "# Replicate Credentials\n",
        "with st.sidebar:\n",
        "\n",
        "    st.title('ü¶ôüí¨ Sistem M√ºhendisliƒüi Giri≈ü')\n",
        "\n",
        "    st.subheader('Models and parameters')\n",
        "\n",
        "    temperature = st.sidebar.slider('temperature', min_value=0.01, max_value=5.0, value=0.1, step=0.01)\n",
        "    top_p = st.sidebar.slider('top_p', min_value=0.01, max_value=1.0, value=0.9, step=0.01)\n",
        "    max_length = st.sidebar.slider('max_length', min_value=32, max_value=128, value=120, step=8)\n",
        "    st.markdown('üìñ Learn how to build this app in this [blog](https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/)!')\n",
        "\n",
        "# Store LLM generated responses\n",
        "if \"messages\" not in st.session_state.keys():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "\n",
        "# Display or clear chat messages\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.write(message[\"content\"])\n",
        "\n",
        "def clear_chat_history():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "st.sidebar.button('Clear Chat History', on_click=clear_chat_history)\n",
        "\n",
        "# Function for generating LLaMA2 response. Refactored from https://github.com/a16z-infra/llama2-chatbot\n",
        "def generate_llama2_response(prompt_input):\n",
        "    string_dialogue = \"You are a helpful assistant. You do not respond as 'User' or pretend to be 'User'. You only respond once as 'Assistant'.\"\n",
        "    for dict_message in st.session_state.messages:\n",
        "        if dict_message[\"role\"] == \"user\":\n",
        "            string_dialogue += \"User: \" + dict_message[\"content\"] + \"\\n\\n\"\n",
        "        else:\n",
        "            string_dialogue += \"Assistant: \" + dict_message[\"content\"] + \"\\n\\n\"\n",
        "\n",
        "    messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Past conservation is here: \" + string_dialogue + \"When you answer user's question, you should consider old conversation and prompt.\" },\n",
        "    {\"role\": \"user\", \"content\": prompt_input},\n",
        "    ]\n",
        "\n",
        "    prompt = pipeline.tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    terminators = [\n",
        "    pipeline.tokenizer.eos_token_id,\n",
        "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "    ]\n",
        "\n",
        "    outputs = pipeline(\n",
        "    prompt,\n",
        "    max_new_tokens=256,\n",
        "    #eos_token_id=terminators,\n",
        "    do_sample=True,\n",
        "    temperature=temperature,\n",
        "    top_p=top_p,\n",
        "    )\n",
        "\n",
        "    return outputs[0][\"generated_text\"]\n",
        "\n",
        "\n",
        "\n",
        "# User-provided prompt\n",
        "if prompt := st.chat_input(\"Mesajƒ±nƒ±zƒ± Giriniz:\"):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.write(prompt)\n",
        "\n",
        "# Generate a new response if last message is not from assistant\n",
        "if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            response = generate_llama2_response(prompt)\n",
        "            placeholder = st.empty()\n",
        "            full_response = ''\n",
        "            for item in response:\n",
        "                full_response += item\n",
        "                placeholder.markdown(full_response)\n",
        "            placeholder.markdown(full_response)\n",
        "    message = {\"role\": \"assistant\", \"content\": full_response}\n",
        "    st.session_state.messages.append(message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6Vo9upAO2GZ",
        "outputId": "89e844a0-8e1b-4567-8ab2-75d41583902e"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app_v5.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!streamlit run /content/app_v5.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "M7GZR0H4O3uR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app_v6.py\n",
        "import streamlit as st\n",
        "import replicate\n",
        "import os\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "# use quantization to lower GPU usage\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "terminators = [\n",
        "    tokenizer.eos_token_id,\n",
        "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "\n",
        "\n",
        "# App title\n",
        "st.set_page_config(page_title=\"ü¶ôüí¨ Sistem M√ºhendisliƒüi Giri≈ü Chatbot\")\n",
        "\n",
        "# Replicate Credentials\n",
        "with st.sidebar:\n",
        "\n",
        "    st.title('ü¶ôüí¨ Sistem M√ºhendisliƒüi Giri≈ü')\n",
        "\n",
        "    st.subheader('Models and parameters')\n",
        "\n",
        "    temperature = st.sidebar.slider('temperature', min_value=0.01, max_value=5.0, value=0.1, step=0.01)\n",
        "    top_p = st.sidebar.slider('top_p', min_value=0.01, max_value=1.0, value=0.9, step=0.01)\n",
        "    max_length = st.sidebar.slider('max_length', min_value=32, max_value=128, value=120, step=8)\n",
        "    st.markdown('üìñ Learn how to build this app in this [blog](https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/)!')\n",
        "\n",
        "# Store LLM generated responses\n",
        "if \"messages\" not in st.session_state.keys():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "\n",
        "# Display or clear chat messages\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.write(message[\"content\"])\n",
        "\n",
        "def clear_chat_history():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "st.sidebar.button('Clear Chat History', on_click=clear_chat_history)\n",
        "\n",
        "# Function for generating LLaMA2 response. Refactored from https://github.com/a16z-infra/llama2-chatbot\n",
        "def generate_llama2_response(prompt_input):\n",
        "    string_dialogue = \"You are a helpful assistant. You do not respond as 'User' or pretend to be 'User'. You only respond once as 'Assistant'.\"\n",
        "    for dict_message in st.session_state.messages:\n",
        "        if dict_message[\"role\"] == \"user\":\n",
        "            string_dialogue += \"User: \" + dict_message[\"content\"] + \"\\n\\n\"\n",
        "        else:\n",
        "            string_dialogue += \"Assistant: \" + dict_message[\"content\"] + \"\\n\\n\"\n",
        "\n",
        "\n",
        "    SYS_PROMPT = \"\"\"You are an assistant for answering questions.\n",
        "    You are given the extracted parts of a long document and a question. Provide a conversational answer.\n",
        "    If you don't know the answer, just say \"I do not know.\" Don't make up an answer.\"\"\"\n",
        "\n",
        "\n",
        "    PROMPT = f\"Question:{prompt_input}\\nContext:\"\n",
        "    \"\"\"\n",
        "    for idx in range(k) :\n",
        "        PROMPT+= f\"{retrieved_documents['text'][idx]}\\n\"\n",
        "    \"\"\"\n",
        "\n",
        "    PROMPT += f\"Malatya; Doƒüu Anadolu B√∂lgesi'nin ve Fƒ±rat Nehri'nin stratejik konumunda yer alan bir yerle≈üim yeridir. Bu konumu nedeniyle ilk yerle≈ümeler M√ñ 6000'lere gitmektedir.[4] Ayrƒ±ca b√∂lge √∂nemli ticaret yollarƒ± √ºzerinden olduƒüundan dolayƒ± s√ºrekli sava≈üƒ±lmƒ±≈ü bir yerdir. S√ºrekli iki devlet arasƒ±nda √ßeki≈ümelere neden olmu≈ütur.\"\n",
        "\n",
        "    messages = [{\"role\":\"system\",\"content\":SYS_PROMPT},{\"role\":\"user\",\"content\":PROMPT}]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "      input_ids,\n",
        "      max_new_tokens=1024,\n",
        "      eos_token_id=terminators,\n",
        "      do_sample=True,\n",
        "      temperature=0.6,\n",
        "      top_p=0.9,\n",
        "    )\n",
        "\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "# User-provided prompt\n",
        "if prompt := st.chat_input(\"Mesajƒ±nƒ±zƒ± Giriniz:\"):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.write(prompt)\n",
        "\n",
        "# Generate a new response if last message is not from assistant\n",
        "if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            response = generate_llama2_response(prompt)\n",
        "            placeholder = st.empty()\n",
        "            full_response = ''\n",
        "            for item in response:\n",
        "                full_response += item\n",
        "                placeholder.markdown(full_response)\n",
        "            placeholder.markdown(full_response)\n",
        "    message = {\"role\": \"assistant\", \"content\": full_response}\n",
        "    st.session_state.messages.append(message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsLrKMl6Qg53",
        "outputId": "d9136cd3-30d1-46e2-a1bb-d743500c6eb2"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app_v6.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!streamlit run /content/app_v6.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "bQWmn2zzUTXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!touch requirements.txt\n",
        "!echo langchain >> requirements.txt\n",
        "!echo langchain-openai >> requirements.txt\n",
        "!echo langchain-google-genai >> requirements.txt\n",
        "!echo langchain_experimental >> requirements.txt\n",
        "!echo openai >> requirements.txt\n",
        "!echo anthropic >> requirements.txt\n",
        "!echo cohere >> requirements.txt\n",
        "!echo streamlit >> requirements.txt\n",
        "!echo python-dotenv >> requirements.txt\n",
        "!echo beautifulsoup4 >> requirements.txt\n",
        "!echo faiss-cpu >> requirements.txt\n",
        "!echo pypdf >> requirements.txt\n",
        "!echo unstructured >> requirements.txt\n",
        "!echo networkx >> requirements.txt\n",
        "!echo openpyxl >> requirements.txt\n",
        "!echo rapidocr-onnxruntime >> requirements.txt"
      ],
      "metadata": {
        "id": "GM4xR3LjnMZN"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "iTnj4pw0nRcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app_v7.py\n",
        "import streamlit as st\n",
        "import replicate\n",
        "import os\n",
        "#from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import FAISS # it is vector database. we will use FAISS database. It is created by Facebook.\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import CohereEmbeddings\n",
        "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "\n",
        "\n",
        "embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
        "    api_key='----',\n",
        "    model_name=\"sentence-transformers/all-MiniLM-l6-v2\"\n",
        ")\n",
        "\n",
        "def rag_with_url(prompt):\n",
        "  from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "  target_url=\"https://kpmg.com/tr/tr/home/gorusler/2023/12/uretken-yapay-zeka-uygulamalarinin-kurumsallasma-yaklasimi.html\"\n",
        "\n",
        "  loader = WebBaseLoader(target_url) # we create a loader object and we assign to this object.\n",
        "  raw_documents = loader.load() # we load document from url. (type is document)#load url\n",
        "\n",
        "\n",
        "  #create chunk using splitters\n",
        "  #below code can run without any parameters but to reach the best answer you should try different parameters.\n",
        "  #  text_splitter = RecursiveCharacterTextSplitter()\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size=1000,\n",
        "      chunk_overlap=0,\n",
        "      length_function=len # it provides that how to estimate lenght of chunk. we will use len because of python list rule.\n",
        "  )\n",
        "\n",
        "  # we run split_documents method which is inside of text_splitter and save to splitted_document\n",
        "  splitted_document = text_splitter.split_documents(raw_documents)\n",
        "\n",
        "  # we need to save splited documents in vector database.\n",
        "  # FAISS create vectors from our documents using our embedding model and save vectordatabase.\n",
        "  vectorstore = FAISS.from_documents(splitted_document, embeddings)\n",
        "  # we reach as_retriever method in vectorstore. We convert vector store into retriever.\n",
        "  # retriever won't retrieve all vectors, it retrieves vector considering context similarity among vector and prompt.\n",
        "  #mostly, similarity is estimated by cosine similarity.\n",
        "  retriever = vectorstore.as_retriever()\n",
        "  relevant_documents = retriever.get_relevant_documents(prompt) # it retrieves the most similar document accordingly our prompt. It can be 3,4 or more documents. Default value is 4.\n",
        "\n",
        "  context_data =\"\"\n",
        "\n",
        "  #Also, we use build function instead of the below code.\n",
        "  for document in relevant_documents:\n",
        "    context_data = context_data + \" \" + document.page_content # document has two features which are metada and page_content.\n",
        "    #because the main data is in context_data.\n",
        "  final_prompt = f\"\"\"≈û√∂yle bir sorum var: {prompt}\n",
        "  Bu soruyu yanƒ±tlamak i√ßin elimizde ≈üu bilgiler var: {context_data} .\n",
        "  Bu sorunun yanƒ±tƒ±nƒ± vermek i√ßin yalnƒ±zca sana burada verdiƒüim eldeki bilgileri kullan. Bunlarƒ±n dƒ±≈üƒ±na asla √ßƒ±kma.\n",
        "  \"\"\"\n",
        "\n",
        "  return final_prompt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "# use quantization to lower GPU usage\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "terminators = [\n",
        "    tokenizer.eos_token_id,\n",
        "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "\n",
        "\n",
        "# App title\n",
        "st.set_page_config(page_title=\"ü¶ôüí¨ Sistem M√ºhendisliƒüi Giri≈ü Chatbot\")\n",
        "\n",
        "# Replicate Credentials\n",
        "with st.sidebar:\n",
        "\n",
        "    st.title('ü¶ôüí¨ Sistem M√ºhendisliƒüi Giri≈ü')\n",
        "\n",
        "    st.subheader('Models and parameters')\n",
        "\n",
        "    temperature = st.sidebar.slider('temperature', min_value=0.01, max_value=5.0, value=0.1, step=0.01)\n",
        "    top_p = st.sidebar.slider('top_p', min_value=0.01, max_value=1.0, value=0.9, step=0.01)\n",
        "    max_length = st.sidebar.slider('max_length', min_value=32, max_value=128, value=120, step=8)\n",
        "    st.markdown('üìñ Learn how to build this app in this [blog](https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/)!')\n",
        "\n",
        "# Store LLM generated responses\n",
        "if \"messages\" not in st.session_state.keys():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "\n",
        "# Display or clear chat messages\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.write(message[\"content\"])\n",
        "\n",
        "def clear_chat_history():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "st.sidebar.button('Clear Chat History', on_click=clear_chat_history)\n",
        "\n",
        "# Function for generating LLaMA2 response. Refactored from https://github.com/a16z-infra/llama2-chatbot\n",
        "def generate_llama2_response(prompt_input):\n",
        "\n",
        "\n",
        "    SYS_PROMPT = \"\"\"You are an assistant for answering questions.\n",
        "    You are given the extracted parts of a long document and a question. Provide a conversational answer.\n",
        "    If you don't know the answer, just say \"I do not know.\" Don't make up an answer.\"\"\"\n",
        "\n",
        "\n",
        "    messages = [{\"role\":\"system\",\"content\":SYS_PROMPT},{\"role\":\"user\",\"content\":prompt_input}]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "      input_ids,\n",
        "      max_new_tokens=1024,\n",
        "      eos_token_id=terminators,\n",
        "      do_sample=True,\n",
        "      temperature=0.6,\n",
        "      top_p=0.9,\n",
        "    )\n",
        "\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "# User-provided prompt\n",
        "if prompt := st.chat_input(\"Mesajƒ±nƒ±zƒ± Giriniz:\"):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.write(prompt)\n",
        "\n",
        "# Generate a new response if last message is not from assistant\n",
        "if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            prompt_input=rag_with_url(prompt)\n",
        "            response = generate_llama2_response(prompt_input)\n",
        "            placeholder = st.empty()\n",
        "            full_response = ''\n",
        "            for item in response:\n",
        "                full_response += item\n",
        "                placeholder.markdown(full_response)\n",
        "            placeholder.markdown(full_response)\n",
        "    message = {\"role\": \"assistant\", \"content\": full_response}\n",
        "    st.session_state.messages.append(message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxDCB6-Ehx9i",
        "outputId": "6e1521c7-3614-4a73-fe07-65bf450d66b3"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app_v7.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!streamlit run /content/app_v7.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "txwhPjQth09O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZnPqMyZkaPO0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64611064-2cc7-4363-b3a4-69fabccd227b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import replicate\n",
        "import os\n",
        "\n",
        "# App title\n",
        "st.set_page_config(page_title=\"ü¶ôüí¨ Llama 2 Chatbot\")\n",
        "\n",
        "# Replicate Credentials\n",
        "with st.sidebar:\n",
        "    st.title('ü¶ôüí¨ Llama 2 Chatbot')\n",
        "    if 'REPLICATE_API_TOKEN' in st.secrets:\n",
        "        st.success('API key already provided!', icon='‚úÖ')\n",
        "        replicate_api = st.secrets['REPLICATE_API_TOKEN']\n",
        "    else:\n",
        "        replicate_api = st.text_input('Enter Replicate API token:', type='password')\n",
        "        if not (replicate_api.startswith('r8_') and len(replicate_api)==40):\n",
        "            st.warning('Please enter your credentials!', icon='‚ö†Ô∏è')\n",
        "        else:\n",
        "            st.success('Proceed to entering your prompt message!', icon='üëâ')\n",
        "    os.environ['REPLICATE_API_TOKEN'] = replicate_api\n",
        "\n",
        "    st.subheader('Models and parameters')\n",
        "    selected_model = st.sidebar.selectbox('Choose a Llama2 model', ['Llama2-7B', 'Llama2-13B'], key='selected_model')\n",
        "    if selected_model == 'Llama2-7B':\n",
        "        llm = 'a16z-infra/llama7b-v2-chat:4f0a4744c7295c024a1de15e1a63c880d3da035fa1f49bfd344fe076074c8eea'\n",
        "    elif selected_model == 'Llama2-13B':\n",
        "        llm = 'a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5'\n",
        "    temperature = st.sidebar.slider('temperature', min_value=0.01, max_value=5.0, value=0.1, step=0.01)\n",
        "    top_p = st.sidebar.slider('top_p', min_value=0.01, max_value=1.0, value=0.9, step=0.01)\n",
        "    max_length = st.sidebar.slider('max_length', min_value=32, max_value=128, value=120, step=8)\n",
        "    st.markdown('üìñ Learn how to build this app in this [blog](https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/)!')\n",
        "\n",
        "# Store LLM generated responses\n",
        "if \"messages\" not in st.session_state.keys():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "\n",
        "# Display or clear chat messages\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.write(message[\"content\"])\n",
        "\n",
        "def clear_chat_history():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "st.sidebar.button('Clear Chat History', on_click=clear_chat_history)\n",
        "\n",
        "# Function for generating LLaMA2 response. Refactored from https://github.com/a16z-infra/llama2-chatbot\n",
        "def generate_llama2_response(prompt_input):\n",
        "    string_dialogue = \"You are a helpful assistant. You do not respond as 'User' or pretend to be 'User'. You only respond once as 'Assistant'.\"\n",
        "    for dict_message in st.session_state.messages:\n",
        "        if dict_message[\"role\"] == \"user\":\n",
        "            string_dialogue += \"User: \" + dict_message[\"content\"] + \"\\n\\n\"\n",
        "        else:\n",
        "            string_dialogue += \"Assistant: \" + dict_message[\"content\"] + \"\\n\\n\"\n",
        "    output = replicate.run('a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5',\n",
        "                           input={\"prompt\": f\"{string_dialogue} {prompt_input} Assistant: \",\n",
        "                                  \"temperature\":temperature, \"top_p\":top_p, \"max_length\":max_length, \"repetition_penalty\":1})\n",
        "    return output\n",
        "\n",
        "# User-provided prompt\n",
        "if prompt := st.chat_input(disabled=not replicate_api):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.write(prompt)\n",
        "\n",
        "# Generate a new response if last message is not from assistant\n",
        "if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            response = generate_llama2_response(prompt)\n",
        "            placeholder = st.empty()\n",
        "            full_response = ''\n",
        "            for item in response:\n",
        "                full_response += item\n",
        "                placeholder.markdown(full_response)\n",
        "            placeholder.markdown(full_response)\n",
        "    message = {\"role\": \"assistant\", \"content\": full_response}\n",
        "    st.session_state.messages.append(message)"
      ]
    }
  ]
}