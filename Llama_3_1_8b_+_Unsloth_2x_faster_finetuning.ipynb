{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmk4444/System_engineering/blob/main/Llama_3_1_8b_%2B_Unsloth_2x_faster_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqM-T1RTzY6C"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://github.com/unslothai/unsloth?tab=readme-ov-file#-installation-instructions).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save) (eg for Llama.cpp).\n",
        "\n",
        "[NEW] Llama-3.1 8b, 70b & 405b are trained on a crazy 15 trillion tokens with 128K long context lengths!\n",
        "\n",
        "**[NEW] Try 2x faster inference in a free Colab for Llama-3.1 8b Instruct [here](https://colab.research.google.com/drive/1T-YBVfnphoVc8E2E854qF3jdia2Ll2W2?usp=sharing)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eSvM9zX_2d3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2v_X2fA0Df5"
      },
      "source": [
        "* We support Llama, Mistral, Phi-3, Gemma, Yi, DeepSeek, Qwen, TinyLlama, Vicuna, Open Hermes etc\n",
        "* We support 16bit LoRA or 4bit QLoRA. Both 2x faster.\n",
        "* `max_seq_length` can be set to anything, since we do automatic RoPE Scaling via [kaiokendev's](https://kaiokendev.github.io/til) method.\n",
        "* [**NEW**] We make Gemma-2 9b / 27b **2x faster**! See our [Gemma-2 9b notebook](https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4?usp=sharing)\n",
        "* [**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)\n",
        "* [**NEW**] We make Mistral NeMo 12B 2x faster and fit in under 12GB of VRAM! [Mistral NeMo notebook](https://colab.research.google.com/drive/17d3U-CAIwzmbDRqbZ9NnpHxCkmXB6LZ0?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
        "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
        "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 8,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HlHqMTbCGCcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We now use the Alpaca dataset from [yahma](https://huggingface.co/datasets/yahma/alpaca-cleaned), which is a filtered version of 52K of the original [Alpaca dataset](https://crfm.stanford.edu/2023/03/13/alpaca.html). You can replace this code section with your own data prep.\n",
        "\n",
        "**[NOTE]** To train only on completions (ignoring the user's input) read TRL's docs [here](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only).\n",
        "\n",
        "**[NOTE]** Remember to add the **EOS_TOKEN** to the tokenized output!! Otherwise you'll get infinite generations!\n",
        "\n",
        "If you want to use the `llama-3` template for ShareGPT datasets, try our conversational [notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing).\n",
        "\n",
        "For text completions like novel writing, try this [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "!pip install openpyxl\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "csv_path = \"/content/drive/MyDrive/Fine-tune/Dataset_last_version.xlsx\"\n",
        "\n",
        "# Excel dosyasını okurken encoding parametresini belirtin\n",
        "df = pd.read_excel(\n",
        "    csv_path,\n",
        "    engine='openpyxl'\n",
        ")\n",
        "\n",
        "# Karakter temizleme işlemini kaldırın veya değiştirin\n",
        "# Bu satırı kaldırın çünkü Türkçe karakterleri siliyor:\n",
        "# df = df.replace(r'[^\\x00-\\x7F]+', '?', regex=True)\n",
        "\n",
        "# Sadece geçersiz karakterleri temizlemek için daha spesifik bir regex kullanın\n",
        "df = df.applymap(lambda x: x if isinstance(x, str) else str(x))\n",
        "# Sadece gerçekten problemli karakterleri temizleyin, Türkçe karakterlere dokunmayın\n",
        "df = df.replace(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]', '', regex=True)\n",
        "\n",
        "# Gerekli sütunları seçin\n",
        "df = df[['instruction', 'input', 'output']]\n",
        "\n",
        "# Dataset'e çevirin\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# Prompt formatlama işlemi\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs = examples[\"input\"]\n",
        "    outputs = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Map işlemini uygulayın\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "# Kontrol için yazdırma\n",
        "df_dataset = dataset.to_pandas()\n",
        "print(df_dataset.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dosyanın içeriğini kontrol etmek için\n"
      ],
      "metadata": {
        "id": "-a8DGXR5eLNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"İlk birkaç satır:\")\n",
        "print(df[['instruction', 'input', 'output']].head(30))\n",
        "print(\"\\nKarakter encoding'i kontrol:\")\n",
        "for col in ['instruction', 'input', 'output']:\n",
        "    print(f\"\\n{col} sütunundaki benzersiz karakterler:\")\n",
        "    unique_chars = set(''.join(df[col].astype(str).values))\n",
        "    print(''.join(sorted(unique_chars)))"
      ],
      "metadata": {
        "id": "MhVvwVZ0eJV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LOG LOSS A GÖRE MAX_STEPS BELİRLEYECEĞİZ. BURADA num_train_epochs=1 YAPINCA TOPLAM STEPS SAYISINI BULACAĞIZ. YANİ 10.000 CİVARI TOPLAM STEPS YAPINCA %100'Ü EĞİTİLİYOR."
      ],
      "metadata": {
        "id": "pIZM52wjRWSP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        #num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 1000,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model kaydetme işlemi - full model olarak kaydet"
      ],
      "metadata": {
        "id": "sNfvmzqDRED0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if True: model.push_to_hub_merged(\"Meta-Llama-3.1-8B-Instruct_syseng_vllm_v3\", tokenizer, save_method = \"merged_16bit\", token = \"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\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Kullanıcının sağladığı 1000 eğitim kaybı değerini içeren tam listeyi yeniden tanımlama\n",
        "#8_8_true\n",
        "training_loss_values = \"\"\"\n",
        "1\t2.175000\n",
        "2\t2.223000\n",
        "3\t2.261000\n",
        "4\t1.943900\n",
        "5\t1.878800\n",
        "6\t1.660700\n",
        "7\t1.518600\n",
        "8\t1.341500\n",
        "9\t1.374400\n",
        "10\t1.146700\n",
        "11\t1.154300\n",
        "12\t1.100100\n",
        "13\t0.992000\n",
        "14\t1.069300\n",
        "15\t1.041100\n",
        "16\t1.152200\n",
        "17\t0.919900\n",
        "18\t1.016800\n",
        "19\t0.970200\n",
        "20\t0.891900\n",
        "21\t1.014700\n",
        "22\t1.052800\n",
        "23\t1.050400\n",
        "24\t1.049200\n",
        "25\t0.910100\n",
        "26\t0.958000\n",
        "27\t0.937500\n",
        "28\t0.895200\n",
        "29\t0.987000\n",
        "30\t0.976400\n",
        "31\t0.905100\n",
        "32\t1.041800\n",
        "33\t0.912500\n",
        "34\t1.008900\n",
        "35\t0.902000\n",
        "36\t0.959000\n",
        "37\t0.895600\n",
        "38\t0.983600\n",
        "39\t0.940700\n",
        "40\t1.048100\n",
        "41\t1.058100\n",
        "42\t0.976400\n",
        "43\t1.009100\n",
        "44\t0.900200\n",
        "45\t0.916800\n",
        "46\t0.916300\n",
        "47\t0.955200\n",
        "48\t0.927200\n",
        "49\t0.986700\n",
        "50\t0.954000\n",
        "51\t0.896500\n",
        "52\t0.975300\n",
        "53\t1.015000\n",
        "54\t0.856200\n",
        "55\t0.836600\n",
        "56\t0.961100\n",
        "57\t0.923400\n",
        "58\t0.935300\n",
        "59\t0.848800\n",
        "60\t0.837900\n",
        "61\t0.826000\n",
        "62\t1.018900\n",
        "63\t0.918500\n",
        "64\t0.955300\n",
        "65\t0.908600\n",
        "66\t0.962600\n",
        "67\t0.907400\n",
        "68\t0.848000\n",
        "69\t0.952300\n",
        "70\t0.861800\n",
        "71\t0.828700\n",
        "72\t0.926900\n",
        "73\t0.846500\n",
        "74\t0.959900\n",
        "75\t0.855200\n",
        "76\t0.791500\n",
        "77\t0.887900\n",
        "78\t0.829500\n",
        "79\t0.829300\n",
        "80\t0.889300\n",
        "81\t0.805800\n",
        "82\t0.815100\n",
        "83\t0.931000\n",
        "84\t0.844700\n",
        "85\t0.959200\n",
        "86\t0.884600\n",
        "87\t0.802800\n",
        "88\t0.941200\n",
        "89\t0.971200\n",
        "90\t0.826200\n",
        "91\t0.927600\n",
        "92\t0.896800\n",
        "93\t0.892300\n",
        "94\t0.919000\n",
        "95\t0.995600\n",
        "96\t0.797000\n",
        "97\t0.994600\n",
        "98\t0.890000\n",
        "99\t0.857600\n",
        "100\t0.814100\n",
        "101\t0.884300\n",
        "102\t0.736400\n",
        "103\t0.887600\n",
        "104\t0.842800\n",
        "105\t0.850100\n",
        "106\t0.889400\n",
        "107\t0.860000\n",
        "108\t0.916600\n",
        "109\t0.796100\n",
        "110\t0.959600\n",
        "111\t0.795400\n",
        "112\t0.786000\n",
        "113\t0.741000\n",
        "114\t0.937000\n",
        "115\t0.806600\n",
        "116\t0.857300\n",
        "117\t0.887800\n",
        "118\t0.849600\n",
        "119\t0.873500\n",
        "120\t0.833800\n",
        "121\t0.788900\n",
        "122\t0.839400\n",
        "123\t0.937200\n",
        "124\t0.863100\n",
        "125\t0.822400\n",
        "126\t0.759100\n",
        "127\t0.911800\n",
        "128\t0.821000\n",
        "129\t0.828200\n",
        "130\t0.884200\n",
        "131\t1.009200\n",
        "132\t0.897600\n",
        "133\t0.927800\n",
        "134\t0.808500\n",
        "135\t0.802100\n",
        "136\t0.791900\n",
        "137\t0.781500\n",
        "138\t0.908500\n",
        "139\t0.823900\n",
        "140\t0.770700\n",
        "141\t0.832100\n",
        "142\t0.931200\n",
        "143\t0.951800\n",
        "144\t0.959100\n",
        "145\t0.836000\n",
        "146\t0.983700\n",
        "147\t1.008100\n",
        "148\t1.002600\n",
        "149\t0.821200\n",
        "150\t0.819400\n",
        "151\t0.815600\n",
        "152\t0.796100\n",
        "153\t0.817800\n",
        "154\t0.878300\n",
        "155\t0.883600\n",
        "156\t0.775500\n",
        "157\t0.849300\n",
        "158\t0.896400\n",
        "159\t0.884800\n",
        "160\t0.731400\n",
        "161\t0.750500\n",
        "162\t0.671600\n",
        "163\t0.913000\n",
        "164\t0.822200\n",
        "165\t0.870700\n",
        "166\t0.747600\n",
        "167\t0.809900\n",
        "168\t0.879600\n",
        "169\t0.906400\n",
        "170\t0.868700\n",
        "171\t0.880800\n",
        "172\t0.722100\n",
        "173\t0.820500\n",
        "174\t0.905700\n",
        "175\t0.931500\n",
        "176\t0.913000\n",
        "177\t1.002900\n",
        "178\t0.868300\n",
        "179\t0.910100\n",
        "180\t0.825900\n",
        "181\t0.721800\n",
        "182\t0.811000\n",
        "183\t0.756100\n",
        "184\t0.804100\n",
        "185\t0.789200\n",
        "186\t0.934500\n",
        "187\t0.908100\n",
        "188\t0.883600\n",
        "189\t0.814400\n",
        "190\t0.849700\n",
        "191\t0.814200\n",
        "192\t0.778300\n",
        "193\t0.863100\n",
        "194\t0.804800\n",
        "195\t0.800700\n",
        "196\t0.860400\n",
        "197\t0.687000\n",
        "198\t0.746000\n",
        "199\t0.730800\n",
        "200\t0.698900\n",
        "201\t0.830700\n",
        "202\t0.710600\n",
        "203\t0.832000\n",
        "204\t0.856600\n",
        "205\t1.050600\n",
        "206\t0.764000\n",
        "207\t0.850900\n",
        "208\t0.820700\n",
        "209\t0.761100\n",
        "210\t0.813300\n",
        "211\t0.823000\n",
        "212\t0.766800\n",
        "213\t0.789400\n",
        "214\t0.798400\n",
        "215\t0.847700\n",
        "216\t0.813400\n",
        "217\t0.862100\n",
        "218\t0.708300\n",
        "219\t0.915100\n",
        "220\t0.761700\n",
        "221\t0.807200\n",
        "222\t0.735500\n",
        "223\t0.852400\n",
        "224\t0.814200\n",
        "225\t0.788200\n",
        "226\t0.770800\n",
        "227\t0.804400\n",
        "228\t0.712300\n",
        "229\t0.819500\n",
        "230\t0.939300\n",
        "231\t0.730300\n",
        "232\t0.860200\n",
        "233\t1.040800\n",
        "234\t0.950300\n",
        "235\t0.813400\n",
        "236\t0.795500\n",
        "237\t0.698700\n",
        "238\t0.755400\n",
        "239\t0.824200\n",
        "240\t0.784400\n",
        "241\t0.820800\n",
        "242\t0.818200\n",
        "243\t0.809000\n",
        "244\t0.761700\n",
        "245\t0.862800\n",
        "246\t0.806900\n",
        "247\t0.790900\n",
        "248\t0.707600\n",
        "249\t0.948600\n",
        "250\t0.856700\n",
        "251\t0.826400\n",
        "252\t0.786000\n",
        "253\t0.776800\n",
        "254\t0.673300\n",
        "255\t0.907800\n",
        "256\t0.723800\n",
        "257\t0.968600\n",
        "258\t0.748400\n",
        "259\t0.731200\n",
        "260\t0.751100\n",
        "261\t0.800700\n",
        "262\t0.741900\n",
        "263\t0.746000\n",
        "264\t0.817200\n",
        "265\t0.853100\n",
        "266\t0.777900\n",
        "267\t0.752200\n",
        "268\t0.659700\n",
        "269\t0.755000\n",
        "270\t0.766400\n",
        "271\t0.872200\n",
        "272\t0.785800\n",
        "273\t0.801200\n",
        "274\t0.687100\n",
        "275\t0.810600\n",
        "276\t0.674400\n",
        "277\t0.705600\n",
        "278\t0.829000\n",
        "279\t0.774400\n",
        "280\t0.779800\n",
        "281\t0.721600\n",
        "282\t0.850200\n",
        "283\t0.689300\n",
        "284\t0.787300\n",
        "285\t0.771700\n",
        "286\t0.798100\n",
        "287\t0.784500\n",
        "288\t0.901400\n",
        "289\t0.686800\n",
        "290\t0.765900\n",
        "291\t0.840100\n",
        "292\t0.892800\n",
        "293\t0.875700\n",
        "294\t0.957800\n",
        "295\t0.837600\n",
        "296\t0.782700\n",
        "297\t0.846900\n",
        "298\t0.711300\n",
        "299\t0.771800\n",
        "300\t0.765100\n",
        "301\t0.774100\n",
        "302\t0.681900\n",
        "303\t0.723800\n",
        "304\t0.851000\n",
        "305\t0.862100\n",
        "306\t0.751600\n",
        "307\t0.927800\n",
        "308\t0.695800\n",
        "309\t0.687900\n",
        "310\t0.840400\n",
        "311\t0.774900\n",
        "312\t0.656300\n",
        "313\t0.836400\n",
        "314\t0.764800\n",
        "315\t0.675300\n",
        "316\t0.807600\n",
        "317\t0.702700\n",
        "318\t0.703700\n",
        "319\t0.683700\n",
        "320\t0.848300\n",
        "321\t0.744900\n",
        "322\t0.689100\n",
        "323\t0.783500\n",
        "324\t0.641200\n",
        "325\t0.823700\n",
        "326\t0.799900\n",
        "327\t0.948900\n",
        "328\t0.812200\n",
        "329\t0.785300\n",
        "330\t0.722100\n",
        "331\t0.721900\n",
        "332\t0.714100\n",
        "333\t0.707300\n",
        "334\t0.813400\n",
        "335\t0.861900\n",
        "336\t0.785600\n",
        "337\t0.748600\n",
        "338\t0.848700\n",
        "339\t0.691400\n",
        "340\t0.719200\n",
        "341\t0.844400\n",
        "342\t0.765100\n",
        "343\t0.723600\n",
        "344\t0.789500\n",
        "345\t0.812900\n",
        "346\t0.640000\n",
        "347\t0.747800\n",
        "348\t0.749500\n",
        "349\t0.854900\n",
        "350\t0.805700\n",
        "351\t0.869500\n",
        "352\t0.715600\n",
        "353\t0.766100\n",
        "354\t0.908300\n",
        "355\t0.783800\n",
        "356\t0.695300\n",
        "357\t0.875300\n",
        "358\t0.775400\n",
        "359\t0.647200\n",
        "360\t0.770100\n",
        "361\t0.698800\n",
        "362\t0.852900\n",
        "363\t0.814800\n",
        "364\t0.814300\n",
        "365\t0.818400\n",
        "366\t0.775700\n",
        "367\t0.767500\n",
        "368\t0.782400\n",
        "369\t0.760200\n",
        "370\t0.947100\n",
        "371\t0.777300\n",
        "372\t0.791800\n",
        "373\t0.784200\n",
        "374\t0.704700\n",
        "375\t0.947400\n",
        "376\t0.776000\n",
        "377\t0.779500\n",
        "378\t0.861200\n",
        "379\t0.795500\n",
        "380\t0.639500\n",
        "381\t0.655800\n",
        "382\t0.841600\n",
        "383\t0.717900\n",
        "384\t0.702700\n",
        "385\t0.810500\n",
        "386\t0.724300\n",
        "387\t0.831900\n",
        "388\t0.770300\n",
        "389\t0.834600\n",
        "390\t0.751000\n",
        "391\t0.852900\n",
        "392\t0.685400\n",
        "393\t0.835000\n",
        "394\t0.851500\n",
        "395\t0.734800\n",
        "396\t0.839900\n",
        "397\t0.704600\n",
        "398\t0.853300\n",
        "399\t0.656500\n",
        "400\t0.753300\n",
        "401\t0.762300\n",
        "402\t0.769000\n",
        "403\t0.753100\n",
        "404\t0.693400\n",
        "405\t0.760500\n",
        "406\t0.660900\n",
        "407\t0.849600\n",
        "408\t0.704500\n",
        "409\t0.757200\n",
        "410\t0.764500\n",
        "411\t0.769400\n",
        "412\t0.763800\n",
        "413\t0.708500\n",
        "414\t0.738500\n",
        "415\t0.708900\n",
        "416\t0.708500\n",
        "417\t0.722900\n",
        "418\t0.735300\n",
        "419\t0.757100\n",
        "420\t0.791800\n",
        "421\t0.712300\n",
        "422\t0.832400\n",
        "423\t0.775300\n",
        "424\t0.746600\n",
        "425\t0.728800\n",
        "426\t0.783300\n",
        "427\t0.689800\n",
        "428\t0.863100\n",
        "429\t0.784300\n",
        "430\t0.696600\n",
        "431\t0.862200\n",
        "432\t0.729800\n",
        "433\t0.713600\n",
        "434\t0.847600\n",
        "435\t0.681700\n",
        "436\t0.791000\n",
        "437\t0.617100\n",
        "438\t0.796900\n",
        "439\t0.770800\n",
        "440\t0.796900\n",
        "441\t0.714500\n",
        "442\t0.665300\n",
        "443\t0.691000\n",
        "444\t0.683200\n",
        "445\t0.647000\n",
        "446\t0.725000\n",
        "447\t0.781700\n",
        "448\t0.676300\n",
        "449\t0.714200\n",
        "450\t0.739800\n",
        "451\t0.843300\n",
        "452\t0.732500\n",
        "453\t0.781300\n",
        "454\t0.756600\n",
        "455\t0.767500\n",
        "456\t0.752700\n",
        "457\t0.683800\n",
        "458\t0.774600\n",
        "459\t0.632800\n",
        "460\t0.873600\n",
        "461\t0.758000\n",
        "462\t0.689200\n",
        "463\t0.864800\n",
        "464\t0.765900\n",
        "465\t0.791700\n",
        "466\t0.817200\n",
        "467\t0.713100\n",
        "468\t0.867200\n",
        "469\t0.678500\n",
        "470\t0.771300\n",
        "471\t0.741300\n",
        "472\t0.677600\n",
        "473\t0.750800\n",
        "474\t0.808800\n",
        "475\t0.760400\n",
        "476\t0.957400\n",
        "477\t0.779800\n",
        "478\t0.713000\n",
        "479\t0.884900\n",
        "480\t0.832600\n",
        "481\t0.722700\n",
        "482\t0.709700\n",
        "483\t0.815400\n",
        "484\t0.727000\n",
        "485\t0.691900\n",
        "486\t0.736800\n",
        "487\t0.676800\n",
        "488\t0.823900\n",
        "489\t0.774500\n",
        "490\t0.781000\n",
        "491\t0.694100\n",
        "492\t0.751000\n",
        "493\t0.691000\n",
        "494\t0.734200\n",
        "495\t0.771900\n",
        "496\t0.767600\n",
        "497\t0.739400\n",
        "498\t0.683200\n",
        "499\t0.747000\n",
        "500\t0.868800\n",
        "501\t0.724900\n",
        "502\t0.760200\n",
        "503\t0.687400\n",
        "504\t0.791800\n",
        "505\t0.699400\n",
        "506\t0.714100\n",
        "507\t0.800500\n",
        "508\t0.610200\n",
        "509\t0.645800\n",
        "510\t0.767400\n",
        "511\t0.741200\n",
        "512\t0.732200\n",
        "513\t0.804700\n",
        "514\t0.703800\n",
        "515\t0.776500\n",
        "516\t0.730200\n",
        "517\t0.803800\n",
        "518\t0.706200\n",
        "519\t0.791800\n",
        "520\t0.707400\n",
        "521\t0.741700\n",
        "522\t0.728400\n",
        "523\t0.780100\n",
        "524\t0.733500\n",
        "525\t0.656500\n",
        "526\t0.698000\n",
        "527\t0.745100\n",
        "528\t0.741500\n",
        "529\t0.860700\n",
        "530\t0.626100\n",
        "531\t0.777600\n",
        "532\t0.682800\n",
        "533\t0.649700\n",
        "534\t0.608200\n",
        "535\t0.756400\n",
        "536\t0.750200\n",
        "537\t0.708400\n",
        "538\t0.691300\n",
        "539\t0.677000\n",
        "540\t0.794700\n",
        "541\t0.780800\n",
        "542\t0.705900\n",
        "543\t0.599200\n",
        "544\t0.729700\n",
        "545\t0.754700\n",
        "546\t0.618900\n",
        "547\t0.775200\n",
        "548\t0.704200\n",
        "549\t0.736600\n",
        "550\t0.777800\n",
        "551\t0.809900\n",
        "552\t0.705300\n",
        "553\t0.694200\n",
        "554\t0.666600\n",
        "555\t0.696300\n",
        "556\t0.707200\n",
        "557\t0.634400\n",
        "558\t0.834700\n",
        "559\t0.724100\n",
        "560\t0.734300\n",
        "561\t0.718200\n",
        "562\t0.724500\n",
        "563\t0.742300\n",
        "564\t0.733200\n",
        "565\t0.667800\n",
        "566\t0.663200\n",
        "567\t0.631800\n",
        "568\t0.768500\n",
        "569\t0.842300\n",
        "570\t0.740600\n",
        "571\t0.799600\n",
        "572\t0.651800\n",
        "573\t0.672200\n",
        "574\t0.836600\n",
        "575\t0.825100\n",
        "576\t0.665300\n",
        "577\t0.746500\n",
        "578\t0.765900\n",
        "579\t0.648100\n",
        "580\t0.735600\n",
        "581\t0.741100\n",
        "582\t0.619500\n",
        "583\t0.885500\n",
        "584\t0.614100\n",
        "585\t0.600900\n",
        "586\t0.739400\n",
        "587\t0.745700\n",
        "588\t0.776400\n",
        "589\t0.724600\n",
        "590\t0.669400\n",
        "591\t0.694800\n",
        "592\t0.809300\n",
        "593\t0.694400\n",
        "594\t0.834500\n",
        "595\t0.696700\n",
        "596\t0.832300\n",
        "597\t0.741500\n",
        "598\t0.818400\n",
        "599\t0.615300\n",
        "600\t0.693400\n",
        "601\t0.677100\n",
        "602\t0.781200\n",
        "603\t0.659200\n",
        "604\t0.762100\n",
        "605\t0.779900\n",
        "606\t0.729300\n",
        "607\t0.800100\n",
        "608\t0.795300\n",
        "609\t0.760200\n",
        "610\t0.726100\n",
        "611\t0.698100\n",
        "612\t0.621500\n",
        "613\t0.771400\n",
        "614\t0.658600\n",
        "615\t0.809400\n",
        "616\t0.856300\n",
        "617\t0.800400\n",
        "618\t0.647000\n",
        "619\t0.745000\n",
        "620\t0.815800\n",
        "621\t0.880500\n",
        "622\t0.678500\n",
        "623\t0.736000\n",
        "624\t0.685600\n",
        "625\t0.691800\n",
        "626\t0.809700\n",
        "627\t0.804200\n",
        "628\t0.627100\n",
        "629\t0.741200\n",
        "630\t0.724000\n",
        "631\t0.847700\n",
        "632\t0.648300\n",
        "633\t0.557700\n",
        "634\t0.595000\n",
        "635\t0.650700\n",
        "636\t0.689400\n",
        "637\t0.796400\n",
        "638\t0.693100\n",
        "639\t0.737600\n",
        "640\t0.704200\n",
        "641\t0.787900\n",
        "642\t0.769100\n",
        "643\t0.671400\n",
        "644\t0.732100\n",
        "645\t0.794300\n",
        "646\t0.846000\n",
        "647\t0.697000\n",
        "648\t0.719400\n",
        "649\t0.701600\n",
        "650\t0.668000\n",
        "651\t0.748900\n",
        "652\t0.653000\n",
        "653\t0.775800\n",
        "654\t0.646500\n",
        "655\t0.619200\n",
        "656\t0.845400\n",
        "657\t0.698800\n",
        "658\t0.661600\n",
        "659\t0.678300\n",
        "660\t0.685700\n",
        "661\t0.742200\n",
        "662\t0.726600\n",
        "663\t0.795200\n",
        "664\t0.758300\n",
        "665\t0.667900\n",
        "666\t0.779300\n",
        "667\t0.852800\n",
        "668\t0.741000\n",
        "669\t0.733300\n",
        "670\t0.796100\n",
        "671\t0.744200\n",
        "672\t0.638500\n",
        "673\t0.737300\n",
        "674\t0.729800\n",
        "675\t0.920600\n",
        "676\t0.693400\n",
        "677\t0.671900\n",
        "678\t0.732500\n",
        "679\t0.790300\n",
        "680\t0.801500\n",
        "681\t0.809400\n",
        "682\t0.789200\n",
        "683\t0.709100\n",
        "684\t0.815200\n",
        "685\t0.777400\n",
        "686\t0.805600\n",
        "687\t0.779700\n",
        "688\t0.773600\n",
        "689\t0.828200\n",
        "690\t0.647100\n",
        "691\t0.672600\n",
        "692\t0.714300\n",
        "693\t0.797200\n",
        "694\t0.678400\n",
        "695\t0.703800\n",
        "696\t0.884100\n",
        "697\t0.718100\n",
        "698\t0.670000\n",
        "699\t0.695000\n",
        "700\t0.722200\n",
        "701\t0.664200\n",
        "702\t0.654700\n",
        "703\t0.835200\n",
        "704\t0.826100\n",
        "705\t0.695400\n",
        "706\t0.633000\n",
        "707\t0.875400\n",
        "708\t0.671800\n",
        "709\t0.653800\n",
        "710\t0.593200\n",
        "711\t0.651300\n",
        "712\t0.670100\n",
        "713\t0.841800\n",
        "714\t0.726800\n",
        "715\t0.765600\n",
        "716\t0.751300\n",
        "717\t0.763200\n",
        "718\t0.688900\n",
        "719\t0.730600\n",
        "720\t0.607100\n",
        "721\t0.648600\n",
        "722\t0.809700\n",
        "723\t0.755100\n",
        "724\t0.798600\n",
        "725\t0.818000\n",
        "726\t0.729600\n",
        "727\t0.658600\n",
        "728\t0.637500\n",
        "729\t0.694100\n",
        "730\t0.707600\n",
        "731\t0.701100\n",
        "732\t0.630600\n",
        "733\t0.688400\n",
        "734\t0.759300\n",
        "735\t0.830900\n",
        "736\t0.705500\n",
        "737\t0.665700\n",
        "738\t0.739800\n",
        "739\t0.719800\n",
        "740\t0.903400\n",
        "741\t0.757400\n",
        "742\t0.672700\n",
        "743\t0.743700\n",
        "744\t0.630700\n",
        "745\t0.810900\n",
        "746\t0.773700\n",
        "747\t0.726900\n",
        "748\t0.680500\n",
        "749\t0.831300\n",
        "750\t0.819100\n",
        "751\t0.673900\n",
        "752\t0.662800\n",
        "753\t0.780600\n",
        "754\t0.665500\n",
        "755\t0.756000\n",
        "756\t0.679700\n",
        "757\t0.655800\n",
        "758\t0.701400\n",
        "759\t0.674200\n",
        "760\t0.621200\n",
        "761\t0.793200\n",
        "762\t0.610300\n",
        "763\t0.682500\n",
        "764\t0.740400\n",
        "765\t0.923600\n",
        "766\t0.703900\n",
        "767\t0.696200\n",
        "768\t0.713700\n",
        "769\t0.698200\n",
        "770\t0.796300\n",
        "771\t0.814700\n",
        "772\t0.736600\n",
        "773\t0.712400\n",
        "774\t0.828400\n",
        "775\t0.706800\n",
        "776\t0.688300\n",
        "777\t0.664400\n",
        "778\t0.777500\n",
        "779\t0.649900\n",
        "780\t0.595400\n",
        "781\t0.741500\n",
        "782\t0.678800\n",
        "783\t0.630400\n",
        "784\t0.723200\n",
        "785\t0.766800\n",
        "786\t0.719100\n",
        "787\t0.781200\n",
        "788\t0.716500\n",
        "789\t0.743700\n",
        "790\t0.681500\n",
        "791\t0.826300\n",
        "792\t0.651600\n",
        "793\t0.746300\n",
        "794\t0.734100\n",
        "795\t0.657200\n",
        "796\t0.605500\n",
        "797\t0.638600\n",
        "798\t0.750400\n",
        "799\t0.619900\n",
        "800\t0.772900\n",
        "801\t0.655400\n",
        "802\t0.740600\n",
        "803\t0.567700\n",
        "804\t0.749800\n",
        "805\t0.707700\n",
        "806\t0.740100\n",
        "807\t0.702900\n",
        "808\t0.648900\n",
        "809\t0.710500\n",
        "810\t0.688200\n",
        "811\t0.688400\n",
        "812\t0.747800\n",
        "813\t0.664400\n",
        "814\t0.778700\n",
        "815\t0.717300\n",
        "816\t0.713400\n",
        "817\t0.612700\n",
        "818\t0.744300\n",
        "819\t0.710200\n",
        "820\t0.791400\n",
        "821\t0.685400\n",
        "822\t0.694000\n",
        "823\t0.674800\n",
        "824\t0.709700\n",
        "825\t0.753000\n",
        "826\t0.795400\n",
        "827\t0.832900\n",
        "828\t0.679400\n",
        "829\t0.611700\n",
        "830\t0.762600\n",
        "831\t0.790000\n",
        "832\t0.776000\n",
        "833\t0.851000\n",
        "834\t0.782100\n",
        "835\t0.709200\n",
        "836\t0.636100\n",
        "837\t0.654700\n",
        "838\t0.743700\n",
        "839\t0.642800\n",
        "840\t0.733500\n",
        "841\t0.692700\n",
        "842\t0.712100\n",
        "843\t0.717600\n",
        "844\t0.740100\n",
        "845\t0.698800\n",
        "846\t0.666500\n",
        "847\t0.700000\n",
        "848\t0.762000\n",
        "849\t0.671400\n",
        "850\t0.678300\n",
        "851\t0.684600\n",
        "852\t0.771900\n",
        "853\t0.761300\n",
        "854\t0.672500\n",
        "855\t0.706400\n",
        "856\t0.740100\n",
        "857\t0.684600\n",
        "858\t0.771300\n",
        "859\t0.696400\n",
        "860\t0.686900\n",
        "861\t0.708900\n",
        "862\t0.692200\n",
        "863\t0.592100\n",
        "864\t0.645000\n",
        "865\t0.712500\n",
        "866\t0.663600\n",
        "867\t0.740300\n",
        "868\t0.615600\n",
        "869\t0.664700\n",
        "870\t0.687900\n",
        "871\t0.730500\n",
        "872\t0.805900\n",
        "873\t0.674200\n",
        "874\t0.822200\n",
        "875\t0.853700\n",
        "876\t0.618600\n",
        "877\t0.773300\n",
        "878\t0.673600\n",
        "879\t0.629900\n",
        "880\t0.712200\n",
        "881\t0.753300\n",
        "882\t0.655100\n",
        "883\t0.718400\n",
        "884\t0.622800\n",
        "885\t0.715300\n",
        "886\t0.563200\n",
        "887\t0.630200\n",
        "888\t0.719700\n",
        "889\t0.662200\n",
        "890\t0.700000\n",
        "891\t0.743800\n",
        "892\t0.600200\n",
        "893\t0.607000\n",
        "894\t0.709000\n",
        "895\t0.718300\n",
        "896\t0.617000\n",
        "897\t0.674200\n",
        "898\t0.671400\n",
        "899\t0.738000\n",
        "900\t0.614200\n",
        "901\t0.605800\n",
        "902\t0.696100\n",
        "903\t0.797400\n",
        "904\t0.763600\n",
        "905\t0.760000\n",
        "906\t0.698100\n",
        "907\t0.771900\n",
        "908\t0.709500\n",
        "909\t0.765200\n",
        "910\t0.668800\n",
        "911\t0.788900\n",
        "912\t0.651500\n",
        "913\t0.690300\n",
        "914\t0.667000\n",
        "915\t0.722200\n",
        "916\t0.635200\n",
        "917\t0.700100\n",
        "918\t0.695400\n",
        "919\t0.698600\n",
        "920\t0.688500\n",
        "921\t0.649900\n",
        "922\t0.618000\n",
        "923\t0.709800\n",
        "924\t0.662200\n",
        "925\t0.763900\n",
        "926\t0.623700\n",
        "927\t0.680000\n",
        "928\t0.902600\n",
        "929\t0.663400\n",
        "930\t0.680200\n",
        "931\t0.683700\n",
        "932\t0.724400\n",
        "933\t0.670300\n",
        "934\t0.809700\n",
        "935\t0.758100\n",
        "936\t0.694100\n",
        "937\t0.669100\n",
        "938\t0.706200\n",
        "939\t0.771200\n",
        "940\t0.660900\n",
        "941\t0.799800\n",
        "942\t0.708600\n",
        "943\t0.738500\n",
        "944\t0.647900\n",
        "945\t0.817300\n",
        "946\t0.670600\n",
        "947\t0.677800\n",
        "948\t0.692200\n",
        "949\t0.671300\n",
        "950\t0.646800\n",
        "951\t0.713600\n",
        "952\t0.665000\n",
        "953\t0.872800\n",
        "954\t0.649100\n",
        "955\t0.772000\n",
        "956\t0.796100\n",
        "957\t0.834500\n",
        "958\t0.667900\n",
        "959\t0.655700\n",
        "960\t0.655600\n",
        "961\t0.697100\n",
        "962\t0.652400\n",
        "963\t0.753700\n",
        "964\t0.707200\n",
        "965\t0.587700\n",
        "966\t0.740600\n",
        "967\t0.610400\n",
        "968\t0.620900\n",
        "969\t0.633800\n",
        "970\t0.621100\n",
        "971\t0.709100\n",
        "972\t0.666800\n",
        "973\t0.918500\n",
        "974\t0.717000\n",
        "975\t0.788400\n",
        "976\t0.720300\n",
        "977\t0.641900\n",
        "978\t0.662600\n",
        "979\t0.824900\n",
        "980\t0.611800\n",
        "981\t0.712500\n",
        "982\t0.711300\n",
        "983\t0.685700\n",
        "984\t0.702800\n",
        "985\t0.630000\n",
        "986\t0.636800\n",
        "987\t0.643800\n",
        "988\t0.791700\n",
        "989\t0.771000\n",
        "990\t0.698900\n",
        "991\t0.687800\n",
        "992\t0.717500\n",
        "993\t0.659900\n",
        "994\t0.811600\n",
        "995\t0.625800\n",
        "996\t0.697100\n",
        "997\t0.696700\n",
        "998\t0.764700\n",
        "999\t0.806000\n",
        "1000\t0.727900\n",
        "\"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Parse the training loss values\n",
        "lines = training_loss_values.strip().split(\"\\n\")\n",
        "epochs = []\n",
        "loss_values = []\n",
        "\n",
        "for line in lines:\n",
        "    epoch, loss = line.split()\n",
        "    epochs.append(int(epoch))\n",
        "    loss_values.append(float(loss))\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(epochs, loss_values, label=\"Training Loss\", color='b')\n",
        "\n",
        "# Labels and title\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss Over Epochs\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hN9fZeqm1Do0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}