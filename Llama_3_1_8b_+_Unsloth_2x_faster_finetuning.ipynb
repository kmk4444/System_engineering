{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmk4444/System_engineering/blob/main/Llama_3_1_8b_%2B_Unsloth_2x_faster_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqM-T1RTzY6C"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://github.com/unslothai/unsloth?tab=readme-ov-file#-installation-instructions).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save) (eg for Llama.cpp).\n",
        "\n",
        "[NEW] Llama-3.1 8b, 70b & 405b are trained on a crazy 15 trillion tokens with 128K long context lengths!\n",
        "\n",
        "**[NEW] Try 2x faster inference in a free Colab for Llama-3.1 8b Instruct [here](https://colab.research.google.com/drive/1T-YBVfnphoVc8E2E854qF3jdia2Ll2W2?usp=sharing)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eSvM9zX_2d3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2v_X2fA0Df5"
      },
      "source": [
        "* We support Llama, Mistral, Phi-3, Gemma, Yi, DeepSeek, Qwen, TinyLlama, Vicuna, Open Hermes etc\n",
        "* We support 16bit LoRA or 4bit QLoRA. Both 2x faster.\n",
        "* `max_seq_length` can be set to anything, since we do automatic RoPE Scaling via [kaiokendev's](https://kaiokendev.github.io/til) method.\n",
        "* [**NEW**] We make Gemma-2 9b / 27b **2x faster**! See our [Gemma-2 9b notebook](https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4?usp=sharing)\n",
        "* [**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)\n",
        "* [**NEW**] We make Mistral NeMo 12B 2x faster and fit in under 12GB of VRAM! [Mistral NeMo notebook](https://colab.research.google.com/drive/17d3U-CAIwzmbDRqbZ9NnpHxCkmXB6LZ0?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
        "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
        "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 8,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HlHqMTbCGCcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We now use the Alpaca dataset from [yahma](https://huggingface.co/datasets/yahma/alpaca-cleaned), which is a filtered version of 52K of the original [Alpaca dataset](https://crfm.stanford.edu/2023/03/13/alpaca.html). You can replace this code section with your own data prep.\n",
        "\n",
        "**[NOTE]** To train only on completions (ignoring the user's input) read TRL's docs [here](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only).\n",
        "\n",
        "**[NOTE]** Remember to add the **EOS_TOKEN** to the tokenized output!! Otherwise you'll get infinite generations!\n",
        "\n",
        "If you want to use the `llama-3` template for ShareGPT datasets, try our conversational [notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing).\n",
        "\n",
        "For text completions like novel writing, try this [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "!pip install openpyxl\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "csv_path = \"/content/drive/MyDrive/Fine-tune/Dataset_last_version.xlsx\"\n",
        "\n",
        "# Excel dosyasını okurken encoding parametresini belirtin\n",
        "df = pd.read_excel(\n",
        "    csv_path,\n",
        "    engine='openpyxl'\n",
        ")\n",
        "\n",
        "# Karakter temizleme işlemini kaldırın veya değiştirin\n",
        "# Bu satırı kaldırın çünkü Türkçe karakterleri siliyor:\n",
        "# df = df.replace(r'[^\\x00-\\x7F]+', '?', regex=True)\n",
        "\n",
        "# Sadece geçersiz karakterleri temizlemek için daha spesifik bir regex kullanın\n",
        "df = df.applymap(lambda x: x if isinstance(x, str) else str(x))\n",
        "# Sadece gerçekten problemli karakterleri temizleyin, Türkçe karakterlere dokunmayın\n",
        "df = df.replace(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]', '', regex=True)\n",
        "\n",
        "# Gerekli sütunları seçin\n",
        "df = df[['instruction', 'input', 'output']]\n",
        "\n",
        "# Dataset'e çevirin\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# Prompt formatlama işlemi\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs = examples[\"input\"]\n",
        "    outputs = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Map işlemini uygulayın\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "# Kontrol için yazdırma\n",
        "df_dataset = dataset.to_pandas()\n",
        "print(df_dataset.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dosyanın içeriğini kontrol etmek için\n"
      ],
      "metadata": {
        "id": "-a8DGXR5eLNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"İlk birkaç satır:\")\n",
        "print(df[['instruction', 'input', 'output']].head(30))\n",
        "print(\"\\nKarakter encoding'i kontrol:\")\n",
        "for col in ['instruction', 'input', 'output']:\n",
        "    print(f\"\\n{col} sütunundaki benzersiz karakterler:\")\n",
        "    unique_chars = set(''.join(df[col].astype(str).values))\n",
        "    print(''.join(sorted(unique_chars)))"
      ],
      "metadata": {
        "id": "MhVvwVZ0eJV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LOG LOSS A GÖRE MAX_STEPS BELİRLEYECEĞİZ. BURADA num_train_epochs=1 YAPINCA TOPLAM STEPS SAYISINI BULACAĞIZ. YANİ 10.000 CİVARI TOPLAM STEPS YAPINCA %100'Ü EĞİTİLİYOR."
      ],
      "metadata": {
        "id": "pIZM52wjRWSP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        #num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 1000,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model kaydetme işlemi - full model olarak kaydet"
      ],
      "metadata": {
        "id": "sNfvmzqDRED0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if True: model.push_to_hub_merged(\"Meta-Llama-3.1-8B-Instruct_syseng_vllm_v3\", tokenizer, save_method = \"merged_16bit\", token = \"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\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Kullanıcının sağladığı 1000 eğitim kaybı değerini içeren tam listeyi yeniden tanımlama\n",
        "#r:8, lora_alpha=8\n",
        "training_loss_values = \"\"\"\n",
        "1\t2.175000\n",
        "2\t2.223000\n",
        "3\t2.280100\n",
        "4\t2.010100\n",
        "5\t2.059600\n",
        "6\t1.939600\n",
        "7\t1.906600\n",
        "8\t1.685100\n",
        "9\t1.692700\n",
        "10\t1.406800\n",
        "11\t1.336000\n",
        "12\t1.268800\n",
        "13\t1.101700\n",
        "14\t1.211500\n",
        "15\t1.163300\n",
        "16\t1.272500\n",
        "17\t1.002200\n",
        "18\t1.101200\n",
        "19\t1.030000\n",
        "20\t0.975800\n",
        "21\t1.086200\n",
        "22\t1.116400\n",
        "23\t1.096700\n",
        "24\t1.065400\n",
        "25\t0.966200\n",
        "26\t1.002600\n",
        "27\t0.979400\n",
        "28\t0.916800\n",
        "29\t1.009300\n",
        "30\t1.021900\n",
        "31\t0.937800\n",
        "32\t1.072400\n",
        "33\t0.956800\n",
        "34\t1.039800\n",
        "35\t0.926900\n",
        "36\t0.987700\n",
        "37\t0.931600\n",
        "38\t1.025900\n",
        "39\t0.974300\n",
        "40\t1.079500\n",
        "41\t1.086800\n",
        "42\t0.998500\n",
        "43\t1.048500\n",
        "44\t0.913900\n",
        "45\t0.943300\n",
        "46\t0.933800\n",
        "47\t0.994200\n",
        "48\t0.968800\n",
        "49\t1.028800\n",
        "50\t0.974400\n",
        "51\t0.920800\n",
        "52\t1.013900\n",
        "53\t1.040900\n",
        "54\t0.880400\n",
        "55\t0.882300\n",
        "56\t0.980500\n",
        "57\t0.930000\n",
        "58\t0.967400\n",
        "59\t0.863000\n",
        "60\t0.854600\n",
        "61\t0.839800\n",
        "62\t1.038300\n",
        "63\t0.922600\n",
        "64\t0.976700\n",
        "65\t0.929000\n",
        "66\t0.991300\n",
        "67\t0.937600\n",
        "68\t0.878200\n",
        "69\t0.972800\n",
        "70\t0.883800\n",
        "71\t0.837900\n",
        "72\t0.973200\n",
        "73\t0.871500\n",
        "74\t1.000900\n",
        "75\t0.878900\n",
        "76\t0.814200\n",
        "77\t0.909100\n",
        "78\t0.850000\n",
        "79\t0.840300\n",
        "80\t0.925300\n",
        "81\t0.824300\n",
        "82\t0.816000\n",
        "83\t0.949800\n",
        "84\t0.858200\n",
        "85\t0.973900\n",
        "86\t0.897000\n",
        "87\t0.826500\n",
        "88\t0.975200\n",
        "89\t0.994000\n",
        "90\t0.847100\n",
        "91\t0.947300\n",
        "92\t0.910100\n",
        "93\t0.914000\n",
        "94\t0.961700\n",
        "95\t1.023300\n",
        "96\t0.821900\n",
        "97\t1.009600\n",
        "98\t0.921000\n",
        "99\t0.877500\n",
        "100\t0.831800\n",
        "101\t0.910700\n",
        "102\t0.757200\n",
        "103\t0.907400\n",
        "104\t0.858800\n",
        "105\t0.861700\n",
        "106\t0.896200\n",
        "107\t0.879000\n",
        "108\t0.953300\n",
        "109\t0.810200\n",
        "110\t0.978800\n",
        "111\t0.812100\n",
        "112\t0.804300\n",
        "113\t0.761900\n",
        "114\t0.957900\n",
        "115\t0.823400\n",
        "116\t0.878200\n",
        "117\t0.904200\n",
        "118\t0.872800\n",
        "119\t0.878100\n",
        "120\t0.861400\n",
        "121\t0.801100\n",
        "122\t0.853800\n",
        "123\t0.950900\n",
        "124\t0.887000\n",
        "125\t0.839900\n",
        "126\t0.776100\n",
        "127\t0.933800\n",
        "128\t0.830200\n",
        "129\t0.848700\n",
        "130\t0.900600\n",
        "131\t1.027800\n",
        "132\t0.914800\n",
        "133\t0.957300\n",
        "134\t0.819200\n",
        "135\t0.823600\n",
        "136\t0.808300\n",
        "137\t0.818900\n",
        "138\t0.917600\n",
        "139\t0.844400\n",
        "140\t0.791400\n",
        "141\t0.857100\n",
        "142\t0.956200\n",
        "143\t0.957300\n",
        "144\t0.973600\n",
        "145\t0.849700\n",
        "146\t1.001600\n",
        "147\t1.025000\n",
        "148\t1.016500\n",
        "149\t0.835200\n",
        "150\t0.836900\n",
        "151\t0.841800\n",
        "152\t0.818400\n",
        "153\t0.825400\n",
        "154\t0.879900\n",
        "155\t0.897900\n",
        "156\t0.801500\n",
        "157\t0.870200\n",
        "158\t0.922200\n",
        "159\t0.890400\n",
        "160\t0.743400\n",
        "161\t0.764000\n",
        "162\t0.695200\n",
        "163\t0.917700\n",
        "164\t0.851700\n",
        "165\t0.882200\n",
        "166\t0.753700\n",
        "167\t0.825900\n",
        "168\t0.898900\n",
        "169\t0.920600\n",
        "170\t0.884300\n",
        "171\t0.909300\n",
        "172\t0.732600\n",
        "173\t0.847500\n",
        "174\t0.937100\n",
        "175\t0.950600\n",
        "176\t0.929700\n",
        "177\t1.021600\n",
        "178\t0.880400\n",
        "179\t0.927700\n",
        "180\t0.825000\n",
        "181\t0.749900\n",
        "182\t0.825600\n",
        "183\t0.768000\n",
        "184\t0.816200\n",
        "185\t0.796400\n",
        "186\t0.941400\n",
        "187\t0.926100\n",
        "188\t0.897000\n",
        "189\t0.831500\n",
        "190\t0.863500\n",
        "191\t0.824700\n",
        "192\t0.784500\n",
        "193\t0.883100\n",
        "194\t0.811000\n",
        "195\t0.800800\n",
        "196\t0.886700\n",
        "197\t0.707500\n",
        "198\t0.765400\n",
        "199\t0.753300\n",
        "200\t0.714600\n",
        "201\t0.853700\n",
        "202\t0.720100\n",
        "203\t0.826300\n",
        "204\t0.887000\n",
        "205\t1.069500\n",
        "206\t0.777400\n",
        "207\t0.858500\n",
        "208\t0.811900\n",
        "209\t0.774000\n",
        "210\t0.826600\n",
        "211\t0.835300\n",
        "212\t0.785300\n",
        "213\t0.799400\n",
        "214\t0.803600\n",
        "215\t0.868600\n",
        "216\t0.831100\n",
        "217\t0.879400\n",
        "218\t0.735100\n",
        "219\t0.916000\n",
        "220\t0.775100\n",
        "221\t0.818400\n",
        "222\t0.742500\n",
        "223\t0.877300\n",
        "224\t0.826000\n",
        "225\t0.810700\n",
        "226\t0.783700\n",
        "227\t0.823000\n",
        "228\t0.727300\n",
        "229\t0.834300\n",
        "230\t0.957500\n",
        "231\t0.736800\n",
        "232\t0.867200\n",
        "233\t1.053500\n",
        "234\t0.951600\n",
        "235\t0.821500\n",
        "236\t0.812100\n",
        "237\t0.703800\n",
        "238\t0.766800\n",
        "239\t0.842500\n",
        "240\t0.797400\n",
        "241\t0.831700\n",
        "242\t0.834600\n",
        "243\t0.824700\n",
        "244\t0.763500\n",
        "245\t0.874800\n",
        "246\t0.817200\n",
        "247\t0.796100\n",
        "248\t0.714400\n",
        "249\t0.963000\n",
        "250\t0.878400\n",
        "251\t0.839400\n",
        "252\t0.811100\n",
        "253\t0.793000\n",
        "254\t0.688800\n",
        "255\t0.917300\n",
        "256\t0.740900\n",
        "257\t0.982000\n",
        "258\t0.766700\n",
        "259\t0.740800\n",
        "260\t0.763700\n",
        "261\t0.815800\n",
        "262\t0.748300\n",
        "263\t0.757600\n",
        "264\t0.828900\n",
        "265\t0.860100\n",
        "266\t0.800400\n",
        "267\t0.759100\n",
        "268\t0.672900\n",
        "269\t0.774800\n",
        "270\t0.768000\n",
        "271\t0.884800\n",
        "272\t0.801000\n",
        "273\t0.816200\n",
        "274\t0.708900\n",
        "275\t0.822200\n",
        "276\t0.686300\n",
        "277\t0.724100\n",
        "278\t0.848400\n",
        "279\t0.804300\n",
        "280\t0.795600\n",
        "281\t0.747500\n",
        "282\t0.865200\n",
        "283\t0.712500\n",
        "284\t0.803900\n",
        "285\t0.790500\n",
        "286\t0.818200\n",
        "287\t0.796300\n",
        "288\t0.916000\n",
        "289\t0.697400\n",
        "290\t0.773800\n",
        "291\t0.852300\n",
        "292\t0.922800\n",
        "293\t0.894100\n",
        "294\t0.975800\n",
        "295\t0.853300\n",
        "296\t0.806100\n",
        "297\t0.860200\n",
        "298\t0.723400\n",
        "299\t0.783600\n",
        "300\t0.776300\n",
        "301\t0.781700\n",
        "302\t0.695600\n",
        "303\t0.733100\n",
        "304\t0.864400\n",
        "305\t0.882600\n",
        "306\t0.768800\n",
        "307\t0.943500\n",
        "308\t0.722500\n",
        "309\t0.690700\n",
        "310\t0.846000\n",
        "311\t0.792800\n",
        "312\t0.663200\n",
        "313\t0.859700\n",
        "314\t0.779600\n",
        "315\t0.686600\n",
        "316\t0.828700\n",
        "317\t0.690900\n",
        "318\t0.703900\n",
        "319\t0.696100\n",
        "320\t0.860900\n",
        "321\t0.761900\n",
        "322\t0.702300\n",
        "323\t0.803500\n",
        "324\t0.646400\n",
        "325\t0.828700\n",
        "326\t0.816600\n",
        "327\t0.956200\n",
        "328\t0.822800\n",
        "329\t0.802200\n",
        "330\t0.738000\n",
        "331\t0.737600\n",
        "332\t0.728400\n",
        "333\t0.708100\n",
        "334\t0.821500\n",
        "335\t0.867600\n",
        "336\t0.793100\n",
        "337\t0.756000\n",
        "338\t0.849700\n",
        "339\t0.700900\n",
        "340\t0.743500\n",
        "341\t0.853300\n",
        "342\t0.777200\n",
        "343\t0.737600\n",
        "344\t0.797900\n",
        "345\t0.824700\n",
        "346\t0.650300\n",
        "347\t0.754900\n",
        "348\t0.754300\n",
        "349\t0.868600\n",
        "350\t0.817800\n",
        "351\t0.880600\n",
        "352\t0.722700\n",
        "353\t0.767100\n",
        "354\t0.930300\n",
        "355\t0.794600\n",
        "356\t0.703100\n",
        "357\t0.889000\n",
        "358\t0.786100\n",
        "359\t0.660000\n",
        "360\t0.777200\n",
        "361\t0.721400\n",
        "362\t0.856300\n",
        "363\t0.834400\n",
        "364\t0.822400\n",
        "365\t0.836700\n",
        "366\t0.793500\n",
        "367\t0.773300\n",
        "368\t0.799900\n",
        "369\t0.773400\n",
        "370\t0.952400\n",
        "371\t0.789000\n",
        "372\t0.808900\n",
        "373\t0.795800\n",
        "374\t0.716400\n",
        "375\t0.963600\n",
        "376\t0.776100\n",
        "377\t0.793400\n",
        "378\t0.870600\n",
        "379\t0.806600\n",
        "380\t0.645300\n",
        "381\t0.677200\n",
        "382\t0.868700\n",
        "383\t0.728400\n",
        "384\t0.709300\n",
        "385\t0.823300\n",
        "386\t0.734000\n",
        "387\t0.847200\n",
        "388\t0.788700\n",
        "389\t0.847100\n",
        "390\t0.763400\n",
        "391\t0.865300\n",
        "392\t0.704900\n",
        "393\t0.857700\n",
        "394\t0.865500\n",
        "395\t0.752800\n",
        "396\t0.845600\n",
        "397\t0.713800\n",
        "398\t0.863200\n",
        "399\t0.665700\n",
        "400\t0.767500\n",
        "401\t0.781200\n",
        "402\t0.787000\n",
        "403\t0.762100\n",
        "404\t0.709100\n",
        "405\t0.773900\n",
        "406\t0.681300\n",
        "407\t0.866500\n",
        "408\t0.708000\n",
        "409\t0.767600\n",
        "410\t0.773600\n",
        "411\t0.779200\n",
        "412\t0.767500\n",
        "413\t0.718100\n",
        "414\t0.750000\n",
        "415\t0.716900\n",
        "416\t0.720500\n",
        "417\t0.724900\n",
        "418\t0.736300\n",
        "419\t0.765100\n",
        "420\t0.804500\n",
        "421\t0.733500\n",
        "422\t0.836800\n",
        "423\t0.801900\n",
        "424\t0.749300\n",
        "425\t0.736300\n",
        "426\t0.783100\n",
        "427\t0.692200\n",
        "428\t0.881700\n",
        "429\t0.803100\n",
        "430\t0.713600\n",
        "431\t0.891400\n",
        "432\t0.745300\n",
        "433\t0.721800\n",
        "434\t0.859000\n",
        "435\t0.688200\n",
        "436\t0.796300\n",
        "437\t0.638300\n",
        "438\t0.817100\n",
        "439\t0.775900\n",
        "440\t0.793900\n",
        "441\t0.719500\n",
        "442\t0.673500\n",
        "443\t0.707300\n",
        "444\t0.694600\n",
        "445\t0.648500\n",
        "446\t0.732200\n",
        "447\t0.789600\n",
        "448\t0.688900\n",
        "449\t0.734000\n",
        "450\t0.755500\n",
        "451\t0.863800\n",
        "452\t0.753000\n",
        "453\t0.802000\n",
        "454\t0.771800\n",
        "455\t0.777300\n",
        "456\t0.763400\n",
        "457\t0.693300\n",
        "458\t0.791300\n",
        "459\t0.644600\n",
        "460\t0.884700\n",
        "461\t0.778600\n",
        "462\t0.693600\n",
        "463\t0.882000\n",
        "464\t0.787100\n",
        "465\t0.813500\n",
        "466\t0.821600\n",
        "467\t0.727000\n",
        "468\t0.869700\n",
        "469\t0.694700\n",
        "470\t0.794300\n",
        "471\t0.758000\n",
        "472\t0.687000\n",
        "473\t0.762100\n",
        "474\t0.819900\n",
        "475\t0.765400\n",
        "476\t0.971300\n",
        "477\t0.792500\n",
        "478\t0.733300\n",
        "479\t0.900800\n",
        "480\t0.841400\n",
        "481\t0.728000\n",
        "482\t0.720300\n",
        "483\t0.840800\n",
        "484\t0.732800\n",
        "485\t0.707200\n",
        "486\t0.753000\n",
        "487\t0.686000\n",
        "488\t0.835700\n",
        "489\t0.795100\n",
        "490\t0.789300\n",
        "491\t0.709600\n",
        "492\t0.761400\n",
        "493\t0.706800\n",
        "494\t0.738600\n",
        "495\t0.782100\n",
        "496\t0.787200\n",
        "497\t0.758100\n",
        "498\t0.702000\n",
        "499\t0.756600\n",
        "500\t0.877900\n",
        "501\t0.735100\n",
        "502\t0.771700\n",
        "503\t0.690000\n",
        "504\t0.802200\n",
        "505\t0.708100\n",
        "506\t0.730600\n",
        "507\t0.806700\n",
        "508\t0.609600\n",
        "509\t0.660200\n",
        "510\t0.780800\n",
        "511\t0.760200\n",
        "512\t0.752300\n",
        "513\t0.809300\n",
        "514\t0.726300\n",
        "515\t0.786900\n",
        "516\t0.750600\n",
        "517\t0.812200\n",
        "518\t0.717500\n",
        "519\t0.798900\n",
        "520\t0.722500\n",
        "521\t0.755800\n",
        "522\t0.738700\n",
        "523\t0.789400\n",
        "524\t0.747900\n",
        "525\t0.664000\n",
        "526\t0.706500\n",
        "527\t0.750500\n",
        "528\t0.755100\n",
        "529\t0.870700\n",
        "530\t0.634700\n",
        "531\t0.780000\n",
        "532\t0.685000\n",
        "533\t0.668600\n",
        "534\t0.610100\n",
        "535\t0.759100\n",
        "536\t0.760600\n",
        "537\t0.715100\n",
        "538\t0.709200\n",
        "539\t0.688800\n",
        "540\t0.798200\n",
        "541\t0.792300\n",
        "542\t0.724400\n",
        "543\t0.620700\n",
        "544\t0.745600\n",
        "545\t0.765200\n",
        "546\t0.631600\n",
        "547\t0.788100\n",
        "548\t0.721700\n",
        "549\t0.743200\n",
        "550\t0.800500\n",
        "551\t0.824600\n",
        "552\t0.718000\n",
        "553\t0.709900\n",
        "554\t0.676200\n",
        "555\t0.700700\n",
        "556\t0.718200\n",
        "557\t0.649100\n",
        "558\t0.839800\n",
        "559\t0.739100\n",
        "560\t0.742600\n",
        "561\t0.735200\n",
        "562\t0.732000\n",
        "563\t0.755300\n",
        "564\t0.743200\n",
        "565\t0.680000\n",
        "566\t0.674000\n",
        "567\t0.637500\n",
        "568\t0.788100\n",
        "569\t0.872000\n",
        "570\t0.756800\n",
        "571\t0.809200\n",
        "572\t0.660400\n",
        "573\t0.693800\n",
        "574\t0.846300\n",
        "575\t0.820200\n",
        "576\t0.680700\n",
        "577\t0.758900\n",
        "578\t0.775500\n",
        "579\t0.663600\n",
        "580\t0.744300\n",
        "581\t0.751900\n",
        "582\t0.629600\n",
        "583\t0.909800\n",
        "584\t0.625400\n",
        "585\t0.610800\n",
        "586\t0.757600\n",
        "587\t0.753700\n",
        "588\t0.794900\n",
        "589\t0.741000\n",
        "590\t0.675900\n",
        "591\t0.708800\n",
        "592\t0.829300\n",
        "593\t0.703700\n",
        "594\t0.859100\n",
        "595\t0.711600\n",
        "596\t0.846500\n",
        "597\t0.756500\n",
        "598\t0.823400\n",
        "599\t0.625000\n",
        "600\t0.699400\n",
        "601\t0.686900\n",
        "602\t0.792500\n",
        "603\t0.670200\n",
        "604\t0.774400\n",
        "605\t0.789800\n",
        "606\t0.748000\n",
        "607\t0.809700\n",
        "608\t0.805400\n",
        "609\t0.771700\n",
        "610\t0.735500\n",
        "611\t0.702500\n",
        "612\t0.635500\n",
        "613\t0.786700\n",
        "614\t0.673800\n",
        "615\t0.826200\n",
        "616\t0.871100\n",
        "617\t0.811900\n",
        "618\t0.664100\n",
        "619\t0.756400\n",
        "620\t0.839100\n",
        "621\t0.896200\n",
        "622\t0.702600\n",
        "623\t0.756000\n",
        "624\t0.694700\n",
        "625\t0.708300\n",
        "626\t0.819800\n",
        "627\t0.820400\n",
        "628\t0.636000\n",
        "629\t0.753100\n",
        "630\t0.739500\n",
        "631\t0.871600\n",
        "632\t0.657200\n",
        "633\t0.572300\n",
        "634\t0.608500\n",
        "635\t0.661800\n",
        "636\t0.704300\n",
        "637\t0.807600\n",
        "638\t0.703100\n",
        "639\t0.752600\n",
        "640\t0.722300\n",
        "641\t0.797100\n",
        "642\t0.785800\n",
        "643\t0.684500\n",
        "644\t0.742200\n",
        "645\t0.801400\n",
        "646\t0.855200\n",
        "647\t0.705100\n",
        "648\t0.729500\n",
        "649\t0.718300\n",
        "650\t0.681000\n",
        "651\t0.757700\n",
        "652\t0.657800\n",
        "653\t0.798000\n",
        "654\t0.662800\n",
        "655\t0.635200\n",
        "656\t0.865000\n",
        "657\t0.707300\n",
        "658\t0.675700\n",
        "659\t0.684200\n",
        "660\t0.695200\n",
        "661\t0.755400\n",
        "662\t0.739800\n",
        "663\t0.813400\n",
        "664\t0.777500\n",
        "665\t0.683000\n",
        "666\t0.792700\n",
        "667\t0.869400\n",
        "668\t0.758100\n",
        "669\t0.748100\n",
        "670\t0.818800\n",
        "671\t0.747000\n",
        "672\t0.655600\n",
        "673\t0.753200\n",
        "674\t0.741200\n",
        "675\t0.933100\n",
        "676\t0.712900\n",
        "677\t0.682600\n",
        "678\t0.739800\n",
        "679\t0.807300\n",
        "680\t0.823000\n",
        "681\t0.832200\n",
        "682\t0.811100\n",
        "683\t0.708800\n",
        "684\t0.829600\n",
        "685\t0.796900\n",
        "686\t0.820100\n",
        "687\t0.794100\n",
        "688\t0.788300\n",
        "689\t0.843000\n",
        "690\t0.671500\n",
        "691\t0.689700\n",
        "692\t0.728300\n",
        "693\t0.813600\n",
        "694\t0.680700\n",
        "695\t0.710800\n",
        "696\t0.905900\n",
        "697\t0.735600\n",
        "698\t0.694100\n",
        "699\t0.714200\n",
        "700\t0.743300\n",
        "701\t0.685600\n",
        "702\t0.666400\n",
        "703\t0.860900\n",
        "704\t0.842600\n",
        "705\t0.711700\n",
        "706\t0.636500\n",
        "707\t0.886600\n",
        "708\t0.688800\n",
        "709\t0.665100\n",
        "710\t0.614400\n",
        "711\t0.671700\n",
        "712\t0.691500\n",
        "713\t0.854600\n",
        "714\t0.741000\n",
        "715\t0.786300\n",
        "716\t0.770400\n",
        "717\t0.777800\n",
        "718\t0.706500\n",
        "719\t0.743800\n",
        "720\t0.611800\n",
        "721\t0.659500\n",
        "722\t0.820800\n",
        "723\t0.771000\n",
        "724\t0.823200\n",
        "725\t0.831600\n",
        "726\t0.748800\n",
        "727\t0.673800\n",
        "728\t0.646800\n",
        "729\t0.708900\n",
        "730\t0.712700\n",
        "731\t0.723300\n",
        "732\t0.642200\n",
        "733\t0.699100\n",
        "734\t0.781600\n",
        "735\t0.852700\n",
        "736\t0.723300\n",
        "737\t0.678900\n",
        "738\t0.750700\n",
        "739\t0.736900\n",
        "740\t0.928700\n",
        "741\t0.777400\n",
        "742\t0.683000\n",
        "743\t0.759100\n",
        "744\t0.640900\n",
        "745\t0.828400\n",
        "746\t0.789400\n",
        "747\t0.739500\n",
        "748\t0.697200\n",
        "749\t0.859700\n",
        "750\t0.824400\n",
        "751\t0.695500\n",
        "752\t0.677800\n",
        "753\t0.796000\n",
        "754\t0.671800\n",
        "755\t0.772400\n",
        "756\t0.698200\n",
        "757\t0.676200\n",
        "758\t0.723800\n",
        "759\t0.696400\n",
        "760\t0.640500\n",
        "761\t0.809900\n",
        "762\t0.620700\n",
        "763\t0.696800\n",
        "764\t0.756900\n",
        "765\t0.937100\n",
        "766\t0.720100\n",
        "767\t0.713800\n",
        "768\t0.731900\n",
        "769\t0.717300\n",
        "770\t0.818700\n",
        "771\t0.833500\n",
        "772\t0.755400\n",
        "773\t0.735600\n",
        "774\t0.842500\n",
        "775\t0.725700\n",
        "776\t0.705400\n",
        "777\t0.687200\n",
        "778\t0.790400\n",
        "779\t0.670200\n",
        "780\t0.609100\n",
        "781\t0.767500\n",
        "782\t0.696000\n",
        "783\t0.656800\n",
        "784\t0.732700\n",
        "785\t0.777600\n",
        "786\t0.729700\n",
        "787\t0.789800\n",
        "788\t0.731800\n",
        "789\t0.754200\n",
        "790\t0.701100\n",
        "791\t0.843900\n",
        "792\t0.654200\n",
        "793\t0.759700\n",
        "794\t0.757300\n",
        "795\t0.676800\n",
        "796\t0.624200\n",
        "797\t0.652800\n",
        "798\t0.768000\n",
        "799\t0.631800\n",
        "800\t0.793500\n",
        "801\t0.673900\n",
        "802\t0.754700\n",
        "803\t0.579600\n",
        "804\t0.764400\n",
        "805\t0.716600\n",
        "806\t0.756100\n",
        "807\t0.715600\n",
        "808\t0.672300\n",
        "809\t0.731700\n",
        "810\t0.702700\n",
        "811\t0.695300\n",
        "812\t0.766100\n",
        "813\t0.683500\n",
        "814\t0.790800\n",
        "815\t0.724800\n",
        "816\t0.732200\n",
        "817\t0.628700\n",
        "818\t0.761300\n",
        "819\t0.720600\n",
        "820\t0.811000\n",
        "821\t0.703900\n",
        "822\t0.708900\n",
        "823\t0.694900\n",
        "824\t0.719300\n",
        "825\t0.773700\n",
        "826\t0.810500\n",
        "827\t0.850800\n",
        "828\t0.689400\n",
        "829\t0.621900\n",
        "830\t0.781500\n",
        "831\t0.812600\n",
        "832\t0.790300\n",
        "833\t0.855100\n",
        "834\t0.804200\n",
        "835\t0.719700\n",
        "836\t0.651500\n",
        "837\t0.670100\n",
        "838\t0.768600\n",
        "839\t0.669700\n",
        "840\t0.755100\n",
        "841\t0.710100\n",
        "842\t0.716100\n",
        "843\t0.731600\n",
        "844\t0.747200\n",
        "845\t0.716600\n",
        "846\t0.681800\n",
        "847\t0.718100\n",
        "848\t0.792700\n",
        "849\t0.681500\n",
        "850\t0.696900\n",
        "851\t0.698000\n",
        "852\t0.801500\n",
        "853\t0.772000\n",
        "854\t0.687800\n",
        "855\t0.720800\n",
        "856\t0.750600\n",
        "857\t0.698200\n",
        "858\t0.795800\n",
        "859\t0.714900\n",
        "860\t0.697400\n",
        "861\t0.726400\n",
        "862\t0.705300\n",
        "863\t0.610300\n",
        "864\t0.661300\n",
        "865\t0.728300\n",
        "866\t0.676600\n",
        "867\t0.753300\n",
        "868\t0.635100\n",
        "869\t0.685000\n",
        "870\t0.705300\n",
        "871\t0.739300\n",
        "872\t0.830100\n",
        "873\t0.677200\n",
        "874\t0.842800\n",
        "875\t0.870000\n",
        "876\t0.634700\n",
        "877\t0.785400\n",
        "878\t0.687800\n",
        "879\t0.637200\n",
        "880\t0.724700\n",
        "881\t0.773600\n",
        "882\t0.682600\n",
        "883\t0.737500\n",
        "884\t0.641300\n",
        "885\t0.735100\n",
        "886\t0.577400\n",
        "887\t0.654600\n",
        "888\t0.732100\n",
        "889\t0.680800\n",
        "890\t0.715000\n",
        "891\t0.768400\n",
        "892\t0.615500\n",
        "893\t0.617200\n",
        "894\t0.729900\n",
        "895\t0.739600\n",
        "896\t0.634000\n",
        "897\t0.688900\n",
        "898\t0.686400\n",
        "899\t0.749300\n",
        "900\t0.633400\n",
        "901\t0.624500\n",
        "902\t0.706600\n",
        "903\t0.815000\n",
        "904\t0.781800\n",
        "905\t0.768600\n",
        "906\t0.711800\n",
        "907\t0.792500\n",
        "908\t0.726800\n",
        "909\t0.773300\n",
        "910\t0.681200\n",
        "911\t0.797900\n",
        "912\t0.668200\n",
        "913\t0.707600\n",
        "914\t0.679600\n",
        "915\t0.735500\n",
        "916\t0.660500\n",
        "917\t0.714100\n",
        "918\t0.707700\n",
        "919\t0.715200\n",
        "920\t0.711400\n",
        "921\t0.664500\n",
        "922\t0.631900\n",
        "923\t0.739600\n",
        "924\t0.677000\n",
        "925\t0.781900\n",
        "926\t0.649200\n",
        "927\t0.684200\n",
        "928\t0.918500\n",
        "929\t0.683300\n",
        "930\t0.692600\n",
        "931\t0.697700\n",
        "932\t0.740300\n",
        "933\t0.688000\n",
        "934\t0.827200\n",
        "935\t0.775900\n",
        "936\t0.714600\n",
        "937\t0.686900\n",
        "938\t0.719200\n",
        "939\t0.788600\n",
        "940\t0.687700\n",
        "941\t0.827000\n",
        "942\t0.736200\n",
        "943\t0.753500\n",
        "944\t0.665300\n",
        "945\t0.832100\n",
        "946\t0.683900\n",
        "947\t0.697500\n",
        "948\t0.717000\n",
        "949\t0.690100\n",
        "950\t0.665600\n",
        "951\t0.740500\n",
        "952\t0.677300\n",
        "953\t0.887900\n",
        "954\t0.660600\n",
        "955\t0.785100\n",
        "956\t0.809700\n",
        "957\t0.853900\n",
        "958\t0.687900\n",
        "959\t0.683800\n",
        "960\t0.676500\n",
        "961\t0.717600\n",
        "962\t0.668700\n",
        "963\t0.770300\n",
        "964\t0.726000\n",
        "965\t0.607600\n",
        "966\t0.754900\n",
        "967\t0.626500\n",
        "968\t0.635600\n",
        "969\t0.650000\n",
        "970\t0.630700\n",
        "971\t0.723800\n",
        "972\t0.681500\n",
        "973\t0.936000\n",
        "974\t0.736100\n",
        "975\t0.802000\n",
        "976\t0.741100\n",
        "977\t0.651700\n",
        "978\t0.681700\n",
        "979\t0.849300\n",
        "980\t0.624700\n",
        "981\t0.728900\n",
        "982\t0.727200\n",
        "983\t0.701400\n",
        "984\t0.714000\n",
        "985\t0.649300\n",
        "986\t0.651300\n",
        "987\t0.664200\n",
        "988\t0.820200\n",
        "989\t0.790700\n",
        "990\t0.713700\n",
        "991\t0.703100\n",
        "992\t0.732100\n",
        "993\t0.676800\n",
        "994\t0.836000\n",
        "995\t0.647400\n",
        "996\t0.722200\n",
        "997\t0.712400\n",
        "998\t0.783900\n",
        "999\t0.824200\n",
        "1000\t0.747000\n",
        "\"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Parse the training loss values\n",
        "lines = training_loss_values.strip().split(\"\\n\")\n",
        "epochs = []\n",
        "loss_values = []\n",
        "\n",
        "for line in lines:\n",
        "    epoch, loss = line.split()\n",
        "    epochs.append(int(epoch))\n",
        "    loss_values.append(float(loss))\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(epochs, loss_values, label=\"Training Loss\", color='b')\n",
        "\n",
        "# Format the y-axis to show four decimal places\n",
        "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:.4f}'))\n",
        "\n",
        "# Labels and title\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss Over Epochs\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hN9fZeqm1Do0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}