{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "mount_file_id": "https://github.com/kmk4444/System_engineering/blob/main/system_eng_part7.ipynb",
      "authorship_tag": "ABX9TyMVP1nuwrIji7zLsi7m0TC8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmk4444/System_engineering/blob/main/system_eng_part7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctlHXVpSSUxi"
      },
      "outputs": [],
      "source": [
        "pip install langchain openai pypdf chroma streamlit langchain_openai langchain_community langchain transformers bitsandbytes accelerate torch faiss-gpu faiss-cpu langchain_chroma langchain_experimental sentence-transformers cohere rank_bm25 nltk scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade huggingface-hub transformers"
      ],
      "metadata": {
        "id": "XSYPGJ7Qfcd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import pipeline\n",
        "\n",
        "login(token = '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')"
      ],
      "metadata": {
        "id": "bkfRKUbXRccW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app_v10.py\n",
        "import streamlit as st\n",
        "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema.document import Document\n",
        "from langchain.vectorstores.chroma import Chroma\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import os\n",
        "import shutil\n",
        "from langchain_core.documents import Document\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "from langchain_community.embeddings import CohereEmbeddings\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "\n",
        "templates = {\n",
        "    \"system\": \"You are a professional prompt engineer. Apply the mentioned prompt engineering technique and provide ONLY the improved prompt without any additional commentary or explanations.\",\n",
        "\n",
        "    \"system_multiple\": \"You are a professional prompt engineer. Thoroughly apply EVERY prompt engineering technique listed in the [Prompt Engineering Techniques to Apply] section. Use these techniques to enhance the original prompt provided below, ensuring the enhancement is clear and effective. Provide ONLY the improved version of the prompt without any additional commentary or explanations.\",\n",
        "\n",
        "    \"lang_default\": \"Identify the language of the user's original prompt in the [original] section. You MUST provide the enhanced version of the prompt in the **same language** as the user's original prompt. You'll be penalized if you translate it into another language unless explicitly requested by the user.\",\n",
        "\n",
        "    \"lang_eng\": \"If the original prompt is not in English, first translate it into English before proceeding with the improvement process.\",\n",
        "\n",
        "    \"deeper_understanding\" : \"When you need clarity or a deeper understanding of a topic, idea, or any piece of information, utilize the following prompts: - Explain [insert specific topic] in simple terms. - Explain to me like I'm 11 years old. - Explain to me as if I'm a beginner in [field]. - Explain to me as if I'm an expert in [field]. - -Write the [essay/text/paragraph] using simple English like you're explaining something to a 5-year-old.\",\n",
        "\n",
        "    \"deeper_understanding_simpler\": \"Explain to me as if Iâ€™m a beginner in System Engineering. Example: Change \\\"Explain system architecture.\\\" to \\\"Explain system architecture to beginners.\\\"\",\n",
        "\n",
        "    \"task_decomposition\": \"For complex or multi-step tasks, divide the original prompt into a series of simpler, more manageable sub-prompts. This approach allows the model to focus on one part of the task at a time, generating more detailed and coherent responses for each step.\\n\\nHere are examples demonstrating how to apply this principle:\\n\\n[original]\\nCreate a short story about a character who discovers an old, mysterious book that grants them extraordinary powers.\\n[improved]\\n1. Create a short story about a character who discovers an old, mysterious book that grants them extraordinary powers. first: Introduce the protagonist, ordinary life, and setting.\\n2. Describe discovering the book and the character's growing powers.\\n3. present challenges from powers and character's growth.Resolve conflicts, and show the character's reflection on the journey.\\n4. write the short story.\\n\\n========\\nBased on this approach, refine the following prompt to enable its breakdown into a series of simpler, step-by-step tasks:\\n\\n[original]\\n{prompt}\\n[improved]\\n\",\n",
        "\n",
        "    \"task_decomposition_simpler\": \"For complex or multi-step tasks, divide the original prompt into a series of simpler, more manageable sub-prompts. This approach allows the model to focus on one part of the task at a time, generating more detailed and coherent responses for each step.\",\n",
        "\n",
        "    \"fewshot_prompting\": \"Improve the original prompt by adding a couple of relevant examples that demonstrate the kind of answer or information being requested. Incorporate those examples smoothly into the prompt to make the desired response clear.\\n\\nHere are examples demonstrating how to apply this principle:\\n\\n[original]\\nClassify the emotion of the following text as positive, negative, or neutral. \\n\\\"It's a good day to try something fun.\\\" \\n[improved]\\nClassify the emotion of the following text as positive, negative, or neutral. \\nExample1: \\\"This music is awesome.\\\" (Positive) \\nExample2: \\\"I don't like spicy flavors.\\\" (Negative)\\nExample3: \\\"Every flower blooms at a different pace.\\\" (Neutral)\\nQuestion: \\\"It's a good day to try something fun.\\\" \\n\\n========\\nBased on this approach, refine the following prompt.\\n\\n[original]\\n{prompt}\\n[improved]\\n\",\n",
        "\n",
        "    \"fewshot_prompting_simpler\": \"Improve the original prompt by adding a couple of relevant examples that demonstrate the kind of answer or information being requested. Incorporate those examples smoothly into the prompt to make the desired response clear.\"\n",
        "}\n",
        "\n",
        "my_key_cohere=\"RsdlzOlRrQmvf5bxQrUHDpfbeg7Fvfr7WJwGCec1\"\n",
        "\n",
        "# Set up Streamlit page configuration\n",
        "st.set_page_config(page_title=\"ðŸ¦™ðŸ’¬ Sistem MÃ¼hendisliÄŸi GiriÅŸ Chatbot\")\n",
        "\n",
        "# Constants\n",
        "DATA_PATH = \"/content/drive/MyDrive/data\"  # PDF dosyalarÄ±nÄ±n bulunduÄŸu dizin\n",
        "CHROMA_PATH = \"chroma\"  # Chroma veritabanÄ±nÄ±n saklanacaÄŸÄ± dizin\n",
        "\n",
        "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "# Initialize Embeddings\n",
        "def initialize_embeddings():\n",
        "    return HuggingFaceInferenceAPIEmbeddings(\n",
        "        api_key='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',\n",
        "        model_name=\"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "    )\n",
        "\n",
        "embeddings = initialize_embeddings()\n",
        "\n",
        "# Load and Split Documents\n",
        "def load_and_split_documents(data_path, embeddings):\n",
        "    document_loader = PyPDFDirectoryLoader(data_path)\n",
        "    raw_documents = document_loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=800,\n",
        "        chunk_overlap=100,\n",
        "        length_function=len\n",
        "    )\n",
        "    return text_splitter.split_documents(raw_documents)\n",
        "\n",
        "# Create Custom Documents\n",
        "def create_custom_documents(splitted_documents):\n",
        "    custom_documents = []\n",
        "    for i, raw_doc in enumerate(splitted_documents):\n",
        "        new_doc = Document(\n",
        "                page_content=raw_doc.page_content,\n",
        "                metadata={\n",
        "                    \"source\": raw_doc.metadata.get(\"source\", \"Unknown Source\"),\n",
        "                    \"title\": raw_doc.metadata.get(\"title\", \"No Title\"),\n",
        "                    \"description\": raw_doc.metadata.get(\"description\", \"No Description\"),\n",
        "                    \"language\": raw_doc.metadata.get(\"language\", \"Unknown Language\"),\n",
        "                    \"doc_id\": i\n",
        "                }\n",
        "        )\n",
        "        custom_documents.append(new_doc)\n",
        "    return custom_documents\n",
        "\n",
        "# Initialize Vectorstore and Retriever\n",
        "def initialize_vectorstore(custom_documents, embeddings, persist_directory):\n",
        "    try:\n",
        "        # Chroma'ya belgeleri ve embeddings'i geÃ§irin\n",
        "        vectorstore = Chroma.from_documents(\n",
        "            custom_documents,\n",
        "            embeddings,\n",
        "            persist_directory=persist_directory\n",
        "        )\n",
        "\n",
        "        return vectorstore.as_retriever(\n",
        "            search_type=\"mmr\",\n",
        "            search_kwargs={'k': 8, 'lambda_mult': 0.75}\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing vectorstore: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Retrieve Relevant Documents\n",
        "def retrieve_relevant_documents(retriever, prompt):\n",
        "    return retriever.get_relevant_documents(prompt)\n",
        "\n",
        "\n",
        "def get_relevant_documents_with_bm25(documents, query):\n",
        "\n",
        "    bm25_retriever = BM25Retriever.from_documents(documents=documents)\n",
        "    bm25_retriever.k = 4\n",
        "\n",
        "    bm25_relevant_documents = bm25_retriever.get_relevant_documents(query=query)\n",
        "\n",
        "    return bm25_relevant_documents, bm25_retriever\n",
        "\n",
        "def get_relevant_documents_for_hybrid_search(query, retriever1, retriever2, weight1, weight2):\n",
        "\n",
        "\n",
        "    ensemble_retriever = EnsembleRetriever(\n",
        "                                retrievers=[retriever1, retriever2],\n",
        "                                weights=[weight1, weight2])\n",
        "\n",
        "    hybrid_relevant_documents = ensemble_retriever.get_relevant_documents(query)\n",
        "\n",
        "    return hybrid_relevant_documents\n",
        "\n",
        "# Generate Final Prompt\n",
        "def generate_final_prompt(prompt, context_data, chat_history, relevant_documents):\n",
        "    history_prompt = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in chat_history])\n",
        "    metadata_info = \"\\n\".join([f\"Belge {doc.metadata['doc_id']} - BaÅŸlÄ±k: {doc.metadata['title']}, Kaynak: {doc.metadata['source']}\" for doc in relevant_documents])\n",
        "    return  f\"\"\"\n",
        "    ###Instruction###\n",
        "    You are an expert assistant dedicated to providing answers for beginner systems engineers. Follow these guidelines:\n",
        "\n",
        "    1. Deliver clear, concise, and expert-level information in Turkish.\n",
        "    2. Use the provided documents (###Context###) and historical conversation (###Previous Conversations###) data to answer questions (###Question###). Reference these documents while interpreting and generating your answers.\n",
        "    3. Ensure that your answers are coherent and free from any repetitive or duplicate content.\n",
        "    4. Each sentence should provide unique and valuable information.\n",
        "    5. You have to answer just in TURKISH.\n",
        "\n",
        "    ###Previous Conversations###\n",
        "    {history_prompt}\n",
        "\n",
        "    ###Question###\n",
        "    {prompt}\n",
        "\n",
        "    ###Context###\n",
        "    This is the information we have to answer the question: {context_data}\n",
        "    \"\"\", metadata_info\n",
        "\n",
        "# Post-processing fonksiyonu: cosine similarity ile benzer cÃ¼mleleri kaldÄ±rÄ±r\n",
        "def remove_repetitions(response, threshold=0.8):\n",
        "    # CÃ¼mlelere bÃ¶lme\n",
        "    sentences = sent_tokenize(response)\n",
        "\n",
        "    # TF-IDF vektÃ¶rizer oluÅŸturma\n",
        "    vectorizer = TfidfVectorizer().fit_transform(sentences)\n",
        "    vectors = vectorizer.toarray()\n",
        "\n",
        "    # Cosine similarity hesaplama\n",
        "    cosine_matrix = cosine_similarity(vectors)\n",
        "\n",
        "    # Tekrar eden cÃ¼mleleri tespit etme ve kaldÄ±rma\n",
        "    unique_sentences = []\n",
        "    seen_indices = set()\n",
        "\n",
        "    for i in range(len(sentences)):\n",
        "        if i in seen_indices:\n",
        "            continue\n",
        "        unique_sentences.append(sentences[i])\n",
        "        for j in range(i + 1, len(sentences)):\n",
        "            if cosine_matrix[i, j] > threshold:\n",
        "                seen_indices.add(j)\n",
        "\n",
        "    # TekrarlarÄ± kaldÄ±rÄ±lmÄ±ÅŸ yanÄ±tÄ± yeniden oluÅŸturma\n",
        "    cleaned_response = ' '.join(unique_sentences)\n",
        "\n",
        "    return cleaned_response\n",
        "\n",
        "# RAG with Multiple PDFs\n",
        "def rag_with_multiple_pdfs(prompt, chat_history):\n",
        "    splitted_documents = load_and_split_documents(DATA_PATH, embeddings)\n",
        "    custom_documents = create_custom_documents(splitted_documents)\n",
        "    chroma_retriever = initialize_vectorstore(custom_documents, embeddings, CHROMA_PATH)\n",
        "    chroma_relevant_documents = retrieve_relevant_documents(chroma_retriever, prompt)\n",
        "\n",
        "    bm25_documents, bm25retriever = get_relevant_documents_with_bm25(custom_documents,prompt)\n",
        "\n",
        "    weight1=0.2\n",
        "    hybrid_search_documents = get_relevant_documents_for_hybrid_search(\n",
        "                                                        query=prompt,\n",
        "                                                        retriever1=bm25retriever,\n",
        "                                                        retriever2=chroma_retriever,\n",
        "                                                        weight1=weight1,\n",
        "                                                        weight2=1-weight1)\n",
        "\n",
        "\n",
        "    context_data = \" \".join([doc.page_content for doc in hybrid_search_documents])\n",
        "    cleaned_context_data = remove_repetitions(context_data)\n",
        "    final_prompt, metadata_info = generate_final_prompt(prompt, cleaned_context_data, chat_history, hybrid_search_documents)\n",
        "    return final_prompt, metadata_info, chroma_relevant_documents, bm25_documents, hybrid_search_documents\n",
        "\n",
        "# Initialize Model\n",
        "def initialize_model():\n",
        "    model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb_config\n",
        "    )\n",
        "    return tokenizer, model\n",
        "\n",
        "tokenizer, model = initialize_model()\n",
        "\n",
        "\n",
        "def generate_prompt_engineering(prompt, tokenizer, model):\n",
        "    system_message = templates[\"system_multiple\"]\n",
        "    system_message += '\\n' + templates[\"lang_eng\"]\n",
        "\n",
        "\n",
        "    skills = [\"deeper_understanding\",\"task_decomposition\",\"fewshot_prompting\"]\n",
        "    integrated_templates = \"[Prompt Engineering Techniques to Apply]\\n\"\n",
        "\n",
        "    for idx, skill in enumerate(skills):\n",
        "        template = templates[f\"{skill}_simpler\"]\n",
        "        integrated_templates += f\"{idx+1}. {skill}: {template}\\n\"\n",
        "    integrated_templates += \"Based on [Prompt engineering techniques to apply], refine the prompt provided below. Ensure that each technique is fully incorporated to achieve a clear and effective improvement:\\n\\n[original]\\n{prompt}\\n[improved]\\n\"\n",
        "\n",
        "    prompt_template = PromptTemplate.from_template(integrated_templates)\n",
        "    formatted_input = prompt_template.format(prompt=prompt)\n",
        "\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": system_message}, {\"role\": \"user\", \"content\": formatted_input}]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=1024,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        temperature=0.5,\n",
        "        top_p=1,\n",
        "    )\n",
        "\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "\n",
        "# Generate LLaMA3 Response\n",
        "def generate_llama3_response(prompt_input, tokenizer, model):\n",
        "    SYS_PROMPT = \"\"\"You are an expert assistant dedicated to guiding and empowering beginner systems engineers. Follow these guidelines:\n",
        "    1. Provide clear, concise, and expert-level information in Turkish.\n",
        "    2. Use the provided documents and historical conversation data to deliver insightful answers. Previous conversation history and document information will be given to you in the user's prompt.\n",
        "    3. Ensure that your answers are free from any repetitive or duplicate content and sentence. You have to eliminate repetitive sentences. DON'T REPEAT THE SAME SENTENCES!\n",
        "    4. If unsure, confidently state \"Bilmiyorum\" without speculating.\n",
        "    5. Your responses should be motivational and tailored to empower beginners in understanding systems engineering principles.\n",
        "    6. Only respond in TURKISH.\"\"\"\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": SYS_PROMPT}, {\"role\": \"user\", \"content\": prompt_input}]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=1024,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        temperature=0.5,\n",
        "        top_p=1,\n",
        "    )\n",
        "\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "# Initialize session state\n",
        "if 'logged_in' not in st.session_state:\n",
        "    st.session_state['logged_in'] = False\n",
        "\n",
        "# Define user credentials\n",
        "USER_CREDENTIALS = {\"admin\": \"123456\"}\n",
        "\n",
        "# Main Function for Ana Sayfa\n",
        "def main():\n",
        "    if not st.session_state['logged_in']:\n",
        "        login()\n",
        "    else:\n",
        "        show_chat_app()\n",
        "\n",
        "\n",
        "# Function for login\n",
        "def login():\n",
        "    st.title(\"Sistem MÃ¼hendisliÄŸi Chatbot GiriÅŸ EkranÄ±\")\n",
        "\n",
        "    if st.session_state['logged_in']:\n",
        "        st.success(\"BaÅŸarÄ±yla giriÅŸ yaptÄ±nÄ±z\")\n",
        "        return\n",
        "\n",
        "    username = st.text_input(\"KullanÄ±cÄ± AdÄ±\")\n",
        "    password = st.text_input(\"Åžifre\", type=\"password\")\n",
        "\n",
        "    if st.button(\"GiriÅŸ\"):\n",
        "        if username in USER_CREDENTIALS and USER_CREDENTIALS[username] == password:\n",
        "            st.session_state['logged_in'] = True\n",
        "            st.experimental_rerun()  # Rerun the app to update UI after login\n",
        "        else:\n",
        "            st.error(\"GeÃ§ersiz kullanÄ±cÄ± adÄ± veya ÅŸifre\")\n",
        "\n",
        "# Function for Chat App\n",
        "def show_chat_app():\n",
        "    col1, col2 = st.sidebar.columns(2)  # Two columns for the buttons\n",
        "    with col1:\n",
        "        st.button('Oturumu Kapat', on_click=logout)\n",
        "    with col2:\n",
        "        st.button('Sohbeti Temizle', on_click=clear_chat_history)\n",
        "\n",
        "    st.title(\"BaÅŸlangÄ±Ã§ Seviye Sistem MÃ¼hendisliÄŸi Chat UygulamasÄ±\")\n",
        "\n",
        "    # Your existing chat app code here\n",
        "    # Replace with your existing main function code\n",
        "\n",
        "    # Describe and Clear document lists in session state\n",
        "    st.session_state.optimized_prompt = None\n",
        "    st.session_state.chroma_relevant_documents = []\n",
        "    st.session_state.bm25_documents = []\n",
        "    st.session_state.hybrid_search_documents = []\n",
        "\n",
        "    if \"messages\" not in st.session_state.keys():\n",
        "        st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"BugÃ¼n sana nasÄ±l yardÄ±mcÄ± olabilirim?\"}]\n",
        "\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.write(message[\"content\"])\n",
        "\n",
        "    if prompt := st.chat_input(\"MesajÄ±nÄ±zÄ± Giriniz:\"):\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "        st.session_state.optimized_prompt = generate_prompt_engineering(prompt, tokenizer, model)\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.write(prompt)\n",
        "\n",
        "        if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
        "            with st.chat_message(\"assistant\"):\n",
        "                with st.spinner(\"DÃ¼ÅŸÃ¼nÃ¼yorum...\"):\n",
        "                    try:\n",
        "                        final_prompt, metadata_info, st.session_state.chroma_relevant_documents, st.session_state.bm25_documents, st.session_state.hybrid_search_documents = rag_with_multiple_pdfs(st.session_state.optimized_prompt, st.session_state.messages)\n",
        "                        response = generate_llama3_response(final_prompt, tokenizer, model)\n",
        "\n",
        "                        placeholder = st.empty()\n",
        "                        full_response = response + \"\\n\\nMetadata Bilgileri:\\n\" + metadata_info\n",
        "                        placeholder.markdown(full_response)\n",
        "                        message = {\"role\": \"assistant\", \"content\": response}\n",
        "                        st.session_state.messages.append(message)\n",
        "                    except Exception as e:\n",
        "                        st.error(f\"An error occurred: {e}\")\n",
        "\n",
        "# Function to logout\n",
        "def logout():\n",
        "    st.session_state['logged_in'] = False\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"BugÃ¼n sana nasÄ±l yardÄ±mcÄ± olabilirim?\"}]\n",
        "\n",
        "# Function to clear chat history\n",
        "def clear_chat_history():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"BugÃ¼n sana nasÄ±l yardÄ±mcÄ± olabilirim?\"}]\n",
        "\n",
        "# Function for Sayfa 2\n",
        "def show_documents():\n",
        "    if not st.session_state['logged_in']:\n",
        "        st.warning(\"Ã–nce giriÅŸ yapmalÄ±sÄ±nÄ±z.\")\n",
        "    else:\n",
        "        st.title(\"Optimize Prompt ve DokÃ¼manlar\")\n",
        "        col_opt_prompt ,col_keyword, col_hybrid, col_semantic = st.columns(4)\n",
        "\n",
        "        with col_opt_prompt:\n",
        "            st.subheader(\"Optimize EdilmiÅŸ Prompt\")\n",
        "            st.divider()\n",
        "            col_opt_prompt.success(st.session_state.optimized_prompt)\n",
        "\n",
        "        with col_keyword:\n",
        "            st.subheader(\"Karakter BazlÄ± Arama | BM25\")\n",
        "            st.divider()\n",
        "            for keyword_doc in st.session_state.bm25_documents:\n",
        "                col_keyword.warning(f\"ID: {keyword_doc.metadata['doc_id']} || {keyword_doc.page_content}\")\n",
        "\n",
        "        with col_hybrid:\n",
        "            st.subheader(\"Hibrit Arama\")\n",
        "            st.divider()\n",
        "            for hybrid_doc in st.session_state.hybrid_search_documents:\n",
        "                col_hybrid.success(f\"ID: {hybrid_doc.metadata['doc_id']} || {hybrid_doc.page_content}\")\n",
        "\n",
        "        with col_semantic:\n",
        "            st.subheader(\"Semantik Arama\")\n",
        "            st.divider()\n",
        "            for chroma_doc in st.session_state.chroma_relevant_documents:\n",
        "                col_semantic.info(f\"ID: {chroma_doc.metadata['doc_id']} || {chroma_doc.page_content}\")\n",
        "\n",
        "# Streamlit app with multiple pages\n",
        "page_names_to_funcs = {\n",
        "    \"Ana Sayfa\": main,\n",
        "    \"Optimize Prompt ve DokÃ¼manlar\": show_documents,\n",
        "}\n",
        "\n",
        "selected_page = st.sidebar.selectbox(\"Sayfa seÃ§iniz\", page_names_to_funcs.keys())\n",
        "page_names_to_funcs[selected_page]()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZMvzQPcRgE_",
        "outputId": "7f6e7b6a-0e3a-4da7-b77d-736fd0a28ab8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app_v10.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!streamlit run /content/app_v10.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZTw5q1mRhfI",
        "outputId": "a427d019-37ba-4dc0-c19c-024f36879e9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25h+ localtunnel@2.0.2\n",
            "updated 1 package and audited 36 packages in 2.23s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 2 \u001b[93mmoderate\u001b[0m severity vulnerabilities\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.173s\n",
            "your url is: https://dry-candles-glow.loca.lt\n"
          ]
        }
      ]
    }
  ]
}