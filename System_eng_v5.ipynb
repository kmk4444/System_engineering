{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "mount_file_id": "https://github.com/kmk4444/System_engineering/blob/main/System_eng_v5.ipynb",
      "authorship_tag": "ABX9TyM8iPzVx5XZwpzc/D8qk/Wc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmk4444/System_engineering/blob/main/System_eng_v5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnsYdyjOcduZ"
      },
      "outputs": [],
      "source": [
        "pip install langchain openai pypdf chroma streamlit langchain_openai langchain_community langchain transformers bitsandbytes accelerate torch faiss-gpu faiss-cpu langchain_chroma langchain_experimental"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence-transformers"
      ],
      "metadata": {
        "id": "u0yrQUyJc6Y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install cohere"
      ],
      "metadata": {
        "id": "aX68-3fmc8-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa7205c6-b53a-4f9b-c247-522c958b3c48"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed boto3-1.34.141 botocore-1.34.141 cohere-5.5.8 fastavro-1.9.5 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 httpx-sse-0.4.0 jmespath-1.0.1 parameterized-0.9.0 s3transfer-0.10.2 types-requests-2.32.0.20240622\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rank_bm25"
      ],
      "metadata": {
        "id": "bhrD34qcc-VU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import pipeline\n",
        "\n",
        "login(token = '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')"
      ],
      "metadata": {
        "id": "wgXZaAtAO7Ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app_v8.py\n",
        "import streamlit as st\n",
        "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema.document import Document\n",
        "from langchain.vectorstores.chroma import Chroma\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import os\n",
        "import shutil\n",
        "from langchain_core.documents import Document\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "from langchain_community.embeddings import CohereEmbeddings\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "optimized_prompt=None\n",
        "chroma_relevant_documents=[]\n",
        "bm25_documents=[]\n",
        "hybrid_search_documents=[]\n",
        "\n",
        "\n",
        "\n",
        "templates = {\n",
        "    \"system\": \"You are a professional prompt engineer. Apply the mentioned prompt engineering technique and provide ONLY the improved prompt without any additional commentary or explanations.\",\n",
        "\n",
        "    \"system_multiple\": \"You are a professional prompt engineer. Thoroughly apply EVERY prompt engineering technique listed in the [Prompt Engineering Techniques to Apply] section. Use these techniques to enhance the original prompt provided below, ensuring the enhancement is clear and effective. Provide ONLY the improved version of the prompt without any additional commentary or explanations.\",\n",
        "\n",
        "    \"lang_default\": \"Identify the language of the user's original prompt in the [original] section. You MUST provide the enhanced version of the prompt in the **same language** as the user's original prompt. You'll be penalized if you translate it into another language unless explicitly requested by the user.\",\n",
        "\n",
        "    \"lang_eng\": \"If the original prompt is not in English, first translate it into English before proceeding with the improvement process.\",\n",
        "\n",
        "    \"deeper_understanding\" : \"When you need clarity or a deeper understanding of a topic, idea, or any piece of information, utilize the following prompts: - Explain [insert specific topic] in simple terms. - Explain to me like I'm 11 years old. - Explain to me as if I'm a beginner in [field]. - Explain to me as if I'm an expert in [field]. - -Write the [essay/text/paragraph] using simple English like you're explaining something to a 5-year-old.\",\n",
        "\n",
        "    \"deeper_understanding_simpler\": \"Explain to me as if Iâ€™m a beginner in System Engineering. Example: Change \\\"Explain system architecture.\\\" to \\\"Explain system architecture to beginners.\\\"\",\n",
        "\n",
        "    \"task_decomposition\": \"For complex or multi-step tasks, divide the original prompt into a series of simpler, more manageable sub-prompts. This approach allows the model to focus on one part of the task at a time, generating more detailed and coherent responses for each step.\\n\\nHere are examples demonstrating how to apply this principle:\\n\\n[original]\\nCreate a short story about a character who discovers an old, mysterious book that grants them extraordinary powers.\\n[improved]\\n1. Create a short story about a character who discovers an old, mysterious book that grants them extraordinary powers. first: Introduce the protagonist, ordinary life, and setting.\\n2. Describe discovering the book and the character's growing powers.\\n3. present challenges from powers and character's growth.Resolve conflicts, and show the character's reflection on the journey.\\n4. write the short story.\\n\\n========\\nBased on this approach, refine the following prompt to enable its breakdown into a series of simpler, step-by-step tasks:\\n\\n[original]\\n{prompt}\\n[improved]\\n\",\n",
        "\n",
        "    \"task_decomposition_simpler\": \"For complex or multi-step tasks, divide the original prompt into a series of simpler, more manageable sub-prompts. This approach allows the model to focus on one part of the task at a time, generating more detailed and coherent responses for each step.\",\n",
        "\n",
        "    \"fewshot_prompting\": \"Improve the original prompt by adding a couple of relevant examples that demonstrate the kind of answer or information being requested. Incorporate those examples smoothly into the prompt to make the desired response clear.\\n\\nHere are examples demonstrating how to apply this principle:\\n\\n[original]\\nClassify the emotion of the following text as positive, negative, or neutral. \\n\\\"It's a good day to try something fun.\\\" \\n[improved]\\nClassify the emotion of the following text as positive, negative, or neutral. \\nExample1: \\\"This music is awesome.\\\" (Positive) \\nExample2: \\\"I don't like spicy flavors.\\\" (Negative)\\nExample3: \\\"Every flower blooms at a different pace.\\\" (Neutral)\\nQuestion: \\\"It's a good day to try something fun.\\\" \\n\\n========\\nBased on this approach, refine the following prompt.\\n\\n[original]\\n{prompt}\\n[improved]\\n\",\n",
        "\n",
        "    \"fewshot_prompting_simpler\": \"Improve the original prompt by adding a couple of relevant examples that demonstrate the kind of answer or information being requested. Incorporate those examples smoothly into the prompt to make the desired response clear.\"\n",
        "}\n",
        "\n",
        "my_key_cohere=\"RsdlzOlRrQmvf5bxQrUHDpfbeg7Fvfr7WJwGCec1\"\n",
        "\n",
        "# Set up Streamlit page configuration\n",
        "st.set_page_config(page_title=\"ðŸ¦™ðŸ’¬ Sistem MÃ¼hendisliÄŸi GiriÅŸ Chatbot\")\n",
        "\n",
        "# Constants\n",
        "DATA_PATH = \"/content/drive/MyDrive/data\"  # PDF dosyalarÄ±nÄ±n bulunduÄŸu dizin\n",
        "CHROMA_PATH = \"chroma\"  # Chroma veritabanÄ±nÄ±n saklanacaÄŸÄ± dizin\n",
        "\n",
        "# Initialize Embeddings\n",
        "def initialize_embeddings():\n",
        "    embeddings = CohereEmbeddings(cohere_api_key=my_key_cohere, model=\"embed-multilingual-v3.0\")\n",
        "    return embeddings\n",
        "\n",
        "embeddings = initialize_embeddings()\n",
        "\n",
        "# Load and Split Documents\n",
        "def load_and_split_documents(data_path, embeddings):\n",
        "    document_loader = PyPDFDirectoryLoader(data_path)\n",
        "    raw_documents = document_loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=800,\n",
        "        chunk_overlap=80,\n",
        "        length_function=len\n",
        "    )\n",
        "    return text_splitter.split_documents(raw_documents)\n",
        "\n",
        "# Create Custom Documents\n",
        "def create_custom_documents(splitted_documents):\n",
        "    custom_documents = []\n",
        "    for i, raw_doc in enumerate(splitted_documents):\n",
        "        new_doc = Document(\n",
        "                page_content=raw_doc.page_content,\n",
        "                metadata={\n",
        "                    \"source\": raw_doc.metadata.get(\"source\", \"Unknown Source\"),\n",
        "                    \"title\": raw_doc.metadata.get(\"title\", \"No Title\"),\n",
        "                    \"description\": raw_doc.metadata.get(\"description\", \"No Description\"),\n",
        "                    \"language\": raw_doc.metadata.get(\"language\", \"Unknown Language\"),\n",
        "                    \"doc_id\": i\n",
        "                }\n",
        "        )\n",
        "        custom_documents.append(new_doc)\n",
        "    return custom_documents\n",
        "\n",
        "# Initialize Vectorstore and Retriever\n",
        "def initialize_vectorstore(custom_documents, embeddings, persist_directory):\n",
        "    try:\n",
        "        # Chroma'ya belgeleri ve embeddings'i geÃ§irin\n",
        "        vectorstore = Chroma.from_documents(\n",
        "            custom_documents,\n",
        "            embeddings,\n",
        "            persist_directory=persist_directory\n",
        "        )\n",
        "\n",
        "        return vectorstore.as_retriever(\n",
        "            search_type=\"mmr\",\n",
        "            search_kwargs={'k': 4, 'lambda_mult': 0.50}\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing vectorstore: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Retrieve Relevant Documents\n",
        "def retrieve_relevant_documents(retriever, prompt):\n",
        "    return retriever.get_relevant_documents(prompt)\n",
        "\n",
        "\n",
        "def get_relevant_documents_with_bm25(documents, query):\n",
        "\n",
        "    bm25_retriever = BM25Retriever.from_documents(documents=documents)\n",
        "    bm25_retriever.k = 4\n",
        "\n",
        "    bm25_relevant_documents = bm25_retriever.get_relevant_documents(query=query)\n",
        "\n",
        "    return bm25_relevant_documents, bm25_retriever\n",
        "\n",
        "def get_relevant_documents_for_hybrid_search(query, retriever1, retriever2, weight1, weight2):\n",
        "\n",
        "\n",
        "    ensemble_retriever = EnsembleRetriever(\n",
        "                                retrievers=[retriever1, retriever2],\n",
        "                                weights=[weight1, weight2])\n",
        "\n",
        "    hybrid_relevant_documents = ensemble_retriever.get_relevant_documents(query)\n",
        "\n",
        "    return hybrid_relevant_documents\n",
        "\n",
        "# Generate Final Prompt\n",
        "def generate_final_prompt(prompt, context_data, chat_history, relevant_documents):\n",
        "    history_prompt = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in chat_history])\n",
        "    metadata_info = \"\\n\".join([f\"Belge {doc.metadata['doc_id']} - BaÅŸlÄ±k: {doc.metadata['title']}, Kaynak: {doc.metadata['source']}\" for doc in relevant_documents])\n",
        "    return  f\"\"\"\n",
        "    ###Instruction###\n",
        "    You are an expert assistant dedicated to providing answers for beginner systems engineers. Your primary role is to deliver clear, concise, and expert-level information in Turkish. Use the provided documents and historical conversation data to answer questions. Do not speculate or go beyond the given data. You have to answer just TURKISH!\n",
        "\n",
        "    ###Previous Conversations###\n",
        "    {history_prompt}\n",
        "\n",
        "    ###Question###\n",
        "    {prompt}\n",
        "\n",
        "    ###Context###\n",
        "    This is the information we have to answer the question: {context_data}\n",
        "    \"\"\", metadata_info\n",
        "\n",
        "# RAG with Multiple PDFs\n",
        "def rag_with_multiple_pdfs(prompt, chat_history):\n",
        "    splitted_documents = load_and_split_documents(DATA_PATH, embeddings)\n",
        "    custom_documents = create_custom_documents(splitted_documents)\n",
        "    chroma_retriever = initialize_vectorstore(custom_documents, embeddings, CHROMA_PATH)\n",
        "    chroma_relevant_documents = retrieve_relevant_documents(chroma_retriever, prompt)\n",
        "\n",
        "    bm25_documents, bm25retriever = get_relevant_documents_with_bm25(custom_documents,prompt)\n",
        "\n",
        "    weight1=0.2\n",
        "    hybrid_search_documents = get_relevant_documents_for_hybrid_search(\n",
        "                                                        query=prompt,\n",
        "                                                        retriever1=bm25retriever,\n",
        "                                                        retriever2=chroma_retriever,\n",
        "                                                        weight1=weight1,\n",
        "                                                        weight2=1-weight1)\n",
        "\n",
        "\n",
        "    context_data = \" \".join([doc.page_content for doc in hybrid_search_documents])\n",
        "    final_prompt, metadata_info = generate_final_prompt(prompt, context_data, chat_history, hybrid_search_documents)\n",
        "    return final_prompt, metadata_info, chroma_relevant_documents, bm25_documents, hybrid_search_documents\n",
        "\n",
        "# Initialize Model\n",
        "def initialize_model():\n",
        "    model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb_config\n",
        "    )\n",
        "    return tokenizer, model\n",
        "\n",
        "tokenizer, model = initialize_model()\n",
        "\n",
        "\n",
        "def generate_prompt_engineering(prompt, tokenizer, model):\n",
        "    system_message = templates[\"system_multiple\"]\n",
        "    system_message += '\\n' + templates[\"lang_eng\"]\n",
        "\n",
        "\n",
        "    skills = [\"deeper_understanding\",\"task_decomposition\",\"fewshot_prompting\"]\n",
        "    integrated_templates = \"[Prompt Engineering Techniques to Apply]\\n\"\n",
        "\n",
        "    for idx, skill in enumerate(skills):\n",
        "        template = templates[f\"{skill}_simpler\"]\n",
        "        integrated_templates += f\"{idx+1}. {skill}: {template}\\n\"\n",
        "    integrated_templates += \"Based on [Prompt engineering techniques to apply], refine the prompt provided below. Ensure that each technique is fully incorporated to achieve a clear and effective improvement:\\n\\n[original]\\n{prompt}\\n[improved]\\n\"\n",
        "\n",
        "    prompt_template = PromptTemplate.from_template(integrated_templates)\n",
        "    formatted_input = prompt_template.format(prompt=prompt)\n",
        "\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": system_message}, {\"role\": \"user\", \"content\": formatted_input}]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=1024,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "\n",
        "# Generate LLaMA3 Response\n",
        "def generate_llama3_response(prompt_input, tokenizer, model):\n",
        "    SYS_PROMPT = \"\"\"You are an expert assistant dedicated to guiding and empowering beginner systems engineers.\n",
        "    Your role is crucial in providing clear, concise, and expert-level information in Turkish.\n",
        "    Utilize the provided documents and historical conversation data to deliver insightful answers.\n",
        "    If unsure, confidently state \"Bilmiyorum\" without speculating.\n",
        "    Your responses should be motivational and tailored to empower beginners in understanding systems engineering principles.\n",
        "    You have to just answer TURKISH!\"\"\"\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": SYS_PROMPT}, {\"role\": \"user\", \"content\": prompt_input}]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=1024,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "\n",
        "# Main Function\n",
        "def main():\n",
        "\n",
        "    global optimized_prompt, chroma_relevant_documents, bm25_documents, hybrid_search_documents\n",
        "\n",
        "    if \"messages\" not in st.session_state.keys():\n",
        "        st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.write(message[\"content\"])\n",
        "\n",
        "    if prompt := st.chat_input(\"MesajÄ±nÄ±zÄ± Giriniz:\"):\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "        optimized_prompt = generate_prompt_engineering(prompt, tokenizer, model)\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.write(prompt)\n",
        "\n",
        "        if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
        "            with st.chat_message(\"assistant\"):\n",
        "                with st.spinner(\"Thinking...\"):\n",
        "                    try:\n",
        "                        final_prompt, metadata_info, chroma_relevant_documents, bm25_documents, hybrid_search_documents = rag_with_multiple_pdfs(optimized_prompt, st.session_state.messages)\n",
        "                        response = generate_llama3_response(final_prompt, tokenizer, model)\n",
        "                        placeholder = st.empty()\n",
        "                        full_response = response\n",
        "                        placeholder.markdown(full_response)\n",
        "                        message = {\"role\": \"assistant\", \"content\": f\"{full_response}\\n\\nMetadata Bilgileri:\\n{metadata_info}\"}\n",
        "                        st.session_state.messages.append(message)\n",
        "                    except Exception as e:\n",
        "                        st.error(f\"An error occurred: {e}\")\n",
        "\n",
        "def clear_chat_history():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "st.sidebar.button('Clear Chat History', on_click=clear_chat_history)\n",
        "\n",
        "# Documents Function\n",
        "def show_documents():\n",
        "    global optimized_prompt, chroma_relevant_documents, bm25_documents, hybrid_search_documents\n",
        "    st.title(\"Sayfa 2\")\n",
        "    col_opt_prompt ,col_keyword, col_hybrid, col_semantic = st.columns(4)\n",
        "\n",
        "    with col_opt_prompt:\n",
        "      st.subheader(\"Optimize EdilmiÅŸ Prompt\")\n",
        "      st.divider()\n",
        "\n",
        "    with col_keyword:\n",
        "      st.subheader(\"Karakter BazlÄ± Arama | BM25\")\n",
        "      st.divider()\n",
        "\n",
        "    with col_hybrid:\n",
        "      st.subheader(\"Hibrit Arama\")\n",
        "      st.divider()\n",
        "\n",
        "    with col_semantic:\n",
        "      st.subheader(\"Semantik Arama\")\n",
        "      st.divider()\n",
        "\n",
        "    col_opt_prompt.success(optimized_prompt)\n",
        "\n",
        "    for keyword_doc in bm25_documents:\n",
        "        col_keyword.warning(f\"ID: {keyword_doc.metadata['doc_id']} || {keyword_doc.page_content}\")\n",
        "\n",
        "    for chroma_doc in chroma_relevant_documents:\n",
        "        col_semantic.info(f\"ID: {chroma_doc.metadata['doc_id']} || {chroma_doc.page_content}\")\n",
        "\n",
        "    for hybrid_doc in hybrid_search_documents:\n",
        "        col_hybrid.success(f\"ID: {hybrid_doc.metadata['doc_id']} || {hybrid_doc.page_content}\")\n",
        "\n",
        "# Streamlit app with multiple pages\n",
        "page_names_to_funcs = {\n",
        "    \"Ana Sayfa\": main,\n",
        "    \"Sayfa 2\": show_documents,\n",
        "}\n",
        "\n",
        "selected_page = st.sidebar.selectbox(\"Sayfa seÃ§in\", page_names_to_funcs.keys())\n",
        "page_names_to_funcs[selected_page]()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WqXLxhi5c3Aa",
        "outputId": "176a63a2-b47f-435d-ea8b-5e4e8d0bbf20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app_v8.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!streamlit run /content/app_v8.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "BE1_LVY3MHNc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78ef66af-2ad3-43d8-f48e-0271a72fe20c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25h+ localtunnel@2.0.2\n",
            "updated 1 package and audited 36 packages in 0.474s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 2 \u001b[93mmoderate\u001b[0m severity vulnerabilities\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 1.453s\n",
            "your url is: https://tall-spies-move.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app_v9.py\n",
        "import streamlit as st\n",
        "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema.document import Document\n",
        "from langchain.vectorstores.chroma import Chroma\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import os\n",
        "import shutil\n",
        "from langchain_core.documents import Document\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "from langchain_community.embeddings import CohereEmbeddings\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "\n",
        "templates = {\n",
        "    \"system\": \"You are a professional prompt engineer. Apply the mentioned prompt engineering technique and provide ONLY the improved prompt without any additional commentary or explanations.\",\n",
        "\n",
        "    \"system_multiple\": \"You are a professional prompt engineer. Thoroughly apply EVERY prompt engineering technique listed in the [Prompt Engineering Techniques to Apply] section. Use these techniques to enhance the original prompt provided below, ensuring the enhancement is clear and effective. Provide ONLY the improved version of the prompt without any additional commentary or explanations.\",\n",
        "\n",
        "    \"lang_default\": \"Identify the language of the user's original prompt in the [original] section. You MUST provide the enhanced version of the prompt in the **same language** as the user's original prompt. You'll be penalized if you translate it into another language unless explicitly requested by the user.\",\n",
        "\n",
        "    \"lang_eng\": \"If the original prompt is not in English, first translate it into English before proceeding with the improvement process.\",\n",
        "\n",
        "    \"deeper_understanding\" : \"When you need clarity or a deeper understanding of a topic, idea, or any piece of information, utilize the following prompts: - Explain [insert specific topic] in simple terms. - Explain to me like I'm 11 years old. - Explain to me as if I'm a beginner in [field]. - Explain to me as if I'm an expert in [field]. - -Write the [essay/text/paragraph] using simple English like you're explaining something to a 5-year-old.\",\n",
        "\n",
        "    \"deeper_understanding_simpler\": \"Explain to me as if Iâ€™m a beginner in System Engineering. Example: Change \\\"Explain system architecture.\\\" to \\\"Explain system architecture to beginners.\\\"\",\n",
        "\n",
        "    \"task_decomposition\": \"For complex or multi-step tasks, divide the original prompt into a series of simpler, more manageable sub-prompts. This approach allows the model to focus on one part of the task at a time, generating more detailed and coherent responses for each step.\\n\\nHere are examples demonstrating how to apply this principle:\\n\\n[original]\\nCreate a short story about a character who discovers an old, mysterious book that grants them extraordinary powers.\\n[improved]\\n1. Create a short story about a character who discovers an old, mysterious book that grants them extraordinary powers. first: Introduce the protagonist, ordinary life, and setting.\\n2. Describe discovering the book and the character's growing powers.\\n3. present challenges from powers and character's growth.Resolve conflicts, and show the character's reflection on the journey.\\n4. write the short story.\\n\\n========\\nBased on this approach, refine the following prompt to enable its breakdown into a series of simpler, step-by-step tasks:\\n\\n[original]\\n{prompt}\\n[improved]\\n\",\n",
        "\n",
        "    \"task_decomposition_simpler\": \"For complex or multi-step tasks, divide the original prompt into a series of simpler, more manageable sub-prompts. This approach allows the model to focus on one part of the task at a time, generating more detailed and coherent responses for each step.\",\n",
        "\n",
        "    \"fewshot_prompting\": \"Improve the original prompt by adding a couple of relevant examples that demonstrate the kind of answer or information being requested. Incorporate those examples smoothly into the prompt to make the desired response clear.\\n\\nHere are examples demonstrating how to apply this principle:\\n\\n[original]\\nClassify the emotion of the following text as positive, negative, or neutral. \\n\\\"It's a good day to try something fun.\\\" \\n[improved]\\nClassify the emotion of the following text as positive, negative, or neutral. \\nExample1: \\\"This music is awesome.\\\" (Positive) \\nExample2: \\\"I don't like spicy flavors.\\\" (Negative)\\nExample3: \\\"Every flower blooms at a different pace.\\\" (Neutral)\\nQuestion: \\\"It's a good day to try something fun.\\\" \\n\\n========\\nBased on this approach, refine the following prompt.\\n\\n[original]\\n{prompt}\\n[improved]\\n\",\n",
        "\n",
        "    \"fewshot_prompting_simpler\": \"Improve the original prompt by adding a couple of relevant examples that demonstrate the kind of answer or information being requested. Incorporate those examples smoothly into the prompt to make the desired response clear.\"\n",
        "}\n",
        "\n",
        "my_key_cohere=\"RsdlzOlRrQmvf5bxQrUHDpfbeg7Fvfr7WJwGCec1\"\n",
        "\n",
        "# Set up Streamlit page configuration\n",
        "st.set_page_config(page_title=\"ðŸ¦™ðŸ’¬ Sistem MÃ¼hendisliÄŸi GiriÅŸ Chatbot\")\n",
        "\n",
        "# Constants\n",
        "DATA_PATH = \"/content/drive/MyDrive/data\"  # PDF dosyalarÄ±nÄ±n bulunduÄŸu dizin\n",
        "CHROMA_PATH = \"chroma\"  # Chroma veritabanÄ±nÄ±n saklanacaÄŸÄ± dizin\n",
        "\n",
        "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "# Initialize Embeddings\n",
        "def initialize_embeddings():\n",
        "    return HuggingFaceInferenceAPIEmbeddings(\n",
        "        api_key='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',\n",
        "        model_name=\"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "    )\n",
        "\n",
        "embeddings = initialize_embeddings()\n",
        "\n",
        "# Load and Split Documents\n",
        "def load_and_split_documents(data_path, embeddings):\n",
        "    document_loader = PyPDFDirectoryLoader(data_path)\n",
        "    raw_documents = document_loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=800,\n",
        "        chunk_overlap=80,\n",
        "        length_function=len\n",
        "    )\n",
        "    return text_splitter.split_documents(raw_documents)\n",
        "\n",
        "# Create Custom Documents\n",
        "def create_custom_documents(splitted_documents):\n",
        "    custom_documents = []\n",
        "    for i, raw_doc in enumerate(splitted_documents):\n",
        "        new_doc = Document(\n",
        "                page_content=raw_doc.page_content,\n",
        "                metadata={\n",
        "                    \"source\": raw_doc.metadata.get(\"source\", \"Unknown Source\"),\n",
        "                    \"title\": raw_doc.metadata.get(\"title\", \"No Title\"),\n",
        "                    \"description\": raw_doc.metadata.get(\"description\", \"No Description\"),\n",
        "                    \"language\": raw_doc.metadata.get(\"language\", \"Unknown Language\"),\n",
        "                    \"doc_id\": i\n",
        "                }\n",
        "        )\n",
        "        custom_documents.append(new_doc)\n",
        "    return custom_documents\n",
        "\n",
        "# Initialize Vectorstore and Retriever\n",
        "def initialize_vectorstore(custom_documents, embeddings, persist_directory):\n",
        "    try:\n",
        "        # Chroma'ya belgeleri ve embeddings'i geÃ§irin\n",
        "        vectorstore = Chroma.from_documents(\n",
        "            custom_documents,\n",
        "            embeddings,\n",
        "            persist_directory=persist_directory\n",
        "        )\n",
        "\n",
        "        return vectorstore.as_retriever(\n",
        "            search_type=\"mmr\",\n",
        "            search_kwargs={'k': 4, 'lambda_mult': 0.50}\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing vectorstore: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Retrieve Relevant Documents\n",
        "def retrieve_relevant_documents(retriever, prompt):\n",
        "    return retriever.get_relevant_documents(prompt)\n",
        "\n",
        "\n",
        "def get_relevant_documents_with_bm25(documents, query):\n",
        "\n",
        "    bm25_retriever = BM25Retriever.from_documents(documents=documents)\n",
        "    bm25_retriever.k = 4\n",
        "\n",
        "    bm25_relevant_documents = bm25_retriever.get_relevant_documents(query=query)\n",
        "\n",
        "    return bm25_relevant_documents, bm25_retriever\n",
        "\n",
        "def get_relevant_documents_for_hybrid_search(query, retriever1, retriever2, weight1, weight2):\n",
        "\n",
        "\n",
        "    ensemble_retriever = EnsembleRetriever(\n",
        "                                retrievers=[retriever1, retriever2],\n",
        "                                weights=[weight1, weight2])\n",
        "\n",
        "    hybrid_relevant_documents = ensemble_retriever.get_relevant_documents(query)\n",
        "\n",
        "    return hybrid_relevant_documents\n",
        "\n",
        "# Generate Final Prompt\n",
        "def generate_final_prompt(prompt, context_data, chat_history, relevant_documents):\n",
        "    history_prompt = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in chat_history])\n",
        "    metadata_info = \"\\n\".join([f\"Belge {doc.metadata['doc_id']} - BaÅŸlÄ±k: {doc.metadata['title']}, Kaynak: {doc.metadata['source']}\" for doc in relevant_documents])\n",
        "    return  f\"\"\"\n",
        "    ###Instruction###\n",
        "    You are an expert assistant dedicated to providing answers for beginner systems engineers. Follow these guidelines:\n",
        "\n",
        "    1. Deliver clear, concise, and expert-level information in Turkish.\n",
        "    2. Use the provided documents (###Context###) and historical conversation (###Previous Conversations###) data to answer questions (###Question###). Reference these documents while interpreting and generating your answers.\n",
        "    3. Do not speculate.\n",
        "    4. Ensure that your answers are coherent and free from any repetitive or duplicate content.\n",
        "    5. Each sentence should provide unique and valuable information.\n",
        "    6. You have to answer just in TURKISH.\n",
        "\n",
        "    ###Previous Conversations###\n",
        "    {history_prompt}\n",
        "\n",
        "    ###Question###\n",
        "    {prompt}\n",
        "\n",
        "    ###Context###\n",
        "    This is the information we have to answer the question: {context_data}\n",
        "    \"\"\", metadata_info\n",
        "\n",
        "# RAG with Multiple PDFs\n",
        "def rag_with_multiple_pdfs(prompt, chat_history):\n",
        "    splitted_documents = load_and_split_documents(DATA_PATH, embeddings)\n",
        "    custom_documents = create_custom_documents(splitted_documents)\n",
        "    chroma_retriever = initialize_vectorstore(custom_documents, embeddings, CHROMA_PATH)\n",
        "    chroma_relevant_documents = retrieve_relevant_documents(chroma_retriever, prompt)\n",
        "\n",
        "    bm25_documents, bm25retriever = get_relevant_documents_with_bm25(custom_documents,prompt)\n",
        "\n",
        "    weight1=0.2\n",
        "    hybrid_search_documents = get_relevant_documents_for_hybrid_search(\n",
        "                                                        query=prompt,\n",
        "                                                        retriever1=bm25retriever,\n",
        "                                                        retriever2=chroma_retriever,\n",
        "                                                        weight1=weight1,\n",
        "                                                        weight2=1-weight1)\n",
        "\n",
        "\n",
        "    context_data = \" \".join([doc.page_content for doc in hybrid_search_documents])\n",
        "    final_prompt, metadata_info = generate_final_prompt(prompt, context_data, chat_history, hybrid_search_documents)\n",
        "    return final_prompt, metadata_info, chroma_relevant_documents, bm25_documents, hybrid_search_documents\n",
        "\n",
        "# Initialize Model\n",
        "def initialize_model():\n",
        "    model_id = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
        "    '''\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )'''\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\"\n",
        "        #quantization_config=bnb_config\n",
        "    )\n",
        "    return tokenizer, model\n",
        "\n",
        "tokenizer, model = initialize_model()\n",
        "\n",
        "\n",
        "def generate_prompt_engineering(prompt, tokenizer, model):\n",
        "    system_message = templates[\"system_multiple\"]\n",
        "    system_message += '\\n' + templates[\"lang_eng\"]\n",
        "\n",
        "\n",
        "    skills = [\"deeper_understanding\",\"task_decomposition\",\"fewshot_prompting\"]\n",
        "    integrated_templates = \"[Prompt Engineering Techniques to Apply]\\n\"\n",
        "\n",
        "    for idx, skill in enumerate(skills):\n",
        "        template = templates[f\"{skill}_simpler\"]\n",
        "        integrated_templates += f\"{idx+1}. {skill}: {template}\\n\"\n",
        "    integrated_templates += \"Based on [Prompt engineering techniques to apply], refine the prompt provided below. Ensure that each technique is fully incorporated to achieve a clear and effective improvement:\\n\\n[original]\\n{prompt}\\n[improved]\\n\"\n",
        "\n",
        "    prompt_template = PromptTemplate.from_template(integrated_templates)\n",
        "    formatted_input = prompt_template.format(prompt=prompt)\n",
        "\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": system_message}, {\"role\": \"user\", \"content\": formatted_input}]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=1024,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        temperature=0.5,\n",
        "        top_p=1,\n",
        "    )\n",
        "\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "\n",
        "# Generate LLaMA3 Response\n",
        "def generate_llama3_response(prompt_input, tokenizer, model):\n",
        "    SYS_PROMPT = \"\"\"You are an expert assistant dedicated to guiding and empowering beginner systems engineers. Follow these guidelines:\n",
        "    1. Provide clear, concise, and expert-level information in Turkish.\n",
        "    2. Use the provided documents and historical conversation data to deliver insightful answers. Previous conversation history and document information will be given to you in the user's prompt.\n",
        "    3. Ensure that your answers are free from any repetitive or duplicate content and sentence. You have to eliminate repetitive sentences. It is very important!\n",
        "    4. If unsure, confidently state \"Bilmiyorum\" without speculating.\n",
        "    5. Your responses should be motivational and tailored to empower beginners in understanding systems engineering principles.\n",
        "    6. Only respond in TURKISH.\"\"\"\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": SYS_PROMPT}, {\"role\": \"user\", \"content\": prompt_input}]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=1024,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        temperature=0.5,\n",
        "        top_p=1,\n",
        "    )\n",
        "\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "# Main Function\n",
        "def main():\n",
        "\n",
        "    # Describe and Clear document lists in session state\n",
        "    st.session_state.optimized_prompt = None\n",
        "    st.session_state.chroma_relevant_documents = []\n",
        "    st.session_state.bm25_documents = []\n",
        "    st.session_state.hybrid_search_documents = []\n",
        "\n",
        "    if \"messages\" not in st.session_state.keys():\n",
        "        st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.write(message[\"content\"])\n",
        "\n",
        "    if prompt := st.chat_input(\"MesajÄ±nÄ±zÄ± Giriniz:\"):\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "        st.session_state.optimized_prompt = generate_prompt_engineering(prompt, tokenizer, model)\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.write(prompt)\n",
        "\n",
        "        if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
        "            with st.chat_message(\"assistant\"):\n",
        "                with st.spinner(\"Thinking...\"):\n",
        "                    try:\n",
        "                        final_prompt, metadata_info, st.session_state.chroma_relevant_documents, st.session_state.bm25_documents, st.session_state.hybrid_search_documents = rag_with_multiple_pdfs(st.session_state.optimized_prompt, st.session_state.messages)\n",
        "                        response = generate_llama3_response(final_prompt, tokenizer, model)\n",
        "                        placeholder = st.empty()\n",
        "                        full_response = response + \"\\n\\nMetadata Bilgileri:\\n\" + metadata_info\n",
        "                        placeholder.markdown(full_response)\n",
        "                        message = {\"role\": \"assistant\", \"content\": response}\n",
        "                        st.session_state.messages.append(message)\n",
        "                    except Exception as e:\n",
        "                        st.error(f\"An error occurred: {e}\")\n",
        "\n",
        "def clear_chat_history():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "st.sidebar.button('Clear Chat History', on_click=clear_chat_history)\n",
        "\n",
        "# Documents Function\n",
        "def show_documents():\n",
        "    global optimized_prompt, chroma_relevant_documents, bm25_documents, hybrid_search_documents\n",
        "    st.title(\"Sayfa 2\")\n",
        "    col_opt_prompt ,col_keyword, col_hybrid, col_semantic = st.columns(4)\n",
        "\n",
        "    with col_opt_prompt:\n",
        "      st.subheader(\"Optimize EdilmiÅŸ Prompt\")\n",
        "      st.divider()\n",
        "\n",
        "    with col_keyword:\n",
        "      st.subheader(\"Karakter BazlÄ± Arama | BM25\")\n",
        "      st.divider()\n",
        "\n",
        "    with col_hybrid:\n",
        "      st.subheader(\"Hibrit Arama\")\n",
        "      st.divider()\n",
        "\n",
        "    with col_semantic:\n",
        "      st.subheader(\"Semantik Arama\")\n",
        "      st.divider()\n",
        "\n",
        "    col_opt_prompt.success(st.session_state.optimized_prompt)\n",
        "\n",
        "    for keyword_doc in st.session_state.bm25_documents:\n",
        "        col_keyword.warning(f\"ID: {keyword_doc.metadata['doc_id']} || {keyword_doc.page_content}\")\n",
        "\n",
        "    for chroma_doc in st.session_state.chroma_relevant_documents:\n",
        "        col_semantic.info(f\"ID: {chroma_doc.metadata['doc_id']} || {chroma_doc.page_content}\")\n",
        "\n",
        "    for hybrid_doc in st.session_state.hybrid_search_documents:\n",
        "        col_hybrid.success(f\"ID: {hybrid_doc.metadata['doc_id']} || {hybrid_doc.page_content}\")\n",
        "\n",
        "# Streamlit app with multiple pages\n",
        "page_names_to_funcs = {\n",
        "    \"Ana Sayfa\": main,\n",
        "    \"Sayfa 2\": show_documents,\n",
        "}\n",
        "\n",
        "selected_page = st.sidebar.selectbox(\"Sayfa seÃ§in\", page_names_to_funcs.keys())\n",
        "page_names_to_funcs[selected_page]()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmc8yIwmcKIF",
        "outputId": "0694d53d-4194-40d5-9f5a-ed787c7948a7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app_v9.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!streamlit run /content/app_v9.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FS_jqNDOdunC",
        "outputId": "5e1c9b5d-8b8c-4129-d7c8-fc68be9773cc"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25h+ localtunnel@2.0.2\n",
            "updated 1 package and audited 36 packages in 0.439s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 2 \u001b[93mmoderate\u001b[0m severity vulnerabilities\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 1.198s\n",
            "your url is: https://mean-ears-wonder.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"risk yÃ¶netimi hakkÄ±nda bilgi verir misin?\"},\n",
        "]\n",
        "pipe = pipeline(\"text-generation\", model=\"unsloth/llama-3-8b-Instruct-bnb-4bit\")\n",
        "# Set max_new_tokens to control the length of generated text\n",
        "result = pipe(messages, max_new_tokens=1024)  # Adjust the value as needed\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGk9SHVfnI24",
        "outputId": "f2cb249c-252e-4158-9113-8fcd93a7577e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': [{'role': 'user', 'content': 'risk yÃ¶netimi hakkÄ±nda bilgi verir misin?'}, {'role': 'assistant', 'content': \"Risk management! It's a crucial aspect of any organization, as it helps identify, assess, and mitigate potential risks that could impact its operations, reputation, and bottom line. Here's an overview of risk management:\\n\\n**What is Risk Management?**\\n\\nRisk management is the process of identifying, assessing, and mitigating potential risks that could impact an organization's goals, objectives, and stakeholders. It involves a systematic approach to understanding and managing risks, which can be categorized into three main types:\\n\\n1. **Strategic Risk**: Risks related to an organization's overall strategy, such as market risks, competitive risks, and regulatory risks.\\n2. **Operational Risk**: Risks related to an organization's day-to-day operations, such as process risks, human resources risks, and supply chain risks.\\n3. **Reputational Risk**: Risks related to an organization's reputation, such as brand risks, customer risks, and stakeholder risks.\\n\\n**The Risk Management Process**\\n\\nThe risk management process typically involves the following steps:\\n\\n1. **Risk Identification**: Identifying potential risks that could impact the organization.\\n2. **Risk Assessment**: Assessing the likelihood and potential impact of each identified risk.\\n3. **Risk Prioritization**: Prioritizing risks based on their likelihood and potential impact.\\n4. **Risk Mitigation**: Implementing controls and mitigation strategies to reduce the likelihood or impact of each prioritized risk.\\n5. **Risk Monitoring**: Continuously monitoring and reviewing the effectiveness of risk mitigation strategies.\\n6. **Risk Reporting**: Reporting risk-related information to stakeholders, including the risk management team, senior management, and the board of directors.\\n\\n**Benefits of Risk Management**\\n\\nEffective risk management can bring numerous benefits to an organization, including:\\n\\n1. **Improved Decision-Making**: Risk management helps organizations make informed decisions by considering potential risks and opportunities.\\n2. **Increased Efficiency**: Risk management can help streamline processes and reduce waste by identifying and mitigating potential risks.\\n3. **Enhanced Reputation**: Organizations that effectively manage risks can maintain a strong reputation and build trust with stakeholders.\\n4. **Reduced Costs**: Risk management can help reduce costs by identifying and mitigating potential risks that could impact the organization's bottom line.\\n5. **Increased Compliance**: Risk management can help organizations comply with regulatory requirements and industry standards.\\n\\n**Risk Management Frameworks**\\n\\nThere are several risk management frameworks that organizations can use to guide their risk management efforts, including:\\n\\n1. **ISO 31000**: A widely recognized international standard for risk management.\\n2. **COSO ERM**: A framework developed by the Committee of Sponsoring Organizations of the Treadway Commission.\\n3. **NIST Cybersecurity Framework**: A framework developed by the National Institute of Standards and Technology.\\n\\nI hope this information helps! Let me know if you have any specific questions or if there's anything else I can help with.\"}]}]\n"
          ]
        }
      ]
    }
  ]
}