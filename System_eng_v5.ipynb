{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "mount_file_id": "https://github.com/kmk4444/System_engineering/blob/main/System_eng_v5.ipynb",
      "authorship_tag": "ABX9TyOHfkv2SF/jtbuwu+qwAg4B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmk4444/System_engineering/blob/main/System_eng_v5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnsYdyjOcduZ"
      },
      "outputs": [],
      "source": [
        "pip install langchain openai pypdf chroma streamlit langchain_openai langchain_community langchain transformers bitsandbytes accelerate torch faiss-gpu faiss-cpu langchain_chroma langchain_experimental"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence-transformers"
      ],
      "metadata": {
        "id": "u0yrQUyJc6Y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install cohere"
      ],
      "metadata": {
        "id": "aX68-3fmc8-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rank_bm25"
      ],
      "metadata": {
        "id": "bhrD34qcc-VU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import pipeline\n",
        "\n",
        "login(token = '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')"
      ],
      "metadata": {
        "id": "wgXZaAtAO7Ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app_v8.py\n",
        "import streamlit as st\n",
        "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema.document import Document\n",
        "from langchain.vectorstores.chroma import Chroma\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import os\n",
        "import shutil\n",
        "from langchain_core.documents import Document\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "from langchain_community.embeddings import CohereEmbeddings\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "optimized_prompt=None\n",
        "chroma_relevant_documents=[]\n",
        "bm25_documents=[]\n",
        "hybrid_search_documents=[]\n",
        "\n",
        "\n",
        "\n",
        "templates = {\n",
        "    \"system\": \"You are a professional prompt engineer. Apply the mentioned prompt engineering technique and provide ONLY the improved prompt without any additional commentary or explanations.\",\n",
        "\n",
        "    \"system_multiple\": \"You are a professional prompt engineer. Thoroughly apply EVERY prompt engineering technique listed in the [Prompt Engineering Techniques to Apply] section. Use these techniques to enhance the original prompt provided below, ensuring the enhancement is clear and effective. Provide ONLY the improved version of the prompt without any additional commentary or explanations.\",\n",
        "\n",
        "    \"lang_default\": \"Identify the language of the user's original prompt in the [original] section. You MUST provide the enhanced version of the prompt in the **same language** as the user's original prompt. You'll be penalized if you translate it into another language unless explicitly requested by the user.\",\n",
        "\n",
        "    \"lang_eng\": \"If the original prompt is not in English, first translate it into English before proceeding with the improvement process.\",\n",
        "\n",
        "    \"deeper_understanding\" : \"When you need clarity or a deeper understanding of a topic, idea, or any piece of information, utilize the following prompts: - Explain [insert specific topic] in simple terms. - Explain to me like I'm 11 years old. - Explain to me as if I'm a beginner in [field]. - Explain to me as if I'm an expert in [field]. - -Write the [essay/text/paragraph] using simple English like you're explaining something to a 5-year-old.\",\n",
        "\n",
        "    \"deeper_understanding_simpler\": \"Explain to me as if Iâ€™m a beginner in System Engineering. Example: Change \\\"Explain system architecture.\\\" to \\\"Explain system architecture to beginners.\\\"\",\n",
        "\n",
        "    \"task_decomposition\": \"For complex or multi-step tasks, divide the original prompt into a series of simpler, more manageable sub-prompts. This approach allows the model to focus on one part of the task at a time, generating more detailed and coherent responses for each step.\\n\\nHere are examples demonstrating how to apply this principle:\\n\\n[original]\\nCreate a short story about a character who discovers an old, mysterious book that grants them extraordinary powers.\\n[improved]\\n1. Create a short story about a character who discovers an old, mysterious book that grants them extraordinary powers. first: Introduce the protagonist, ordinary life, and setting.\\n2. Describe discovering the book and the character's growing powers.\\n3. present challenges from powers and character's growth.Resolve conflicts, and show the character's reflection on the journey.\\n4. write the short story.\\n\\n========\\nBased on this approach, refine the following prompt to enable its breakdown into a series of simpler, step-by-step tasks:\\n\\n[original]\\n{prompt}\\n[improved]\\n\",\n",
        "\n",
        "    \"task_decomposition_simpler\": \"For complex or multi-step tasks, divide the original prompt into a series of simpler, more manageable sub-prompts. This approach allows the model to focus on one part of the task at a time, generating more detailed and coherent responses for each step.\",\n",
        "\n",
        "    \"fewshot_prompting\": \"Improve the original prompt by adding a couple of relevant examples that demonstrate the kind of answer or information being requested. Incorporate those examples smoothly into the prompt to make the desired response clear.\\n\\nHere are examples demonstrating how to apply this principle:\\n\\n[original]\\nClassify the emotion of the following text as positive, negative, or neutral. \\n\\\"It's a good day to try something fun.\\\" \\n[improved]\\nClassify the emotion of the following text as positive, negative, or neutral. \\nExample1: \\\"This music is awesome.\\\" (Positive) \\nExample2: \\\"I don't like spicy flavors.\\\" (Negative)\\nExample3: \\\"Every flower blooms at a different pace.\\\" (Neutral)\\nQuestion: \\\"It's a good day to try something fun.\\\" \\n\\n========\\nBased on this approach, refine the following prompt.\\n\\n[original]\\n{prompt}\\n[improved]\\n\",\n",
        "\n",
        "    \"fewshot_prompting_simpler\": \"Improve the original prompt by adding a couple of relevant examples that demonstrate the kind of answer or information being requested. Incorporate those examples smoothly into the prompt to make the desired response clear.\"\n",
        "}\n",
        "\n",
        "my_key_cohere=\"RsdlzOlRrQmvf5bxQrUHDpfbeg7Fvfr7WJwGCec1\"\n",
        "\n",
        "# Set up Streamlit page configuration\n",
        "st.set_page_config(page_title=\"ðŸ¦™ðŸ’¬ Sistem MÃ¼hendisliÄŸi GiriÅŸ Chatbot\")\n",
        "\n",
        "# Constants\n",
        "DATA_PATH = \"/content/drive/MyDrive/data\"  # PDF dosyalarÄ±nÄ±n bulunduÄŸu dizin\n",
        "CHROMA_PATH = \"chroma\"  # Chroma veritabanÄ±nÄ±n saklanacaÄŸÄ± dizin\n",
        "\n",
        "# Initialize Embeddings\n",
        "def initialize_embeddings():\n",
        "    embeddings = CohereEmbeddings(cohere_api_key=my_key_cohere, model=\"embed-multilingual-v3.0\")\n",
        "    return embeddings\n",
        "\n",
        "embeddings = initialize_embeddings()\n",
        "\n",
        "# Load and Split Documents\n",
        "def load_and_split_documents(data_path, embeddings):\n",
        "    document_loader = PyPDFDirectoryLoader(data_path)\n",
        "    raw_documents = document_loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=800,\n",
        "        chunk_overlap=80,\n",
        "        length_function=len\n",
        "    )\n",
        "    return text_splitter.split_documents(raw_documents)\n",
        "\n",
        "# Create Custom Documents\n",
        "def create_custom_documents(splitted_documents):\n",
        "    custom_documents = []\n",
        "    for i, raw_doc in enumerate(splitted_documents):\n",
        "        new_doc = Document(\n",
        "                page_content=raw_doc.page_content,\n",
        "                metadata={\n",
        "                    \"source\": raw_doc.metadata.get(\"source\", \"Unknown Source\"),\n",
        "                    \"title\": raw_doc.metadata.get(\"title\", \"No Title\"),\n",
        "                    \"description\": raw_doc.metadata.get(\"description\", \"No Description\"),\n",
        "                    \"language\": raw_doc.metadata.get(\"language\", \"Unknown Language\"),\n",
        "                    \"doc_id\": i\n",
        "                }\n",
        "        )\n",
        "        custom_documents.append(new_doc)\n",
        "    return custom_documents\n",
        "\n",
        "# Initialize Vectorstore and Retriever\n",
        "def initialize_vectorstore(custom_documents, embeddings, persist_directory):\n",
        "    try:\n",
        "        # Chroma'ya belgeleri ve embeddings'i geÃ§irin\n",
        "        vectorstore = Chroma.from_documents(\n",
        "            custom_documents,\n",
        "            embeddings,\n",
        "            persist_directory=persist_directory\n",
        "        )\n",
        "\n",
        "        return vectorstore.as_retriever(\n",
        "            search_type=\"mmr\",\n",
        "            search_kwargs={'k': 4, 'lambda_mult': 0.50}\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing vectorstore: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Retrieve Relevant Documents\n",
        "def retrieve_relevant_documents(retriever, prompt):\n",
        "    return retriever.get_relevant_documents(prompt)\n",
        "\n",
        "\n",
        "def get_relevant_documents_with_bm25(documents, query):\n",
        "\n",
        "    bm25_retriever = BM25Retriever.from_documents(documents=documents)\n",
        "    bm25_retriever.k = 4\n",
        "\n",
        "    bm25_relevant_documents = bm25_retriever.get_relevant_documents(query=query)\n",
        "\n",
        "    return bm25_relevant_documents, bm25_retriever\n",
        "\n",
        "def get_relevant_documents_for_hybrid_search(query, retriever1, retriever2, weight1, weight2):\n",
        "\n",
        "\n",
        "    ensemble_retriever = EnsembleRetriever(\n",
        "                                retrievers=[retriever1, retriever2],\n",
        "                                weights=[weight1, weight2])\n",
        "\n",
        "    hybrid_relevant_documents = ensemble_retriever.get_relevant_documents(query)\n",
        "\n",
        "    return hybrid_relevant_documents\n",
        "\n",
        "# Generate Final Prompt\n",
        "def generate_final_prompt(prompt, context_data, chat_history, relevant_documents):\n",
        "    history_prompt = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in chat_history])\n",
        "    metadata_info = \"\\n\".join([f\"Belge {doc.metadata['doc_id']} - BaÅŸlÄ±k: {doc.metadata['title']}, Kaynak: {doc.metadata['source']}\" for doc in relevant_documents])\n",
        "    return  f\"\"\"\n",
        "    ###Instruction###\n",
        "    You are an expert assistant dedicated to providing answers for beginner systems engineers. Your primary role is to deliver clear, concise, and expert-level information in Turkish. Use the provided documents and historical conversation data to answer questions. Do not speculate or go beyond the given data. You have to answer just TURKISH!\n",
        "\n",
        "    ###Previous Conversations###\n",
        "    {history_prompt}\n",
        "\n",
        "    ###Question###\n",
        "    {prompt}\n",
        "\n",
        "    ###Context###\n",
        "    This is the information we have to answer the question: {context_data}\n",
        "    \"\"\", metadata_info\n",
        "\n",
        "# RAG with Multiple PDFs\n",
        "def rag_with_multiple_pdfs(prompt, chat_history):\n",
        "    splitted_documents = load_and_split_documents(DATA_PATH, embeddings)\n",
        "    custom_documents = create_custom_documents(splitted_documents)\n",
        "    chroma_retriever = initialize_vectorstore(custom_documents, embeddings, CHROMA_PATH)\n",
        "    chroma_relevant_documents = retrieve_relevant_documents(chroma_retriever, prompt)\n",
        "\n",
        "    bm25_documents, bm25retriever = get_relevant_documents_with_bm25(custom_documents,prompt)\n",
        "\n",
        "    weight1=0.2\n",
        "    hybrid_search_documents = get_relevant_documents_for_hybrid_search(\n",
        "                                                        query=prompt,\n",
        "                                                        retriever1=bm25retriever,\n",
        "                                                        retriever2=chroma_retriever,\n",
        "                                                        weight1=weight1,\n",
        "                                                        weight2=1-weight1)\n",
        "\n",
        "\n",
        "    context_data = \" \".join([doc.page_content for doc in hybrid_search_documents])\n",
        "    final_prompt, metadata_info = generate_final_prompt(prompt, context_data, chat_history, hybrid_search_documents)\n",
        "    return final_prompt, metadata_info, chroma_relevant_documents, bm25_documents, hybrid_search_documents\n",
        "\n",
        "# Initialize Model\n",
        "def initialize_model():\n",
        "    model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb_config\n",
        "    )\n",
        "    return tokenizer, model\n",
        "\n",
        "tokenizer, model = initialize_model()\n",
        "\n",
        "\n",
        "def generate_prompt_engineering(prompt, tokenizer, model):\n",
        "    system_message = templates[\"system_multiple\"]\n",
        "    system_message += '\\n' + templates[\"lang_eng\"]\n",
        "\n",
        "\n",
        "    skills = [\"deeper_understanding\",\"task_decomposition\",\"fewshot_prompting\"]\n",
        "    integrated_templates = \"[Prompt Engineering Techniques to Apply]\\n\"\n",
        "\n",
        "    for idx, skill in enumerate(skills):\n",
        "        template = templates[f\"{skill}_simpler\"]\n",
        "        integrated_templates += f\"{idx+1}. {skill}: {template}\\n\"\n",
        "    integrated_templates += \"Based on [Prompt engineering techniques to apply], refine the prompt provided below. Ensure that each technique is fully incorporated to achieve a clear and effective improvement:\\n\\n[original]\\n{prompt}\\n[improved]\\n\"\n",
        "\n",
        "    prompt_template = PromptTemplate.from_template(integrated_templates)\n",
        "    formatted_input = prompt_template.format(prompt=prompt)\n",
        "\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": system_message}, {\"role\": \"user\", \"content\": formatted_input}]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=1024,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "\n",
        "# Generate LLaMA3 Response\n",
        "def generate_llama3_response(prompt_input, tokenizer, model):\n",
        "    SYS_PROMPT = \"\"\"You are an expert assistant dedicated to guiding and empowering beginner systems engineers.\n",
        "    Your role is crucial in providing clear, concise, and expert-level information in Turkish.\n",
        "    Utilize the provided documents and historical conversation data to deliver insightful answers.\n",
        "    If unsure, confidently state \"Bilmiyorum\" without speculating.\n",
        "    Your responses should be motivational and tailored to empower beginners in understanding systems engineering principles.\n",
        "    You have to just answer TURKISH!\"\"\"\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": SYS_PROMPT}, {\"role\": \"user\", \"content\": prompt_input}]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=1024,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "\n",
        "# Main Function\n",
        "def main():\n",
        "\n",
        "    global optimized_prompt, chroma_relevant_documents, bm25_documents, hybrid_search_documents\n",
        "\n",
        "    if \"messages\" not in st.session_state.keys():\n",
        "        st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.write(message[\"content\"])\n",
        "\n",
        "    if prompt := st.chat_input(\"MesajÄ±nÄ±zÄ± Giriniz:\"):\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "        optimized_prompt = generate_prompt_engineering(prompt, tokenizer, model)\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.write(prompt)\n",
        "\n",
        "        if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
        "            with st.chat_message(\"assistant\"):\n",
        "                with st.spinner(\"Thinking...\"):\n",
        "                    try:\n",
        "                        final_prompt, metadata_info, chroma_relevant_documents, bm25_documents, hybrid_search_documents = rag_with_multiple_pdfs(optimized_prompt, st.session_state.messages)\n",
        "                        response = generate_llama3_response(final_prompt, tokenizer, model)\n",
        "                        placeholder = st.empty()\n",
        "                        full_response = response\n",
        "                        placeholder.markdown(full_response)\n",
        "                        message = {\"role\": \"assistant\", \"content\": f\"{full_response}\\n\\nMetadata Bilgileri:\\n{metadata_info}\"}\n",
        "                        st.session_state.messages.append(message)\n",
        "                    except Exception as e:\n",
        "                        st.error(f\"An error occurred: {e}\")\n",
        "\n",
        "def clear_chat_history():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "st.sidebar.button('Clear Chat History', on_click=clear_chat_history)\n",
        "\n",
        "# Documents Function\n",
        "def show_documents():\n",
        "    global optimized_prompt, chroma_relevant_documents, bm25_documents, hybrid_search_documents\n",
        "    st.title(\"Sayfa 2\")\n",
        "    col_opt_prompt ,col_keyword, col_hybrid, col_semantic = st.columns(4)\n",
        "\n",
        "    with col_opt_prompt:\n",
        "      st.subheader(\"Optimize EdilmiÅŸ Prompt\")\n",
        "      st.divider()\n",
        "\n",
        "    with col_keyword:\n",
        "      st.subheader(\"Karakter BazlÄ± Arama | BM25\")\n",
        "      st.divider()\n",
        "\n",
        "    with col_hybrid:\n",
        "      st.subheader(\"Hibrit Arama\")\n",
        "      st.divider()\n",
        "\n",
        "    with col_semantic:\n",
        "      st.subheader(\"Semantik Arama\")\n",
        "      st.divider()\n",
        "\n",
        "    col_opt_prompt.success(optimized_prompt)\n",
        "\n",
        "    for keyword_doc in bm25_documents:\n",
        "        col_keyword.warning(f\"ID: {keyword_doc.metadata['doc_id']} || {keyword_doc.page_content}\")\n",
        "\n",
        "    for chroma_doc in chroma_relevant_documents:\n",
        "        col_semantic.info(f\"ID: {chroma_doc.metadata['doc_id']} || {chroma_doc.page_content}\")\n",
        "\n",
        "    for hybrid_doc in hybrid_search_documents:\n",
        "        col_hybrid.success(f\"ID: {hybrid_doc.metadata['doc_id']} || {hybrid_doc.page_content}\")\n",
        "\n",
        "# Streamlit app with multiple pages\n",
        "page_names_to_funcs = {\n",
        "    \"Ana Sayfa\": main,\n",
        "    \"Sayfa 2\": show_documents,\n",
        "}\n",
        "\n",
        "selected_page = st.sidebar.selectbox(\"Sayfa seÃ§in\", page_names_to_funcs.keys())\n",
        "page_names_to_funcs[selected_page]()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WqXLxhi5c3Aa",
        "outputId": "ea567ced-702d-43ae-da96-37af3163f970",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app_v8.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!streamlit run /content/app_v8.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "BE1_LVY3MHNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app_v9.py\n",
        "import streamlit as st\n",
        "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema.document import Document\n",
        "from langchain.vectorstores.chroma import Chroma\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import os\n",
        "import shutil\n",
        "from langchain_core.documents import Document\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "from langchain_community.embeddings import CohereEmbeddings\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "\n",
        "\n",
        "templates = {\n",
        "    \"system\": \"You are a professional prompt engineer. Apply the mentioned prompt engineering technique and provide ONLY the improved prompt without any additional commentary or explanations.\",\n",
        "\n",
        "    \"system_multiple\": \"You are a professional prompt engineer. Thoroughly apply EVERY prompt engineering technique listed in the [Prompt Engineering Techniques to Apply] section. Use these techniques to enhance the original prompt provided below, ensuring the enhancement is clear and effective. Provide ONLY the improved version of the prompt without any additional commentary or explanations.\",\n",
        "\n",
        "    \"lang_default\": \"Identify the language of the user's original prompt in the [original] section. You MUST provide the enhanced version of the prompt in the **same language** as the user's original prompt. You'll be penalized if you translate it into another language unless explicitly requested by the user.\",\n",
        "\n",
        "    \"lang_eng\": \"If the original prompt is not in English, first translate it into English before proceeding with the improvement process.\",\n",
        "\n",
        "    \"deeper_understanding\" : \"When you need clarity or a deeper understanding of a topic, idea, or any piece of information, utilize the following prompts: - Explain [insert specific topic] in simple terms. - Explain to me like I'm 11 years old. - Explain to me as if I'm a beginner in [field]. - Explain to me as if I'm an expert in [field]. - -Write the [essay/text/paragraph] using simple English like you're explaining something to a 5-year-old.\",\n",
        "\n",
        "    \"deeper_understanding_simpler\": \"Explain to me as if Iâ€™m a beginner in System Engineering. Example: Change \\\"Explain system architecture.\\\" to \\\"Explain system architecture to beginners.\\\"\",\n",
        "\n",
        "    \"task_decomposition\": \"For complex or multi-step tasks, divide the original prompt into a series of simpler, more manageable sub-prompts. This approach allows the model to focus on one part of the task at a time, generating more detailed and coherent responses for each step.\\n\\nHere are examples demonstrating how to apply this principle:\\n\\n[original]\\nCreate a short story about a character who discovers an old, mysterious book that grants them extraordinary powers.\\n[improved]\\n1. Create a short story about a character who discovers an old, mysterious book that grants them extraordinary powers. first: Introduce the protagonist, ordinary life, and setting.\\n2. Describe discovering the book and the character's growing powers.\\n3. present challenges from powers and character's growth.Resolve conflicts, and show the character's reflection on the journey.\\n4. write the short story.\\n\\n========\\nBased on this approach, refine the following prompt to enable its breakdown into a series of simpler, step-by-step tasks:\\n\\n[original]\\n{prompt}\\n[improved]\\n\",\n",
        "\n",
        "    \"task_decomposition_simpler\": \"For complex or multi-step tasks, divide the original prompt into a series of simpler, more manageable sub-prompts. This approach allows the model to focus on one part of the task at a time, generating more detailed and coherent responses for each step.\",\n",
        "\n",
        "    \"fewshot_prompting\": \"Improve the original prompt by adding a couple of relevant examples that demonstrate the kind of answer or information being requested. Incorporate those examples smoothly into the prompt to make the desired response clear.\\n\\nHere are examples demonstrating how to apply this principle:\\n\\n[original]\\nClassify the emotion of the following text as positive, negative, or neutral. \\n\\\"It's a good day to try something fun.\\\" \\n[improved]\\nClassify the emotion of the following text as positive, negative, or neutral. \\nExample1: \\\"This music is awesome.\\\" (Positive) \\nExample2: \\\"I don't like spicy flavors.\\\" (Negative)\\nExample3: \\\"Every flower blooms at a different pace.\\\" (Neutral)\\nQuestion: \\\"It's a good day to try something fun.\\\" \\n\\n========\\nBased on this approach, refine the following prompt.\\n\\n[original]\\n{prompt}\\n[improved]\\n\",\n",
        "\n",
        "    \"fewshot_prompting_simpler\": \"Improve the original prompt by adding a couple of relevant examples that demonstrate the kind of answer or information being requested. Incorporate those examples smoothly into the prompt to make the desired response clear.\"\n",
        "}\n",
        "\n",
        "my_key_cohere=\"RsdlzOlRrQmvf5bxQrUHDpfbeg7Fvfr7WJwGCec1\"\n",
        "\n",
        "# Set up Streamlit page configuration\n",
        "st.set_page_config(page_title=\"ðŸ¦™ðŸ’¬ Sistem MÃ¼hendisliÄŸi GiriÅŸ Chatbot\")\n",
        "\n",
        "# Constants\n",
        "DATA_PATH = \"/content/drive/MyDrive/data\"  # PDF dosyalarÄ±nÄ±n bulunduÄŸu dizin\n",
        "CHROMA_PATH = \"chroma\"  # Chroma veritabanÄ±nÄ±n saklanacaÄŸÄ± dizin\n",
        "\n",
        "# Initialize Embeddings\n",
        "def initialize_embeddings():\n",
        "    embeddings = CohereEmbeddings(cohere_api_key=my_key_cohere, model=\"embed-multilingual-v3.0\")\n",
        "    return embeddings\n",
        "\n",
        "embeddings = initialize_embeddings()\n",
        "\n",
        "# Load and Split Documents\n",
        "def load_and_split_documents(data_path, embeddings):\n",
        "    document_loader = PyPDFDirectoryLoader(data_path)\n",
        "    raw_documents = document_loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=800,\n",
        "        chunk_overlap=80,\n",
        "        length_function=len\n",
        "    )\n",
        "    return text_splitter.split_documents(raw_documents)\n",
        "\n",
        "# Create Custom Documents\n",
        "def create_custom_documents(splitted_documents):\n",
        "    custom_documents = []\n",
        "    for i, raw_doc in enumerate(splitted_documents):\n",
        "        new_doc = Document(\n",
        "                page_content=raw_doc.page_content,\n",
        "                metadata={\n",
        "                    \"source\": raw_doc.metadata.get(\"source\", \"Unknown Source\"),\n",
        "                    \"title\": raw_doc.metadata.get(\"title\", \"No Title\"),\n",
        "                    \"description\": raw_doc.metadata.get(\"description\", \"No Description\"),\n",
        "                    \"language\": raw_doc.metadata.get(\"language\", \"Unknown Language\"),\n",
        "                    \"doc_id\": i\n",
        "                }\n",
        "        )\n",
        "        custom_documents.append(new_doc)\n",
        "    return custom_documents\n",
        "\n",
        "# Initialize Vectorstore and Retriever\n",
        "def initialize_vectorstore(custom_documents, embeddings, persist_directory):\n",
        "    try:\n",
        "        # Chroma'ya belgeleri ve embeddings'i geÃ§irin\n",
        "        vectorstore = Chroma.from_documents(\n",
        "            custom_documents,\n",
        "            embeddings,\n",
        "            persist_directory=persist_directory\n",
        "        )\n",
        "\n",
        "        return vectorstore.as_retriever(\n",
        "            search_type=\"mmr\",\n",
        "            search_kwargs={'k': 4, 'lambda_mult': 0.50}\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing vectorstore: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Retrieve Relevant Documents\n",
        "def retrieve_relevant_documents(retriever, prompt):\n",
        "    return retriever.get_relevant_documents(prompt)\n",
        "\n",
        "\n",
        "def get_relevant_documents_with_bm25(documents, query):\n",
        "\n",
        "    bm25_retriever = BM25Retriever.from_documents(documents=documents)\n",
        "    bm25_retriever.k = 4\n",
        "\n",
        "    bm25_relevant_documents = bm25_retriever.get_relevant_documents(query=query)\n",
        "\n",
        "    return bm25_relevant_documents, bm25_retriever\n",
        "\n",
        "def get_relevant_documents_for_hybrid_search(query, retriever1, retriever2, weight1, weight2):\n",
        "\n",
        "\n",
        "    ensemble_retriever = EnsembleRetriever(\n",
        "                                retrievers=[retriever1, retriever2],\n",
        "                                weights=[weight1, weight2])\n",
        "\n",
        "    hybrid_relevant_documents = ensemble_retriever.get_relevant_documents(query)\n",
        "\n",
        "    return hybrid_relevant_documents\n",
        "\n",
        "# Generate Final Prompt\n",
        "def generate_final_prompt(prompt, context_data, chat_history, relevant_documents):\n",
        "    history_prompt = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in chat_history])\n",
        "    metadata_info = \"\\n\".join([f\"Belge {doc.metadata['doc_id']} - BaÅŸlÄ±k: {doc.metadata['title']}, Kaynak: {doc.metadata['source']}\" for doc in relevant_documents])\n",
        "    return  f\"\"\"\n",
        "    ###Instruction###\n",
        "    You are an expert assistant dedicated to providing answers for beginner systems engineers. Your primary role is to deliver clear, concise, and expert-level information in Turkish. Use the provided documents and historical conversation data to answer questions. Do not speculate or go beyond the given data. Ensure that your answers are free from any repetitive or duplicate content. You have to answer just in TURKISH!\n",
        "\n",
        "    ###Previous Conversations###\n",
        "    {history_prompt}\n",
        "\n",
        "    ###Question###\n",
        "    {prompt}\n",
        "\n",
        "    ###Context###\n",
        "    This is the information we have to answer the question: {context_data}\n",
        "    \"\"\", metadata_info\n",
        "\n",
        "# RAG with Multiple PDFs\n",
        "def rag_with_multiple_pdfs(prompt, chat_history):\n",
        "    splitted_documents = load_and_split_documents(DATA_PATH, embeddings)\n",
        "    custom_documents = create_custom_documents(splitted_documents)\n",
        "    chroma_retriever = initialize_vectorstore(custom_documents, embeddings, CHROMA_PATH)\n",
        "    chroma_relevant_documents = retrieve_relevant_documents(chroma_retriever, prompt)\n",
        "\n",
        "    bm25_documents, bm25retriever = get_relevant_documents_with_bm25(custom_documents,prompt)\n",
        "\n",
        "    weight1=0.2\n",
        "    hybrid_search_documents = get_relevant_documents_for_hybrid_search(\n",
        "                                                        query=prompt,\n",
        "                                                        retriever1=bm25retriever,\n",
        "                                                        retriever2=chroma_retriever,\n",
        "                                                        weight1=weight1,\n",
        "                                                        weight2=1-weight1)\n",
        "\n",
        "\n",
        "    context_data = \" \".join([doc.page_content for doc in hybrid_search_documents])\n",
        "    final_prompt, metadata_info = generate_final_prompt(prompt, context_data, chat_history, hybrid_search_documents)\n",
        "    return final_prompt, metadata_info, chroma_relevant_documents, bm25_documents, hybrid_search_documents\n",
        "\n",
        "# Initialize Model\n",
        "def initialize_model():\n",
        "    model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb_config\n",
        "    )\n",
        "    return tokenizer, model\n",
        "\n",
        "tokenizer, model = initialize_model()\n",
        "\n",
        "\n",
        "def generate_prompt_engineering(prompt, tokenizer, model):\n",
        "    system_message = templates[\"system_multiple\"]\n",
        "    system_message += '\\n' + templates[\"lang_eng\"]\n",
        "\n",
        "\n",
        "    skills = [\"deeper_understanding\",\"task_decomposition\",\"fewshot_prompting\"]\n",
        "    integrated_templates = \"[Prompt Engineering Techniques to Apply]\\n\"\n",
        "\n",
        "    for idx, skill in enumerate(skills):\n",
        "        template = templates[f\"{skill}_simpler\"]\n",
        "        integrated_templates += f\"{idx+1}. {skill}: {template}\\n\"\n",
        "    integrated_templates += \"Based on [Prompt engineering techniques to apply], refine the prompt provided below. Ensure that each technique is fully incorporated to achieve a clear and effective improvement:\\n\\n[original]\\n{prompt}\\n[improved]\\n\"\n",
        "\n",
        "    prompt_template = PromptTemplate.from_template(integrated_templates)\n",
        "    formatted_input = prompt_template.format(prompt=prompt)\n",
        "\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": system_message}, {\"role\": \"user\", \"content\": formatted_input}]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=1024,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "\n",
        "# Generate LLaMA3 Response\n",
        "def generate_llama3_response(prompt_input, tokenizer, model):\n",
        "    SYS_PROMPT = \"\"\"You are an expert assistant dedicated to guiding and empowering beginner systems engineers.\n",
        "    Your role is crucial in providing clear, concise, and expert-level information in Turkish.\n",
        "    Utilize the provided documents and historical conversation data to deliver insightful answers.\n",
        "    Ensure that your answers are free from any repetitive or duplicate content.\n",
        "    If unsure, confidently state \"Bilmiyorum\" without speculating.\n",
        "    Your responses should be motivational and tailored to empower beginners in understanding systems engineering principles.\n",
        "    You have to just answer TURKISH!\"\"\"\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": SYS_PROMPT}, {\"role\": \"user\", \"content\": prompt_input}]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=1024,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "# Main Function\n",
        "def main():\n",
        "\n",
        "    # Describe and Clear document lists in session state\n",
        "    st.session_state.optimized_prompt = None\n",
        "    st.session_state.chroma_relevant_documents = []\n",
        "    st.session_state.bm25_documents = []\n",
        "    st.session_state.hybrid_search_documents = []\n",
        "\n",
        "    if \"messages\" not in st.session_state.keys():\n",
        "        st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.write(message[\"content\"])\n",
        "\n",
        "    if prompt := st.chat_input(\"MesajÄ±nÄ±zÄ± Giriniz:\"):\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "        st.session_state.optimized_prompt = generate_prompt_engineering(prompt, tokenizer, model)\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.write(prompt)\n",
        "\n",
        "        if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
        "            with st.chat_message(\"assistant\"):\n",
        "                with st.spinner(\"Thinking...\"):\n",
        "                    try:\n",
        "                        final_prompt, metadata_info, st.session_state.chroma_relevant_documents, st.session_state.bm25_documents, st.session_state.hybrid_search_documents = rag_with_multiple_pdfs(st.session_state.optimized_prompt, st.session_state.messages)\n",
        "                        response = generate_llama3_response(final_prompt, tokenizer, model)\n",
        "                        placeholder = st.empty()\n",
        "                        full_response = response + \"\\n\\nMetadata Bilgileri:\\n\" + metadata_info\n",
        "                        placeholder.markdown(full_response)\n",
        "                        message = {\"role\": \"assistant\", \"content\": response}\n",
        "                        st.session_state.messages.append(message)\n",
        "                    except Exception as e:\n",
        "                        st.error(f\"An error occurred: {e}\")\n",
        "\n",
        "def clear_chat_history():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "st.sidebar.button('Clear Chat History', on_click=clear_chat_history)\n",
        "\n",
        "# Documents Function\n",
        "def show_documents():\n",
        "    global optimized_prompt, chroma_relevant_documents, bm25_documents, hybrid_search_documents\n",
        "    st.title(\"Sayfa 2\")\n",
        "    col_opt_prompt ,col_keyword, col_hybrid, col_semantic = st.columns(4)\n",
        "\n",
        "    with col_opt_prompt:\n",
        "      st.subheader(\"Optimize EdilmiÅŸ Prompt\")\n",
        "      st.divider()\n",
        "\n",
        "    with col_keyword:\n",
        "      st.subheader(\"Karakter BazlÄ± Arama | BM25\")\n",
        "      st.divider()\n",
        "\n",
        "    with col_hybrid:\n",
        "      st.subheader(\"Hibrit Arama\")\n",
        "      st.divider()\n",
        "\n",
        "    with col_semantic:\n",
        "      st.subheader(\"Semantik Arama\")\n",
        "      st.divider()\n",
        "\n",
        "    col_opt_prompt.success(st.session_state.optimized_prompt)\n",
        "\n",
        "    for keyword_doc in st.session_state.bm25_documents:\n",
        "        col_keyword.warning(f\"ID: {keyword_doc.metadata['doc_id']} || {keyword_doc.page_content}\")\n",
        "\n",
        "    for chroma_doc in st.session_state.chroma_relevant_documents:\n",
        "        col_semantic.info(f\"ID: {chroma_doc.metadata['doc_id']} || {chroma_doc.page_content}\")\n",
        "\n",
        "    for hybrid_doc in st.session_state.hybrid_search_documents:\n",
        "        col_hybrid.success(f\"ID: {hybrid_doc.metadata['doc_id']} || {hybrid_doc.page_content}\")\n",
        "\n",
        "# Streamlit app with multiple pages\n",
        "page_names_to_funcs = {\n",
        "    \"Ana Sayfa\": main,\n",
        "    \"Sayfa 2\": show_documents,\n",
        "}\n",
        "\n",
        "selected_page = st.sidebar.selectbox(\"Sayfa seÃ§in\", page_names_to_funcs.keys())\n",
        "page_names_to_funcs[selected_page]()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmc8yIwmcKIF",
        "outputId": "445fa826-0b7b-4ebd-cfef-76bdc95f91b0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app_v9.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!streamlit run /content/app_v9.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FS_jqNDOdunC",
        "outputId": "f4b64f7f-4026-4141-9108-73cd935afa3d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25h+ localtunnel@2.0.2\n",
            "updated 1 package and audited 36 packages in 0.384s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 2 \u001b[93mmoderate\u001b[0m severity vulnerabilities\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 1.597s\n",
            "your url is: https://huge-keys-like.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}