{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "mount_file_id": "https://github.com/kmk4444/System_engineering/blob/main/system_engineering_v12(deepseekr1).ipynb",
      "authorship_tag": "ABX9TyMSZGXZSVuLSP+sBGxWj0+M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8a34cdff52ad4e9898a7fc159da5a505": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e9396a48e6eb4909a28dc84c7869b2e0",
              "IPY_MODEL_bfde3864752948df87146da6e94f594b",
              "IPY_MODEL_6b6048c7d4ef48fcab1150b2b1642191"
            ],
            "layout": "IPY_MODEL_c1f4b45b78984f54995f8e417c8f9fbf"
          }
        },
        "e9396a48e6eb4909a28dc84c7869b2e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e7f40c1e0de44c0aa369504d5bbae91",
            "placeholder": "​",
            "style": "IPY_MODEL_39d1652b4953466e9410ff3aff535fb1",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "bfde3864752948df87146da6e94f594b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44a479a0f46f4eb9bb8123a0dabff4ac",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5ca70e7dbff44fa1ae7c607c7941d131",
            "value": 4
          }
        },
        "6b6048c7d4ef48fcab1150b2b1642191": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d3c69892610448b87b25e3250b3fa43",
            "placeholder": "​",
            "style": "IPY_MODEL_a8b3d36a18f9401182d12d0c9935b4ae",
            "value": " 4/4 [00:14&lt;00:00,  3.18s/it]"
          }
        },
        "c1f4b45b78984f54995f8e417c8f9fbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e7f40c1e0de44c0aa369504d5bbae91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39d1652b4953466e9410ff3aff535fb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "44a479a0f46f4eb9bb8123a0dabff4ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ca70e7dbff44fa1ae7c607c7941d131": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1d3c69892610448b87b25e3250b3fa43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8b3d36a18f9401182d12d0c9935b4ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3bc7b17ce0e14745a881266c8651d6cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3db7134f1b2d4635bd91b53a6119c5ed",
              "IPY_MODEL_3d51b1a1ad5e464da9a5e0e1d2363b45",
              "IPY_MODEL_262bba1df0214b768d59825603e5155c"
            ],
            "layout": "IPY_MODEL_b6606a2c7e5c4c07b90a59fdda94f8a9"
          }
        },
        "3db7134f1b2d4635bd91b53a6119c5ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecf352c1023b4ec7b904f4b4679220b8",
            "placeholder": "​",
            "style": "IPY_MODEL_67529f21a6ff4edf92cc7910474588d8",
            "value": "generation_config.json: 100%"
          }
        },
        "3d51b1a1ad5e464da9a5e0e1d2363b45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0840d1073cfd4bda90aceec8e347b0e5",
            "max": 231,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fa6d5b72c18743788b76bd0bb13dec61",
            "value": 231
          }
        },
        "262bba1df0214b768d59825603e5155c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0c76f5270bd4b6fbd7695baf7484f0b",
            "placeholder": "​",
            "style": "IPY_MODEL_50d1d017c2cd4e9aa25aadf42cf24685",
            "value": " 231/231 [00:00&lt;00:00, 30.1kB/s]"
          }
        },
        "b6606a2c7e5c4c07b90a59fdda94f8a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecf352c1023b4ec7b904f4b4679220b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67529f21a6ff4edf92cc7910474588d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0840d1073cfd4bda90aceec8e347b0e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa6d5b72c18743788b76bd0bb13dec61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f0c76f5270bd4b6fbd7695baf7484f0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50d1d017c2cd4e9aa25aadf42cf24685": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "737b811014434b7eb28b293806ff2dc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6bd217b388f74429a520c7492b9ba2a2",
              "IPY_MODEL_b93314acfd9a475cbc02b2b0f9dbed76",
              "IPY_MODEL_5e4ad91df6024df5975deabd192c16ec"
            ],
            "layout": "IPY_MODEL_8a8cd69c5b1f405daf56a926d9865ae4"
          }
        },
        "6bd217b388f74429a520c7492b9ba2a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9d2bba920e64a45a0369e2d1ff0864d",
            "placeholder": "​",
            "style": "IPY_MODEL_da97282966264b9a9192860c5340df3a",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "b93314acfd9a475cbc02b2b0f9dbed76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8da869c7abdc4221bdf6a0351b58815e",
            "max": 52956,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd650c3f14ee40ce8c94fc7fb03b11ae",
            "value": 52956
          }
        },
        "5e4ad91df6024df5975deabd192c16ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2206e719402a4a6093eb29c5403858d8",
            "placeholder": "​",
            "style": "IPY_MODEL_dc6f0da93c7b4d24886cdaeca8444407",
            "value": " 53.0k/53.0k [00:00&lt;00:00, 6.35MB/s]"
          }
        },
        "8a8cd69c5b1f405daf56a926d9865ae4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9d2bba920e64a45a0369e2d1ff0864d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da97282966264b9a9192860c5340df3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8da869c7abdc4221bdf6a0351b58815e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd650c3f14ee40ce8c94fc7fb03b11ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2206e719402a4a6093eb29c5403858d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc6f0da93c7b4d24886cdaeca8444407": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6cc0a135743248c7b3916da08ac741c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ee2d2cb67094424da359e3f931c39d1c",
              "IPY_MODEL_0323b91c924048c08c431d4240bc0796",
              "IPY_MODEL_d749a97e58dd4a10a970da4d5c05ceb7"
            ],
            "layout": "IPY_MODEL_bf10cfec9fcd47c692a3f2eff1e656ef"
          }
        },
        "ee2d2cb67094424da359e3f931c39d1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87b4138cd9594a9daa8fc6a52b7fa201",
            "placeholder": "​",
            "style": "IPY_MODEL_5d642f6b889641af90e903a9cf7b7396",
            "value": "tokenizer.json: 100%"
          }
        },
        "0323b91c924048c08c431d4240bc0796": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de44d9863d1f445b9bb311ba57a3778c",
            "max": 17209530,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_28fb653584ec48b69a7ec96f2e47f3df",
            "value": 17209530
          }
        },
        "d749a97e58dd4a10a970da4d5c05ceb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33ee762b01484a0ca033a6edc18fe3ae",
            "placeholder": "​",
            "style": "IPY_MODEL_78c18f67904b4b479bb1dac6447bb31a",
            "value": " 17.2M/17.2M [00:00&lt;00:00, 76.3MB/s]"
          }
        },
        "bf10cfec9fcd47c692a3f2eff1e656ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87b4138cd9594a9daa8fc6a52b7fa201": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d642f6b889641af90e903a9cf7b7396": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de44d9863d1f445b9bb311ba57a3778c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28fb653584ec48b69a7ec96f2e47f3df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "33ee762b01484a0ca033a6edc18fe3ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78c18f67904b4b479bb1dac6447bb31a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dcb15209b9624c7c9d1212687f03cdff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6c17d00d7e5d411589ed49462f212e41",
              "IPY_MODEL_6f072122911f42d4be32e2f65dd19341",
              "IPY_MODEL_5927120bfe3545a3a7521d72ff324728"
            ],
            "layout": "IPY_MODEL_a169f943bd50418a9371a5d2b15ce98a"
          }
        },
        "6c17d00d7e5d411589ed49462f212e41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24c2817d795c41438f3f69a5590333e6",
            "placeholder": "​",
            "style": "IPY_MODEL_631ad71aa0d34abaa27a1d88110aff67",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "6f072122911f42d4be32e2f65dd19341": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a127b7a541584287b4cc21018780b3c1",
            "max": 483,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_001331a931d445bbae8b179677021aed",
            "value": 483
          }
        },
        "5927120bfe3545a3a7521d72ff324728": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a136e67ec19b456cb1ef9b1142253444",
            "placeholder": "​",
            "style": "IPY_MODEL_f4711507efca497f8db366f028f40a9f",
            "value": " 483/483 [00:00&lt;00:00, 62.6kB/s]"
          }
        },
        "a169f943bd50418a9371a5d2b15ce98a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24c2817d795c41438f3f69a5590333e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "631ad71aa0d34abaa27a1d88110aff67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a127b7a541584287b4cc21018780b3c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "001331a931d445bbae8b179677021aed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a136e67ec19b456cb1ef9b1142253444": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4711507efca497f8db366f028f40a9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmk4444/System_engineering/blob/main/system_engineering_v15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain openai pypdf chroma streamlit langchain_openai langchain_community langchain transformers bitsandbytes accelerate torch"
      ],
      "metadata": {
        "id": "cp9iCXKFtO2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U faiss-gpu --no-deps"
      ],
      "metadata": {
        "id": "AQH_UgdMgcMo",
        "outputId": "bac6905c-817e-4497-c3b9-b5abeedb7f8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss-gpu\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain_chroma langchain_experimental sentence-transformers cohere rank_bm25 nltk scikit-learn"
      ],
      "metadata": {
        "id": "KzAEXcnqge2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade huggingface-hub transformers"
      ],
      "metadata": {
        "id": "xTFQ81pdtOwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip uninstall -y numpy transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkiV6YAmXnzf",
        "outputId": "755195a6-d3a9-4ba3-8b19-8324407797cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Found existing installation: transformers 4.49.0\n",
            "Uninstalling transformers-4.49.0:\n",
            "  Successfully uninstalled transformers-4.49.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gtx2oAaYYimx",
        "outputId": "16d2a1b8-8c74-412b-bbcc-6f655995f7d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n",
            "  Using cached numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Using cached numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "Using cached transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
            "Installing collected packages: numpy, transformers\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-chroma 0.2.2 requires numpy<2.0.0,>=1.22.4; python_version < \"3.12\", but you have numpy 2.2.4 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.4 transformers-4.49.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import pipeline\n",
        "\n",
        "login(token = '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')"
      ],
      "metadata": {
        "id": "r-6dy_w0tOot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hücre 1: Gerekli Kütüphaneler ve İndirme Fonksiyonları"
      ],
      "metadata": {
        "id": "2qMStg5E_T3F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXoDJDdMtJcA",
        "outputId": "a637847f-277f-4069-c265-eed17ed382b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "8a34cdff52ad4e9898a7fc159da5a505",
            "e9396a48e6eb4909a28dc84c7869b2e0",
            "bfde3864752948df87146da6e94f594b",
            "6b6048c7d4ef48fcab1150b2b1642191",
            "c1f4b45b78984f54995f8e417c8f9fbf",
            "1e7f40c1e0de44c0aa369504d5bbae91",
            "39d1652b4953466e9410ff3aff535fb1",
            "44a479a0f46f4eb9bb8123a0dabff4ac",
            "5ca70e7dbff44fa1ae7c607c7941d131",
            "1d3c69892610448b87b25e3250b3fa43",
            "a8b3d36a18f9401182d12d0c9935b4ae",
            "3bc7b17ce0e14745a881266c8651d6cf",
            "3db7134f1b2d4635bd91b53a6119c5ed",
            "3d51b1a1ad5e464da9a5e0e1d2363b45",
            "262bba1df0214b768d59825603e5155c",
            "b6606a2c7e5c4c07b90a59fdda94f8a9",
            "ecf352c1023b4ec7b904f4b4679220b8",
            "67529f21a6ff4edf92cc7910474588d8",
            "0840d1073cfd4bda90aceec8e347b0e5",
            "fa6d5b72c18743788b76bd0bb13dec61",
            "f0c76f5270bd4b6fbd7695baf7484f0b",
            "50d1d017c2cd4e9aa25aadf42cf24685",
            "737b811014434b7eb28b293806ff2dc1",
            "6bd217b388f74429a520c7492b9ba2a2",
            "b93314acfd9a475cbc02b2b0f9dbed76",
            "5e4ad91df6024df5975deabd192c16ec",
            "8a8cd69c5b1f405daf56a926d9865ae4",
            "d9d2bba920e64a45a0369e2d1ff0864d",
            "da97282966264b9a9192860c5340df3a",
            "8da869c7abdc4221bdf6a0351b58815e",
            "dd650c3f14ee40ce8c94fc7fb03b11ae",
            "2206e719402a4a6093eb29c5403858d8",
            "dc6f0da93c7b4d24886cdaeca8444407",
            "6cc0a135743248c7b3916da08ac741c8",
            "ee2d2cb67094424da359e3f931c39d1c",
            "0323b91c924048c08c431d4240bc0796",
            "d749a97e58dd4a10a970da4d5c05ceb7",
            "bf10cfec9fcd47c692a3f2eff1e656ef",
            "87b4138cd9594a9daa8fc6a52b7fa201",
            "5d642f6b889641af90e903a9cf7b7396",
            "de44d9863d1f445b9bb311ba57a3778c",
            "28fb653584ec48b69a7ec96f2e47f3df",
            "33ee762b01484a0ca033a6edc18fe3ae",
            "78c18f67904b4b479bb1dac6447bb31a",
            "dcb15209b9624c7c9d1212687f03cdff",
            "6c17d00d7e5d411589ed49462f212e41",
            "6f072122911f42d4be32e2f65dd19341",
            "5927120bfe3545a3a7521d72ff324728",
            "a169f943bd50418a9371a5d2b15ce98a",
            "24c2817d795c41438f3f69a5590333e6",
            "631ad71aa0d34abaa27a1d88110aff67",
            "a127b7a541584287b4cc21018780b3c1",
            "001331a931d445bbae8b179677021aed",
            "a136e67ec19b456cb1ef9b1142253444",
            "f4711507efca497f8db366f028f40a9f"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a34cdff52ad4e9898a7fc159da5a505"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/231 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3bc7b17ce0e14745a881266c8651d6cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/53.0k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "737b811014434b7eb28b293806ff2dc1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6cc0a135743248c7b3916da08ac741c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dcb15209b9624c7c9d1212687f03cdff"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Gerekli kütüphanelerin import edilmesi\n",
        "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema.document import Document\n",
        "from langchain.vectorstores.chroma import Chroma\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
        "import torch\n",
        "import os\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "#nltk.download('punkt')\n",
        "#from nltk.tokenize import sent_tokenize\n",
        "\n",
        "templates = {\n",
        "    \"system\": \"You are a professional prompt engineer. Apply the mentioned prompt engineering technique and provide ONLY the improved prompt without any additional commentary or explanations.\",\n",
        "    \"system_multiple\": \"You are a professional prompt engineer. Thoroughly apply EVERY prompt engineering technique listed in the [Prompt Engineering Techniques to Apply] section. Use these techniques to enhance the original prompt provided below, ensuring the enhancement is clear and effective. Provide ONLY the improved version of the prompt without any additional commentary or explanations.\",\n",
        "    \"lang_default\": \"Identify the language of the user's original prompt in the [original] section. You MUST provide the enhanced version of the prompt in the **same language** as the user's original prompt. You'll be penalized if you translate it into another language unless explicitly requested by the user.\",\n",
        "    \"lang_eng\": \"Original prompt is Turkish, first translate it into English before proceeding with the improvement process.It is very important!\",\n",
        "    \"deeper_understanding_simpler\": \"Explain to me as if I’m a beginner in System Engineering. Example: Change \\\"Explain system architecture.\\\" to \\\"Explain system architecture to beginners.\\\"\",\n",
        "    \"task_decomposition_simpler\": \"For complex or multi-step tasks, divide the original prompt into a series of simpler, more manageable sub-prompts. This approach allows the model to focus on one part of the task at a time, generating more detailed and coherent responses for each step.\",\n",
        "    \"fewshot_prompting_simpler\": \"Improve the original prompt by adding a couple of relevant examples that demonstrate the kind of answer or information being requested. Incorporate those examples smoothly into the prompt to make the desired response clear.\"\n",
        "}\n",
        "\n",
        "# Sabitler\n",
        "DATA_PATH = \"/content/drive/MyDrive/data\"  # PDF dosyalarının bulunduğu dizin\n",
        "CHROMA_PATH = \"chroma\"  # Chroma veritabanının saklanacağı dizin\n",
        "\n",
        "\n",
        "from langchain.embeddings.base import Embeddings\n",
        "from typing import List\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class JinaEmbeddings(Embeddings):\n",
        "    def __init__(self, model_name=\"jinaai/jina-embeddings-v3\"):\n",
        "        from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def _get_embedding(self, text: str) -> List[float]:\n",
        "        # Metni tokenize et\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Embeddingi hesapla\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            embeddings = outputs.last_hidden_state.mean(dim=1).to(torch.float32)\n",
        "\n",
        "        # GPU'dan CPU'ya taşı ve numpy dizisine dönüştür\n",
        "        return embeddings.cpu().numpy()[0].tolist()\n",
        "\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Metinler listesi için embedding vektörleri üretir.\"\"\"\n",
        "        return [self._get_embedding(text) for text in texts]\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        \"\"\"Tek bir metin için embedding vektörü üretir.\"\"\"\n",
        "        return self._get_embedding(text)\n",
        "\n",
        "def initialize_embeddings():\n",
        "    \"\"\"Jina embeddings modelini başlatır.\"\"\"\n",
        "    return JinaEmbeddings(model_name=\"jinaai/jina-embeddings-v3\")\n",
        "\n",
        "embeddings = initialize_embeddings()\n",
        "\n",
        "def initialize_colbertv2_model():\n",
        "    \"\"\"\n",
        "    ColBERTv2 tokenizer ve modelini yükler.\n",
        "\n",
        "    Returns:\n",
        "        colbert_tokenizer: Tokenizer nesnesi\n",
        "        colbert_model: Model nesnesi\n",
        "    \"\"\"\n",
        "    # ColBERTv2 tokenizer ve modelini yükleme\n",
        "    colbert_tokenizer = AutoTokenizer.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "    colbert_model = AutoModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "\n",
        "    return colbert_tokenizer, colbert_model\n",
        "\n",
        "# Kullanım:\n",
        "colbert_tokenizer, colbert_model = initialize_colbertv2_model()\n",
        "\n",
        "# Modelin başlatılması\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "#AutoModelForCausalLM\n",
        "#AutoPeftModelForCausalLM\n",
        "\n",
        "def initialize_model():\n",
        "    # 4-bit quantization için gerekli konfigürasyon\n",
        "    load_in_4bit = False\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"kmk4444/Deepseek-r1-8B-Instruct_syseng_vllm_last\",\n",
        "        load_in_4bit=load_in_4bit,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"kmk4444/Deepseek-r1-8B-Instruct_syseng_vllm_last\")\n",
        "    return tokenizer, model\n",
        "\n",
        "# Modeli ve tokenizer'ı başlat\n",
        "tokenizer, model = initialize_model()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hücre 2: Yardımcı Fonksiyonlar"
      ],
      "metadata": {
        "id": "c9n0F7q6_dbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "# Uyarıları kapat\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "def is_prompt_about_engineering(prompt, tokenizer, model):\n",
        "    \"\"\"\n",
        "    Kullanıcının girdiği promptun mühendislikle ilgili olup olmadığını çoklu kontrol mekanizmasıyla değerlendirir.\n",
        "    \"\"\"\n",
        "    # Yasaklı kelimeler ve konular listesi\n",
        "    forbidden_topics = [\n",
        "        # Günlük yaşam & kültür\n",
        "        \"yemek\", \"tarif\", \"içki\", \"kahve\", \"restoran\", \"gezi\", \"tatil\", \"seyahat\",\n",
        "        \"moda\", \"giyim\", \"alışveriş\", \"makyaj\", \"stil\", \"parfüm\", \"kıyafet\",\n",
        "\n",
        "        # Eğlence & medya\n",
        "        \"film\", \"dizi\", \"sinema\", \"televizyon\", \"oyuncu\", \"müzik\", \"konser\",\n",
        "        \"şarkı\", \"albüm\", \"klip\", \"oyun\", \"video oyunu\", \"playstation\", \"xbox\",\n",
        "\n",
        "        # Magazin & sosyal medya\n",
        "        \"ünlü\", \"magazin\", \"dedikodu\", \"influencer\", \"tiktok\", \"instagram\", \"youtube\", \"takipçi\",\n",
        "\n",
        "        # Spor & bahis\n",
        "        \"futbol\", \"basketbol\", \"voleybol\", \"maç\", \"gol\", \"şampiyonluk\", \"puan\", \"lig\",\n",
        "        \"iddaa\", \"bahis\", \"kupon\", \"oran\", \"tahmin\",\n",
        "\n",
        "        # Astroloji & metafizik\n",
        "        \"burç\", \"astroloji\", \"fal\", \"tarot\", \"rüya\", \"enerji\", \"çakra\", \"reiki\", \"spiritüel\",\n",
        "\n",
        "        # Politika & toplum\n",
        "        \"siyaset\", \"hükümet\", \"seçim\", \"parti\", \"milletvekili\", \"bakan\", \"cumhurbaşkanı\",\n",
        "        \"lider\", \"politikacı\", \"anayasa\", \"protesto\", \"miting\",\n",
        "\n",
        "        # Finans & ekonomi\n",
        "        \"borsa\", \"döviz\", \"kripto\", \"bitcoin\", \"altın\", \"yatırım\", \"emlak\", \"faiz\", \"ekonomi\",\n",
        "\n",
        "        # İlişkiler & kişisel\n",
        "        \"ilişki\", \"aşk\", \"sevgili\", \"flört\", \"boşanma\", \"evlilik\", \"kız arkadaş\", \"erkek arkadaş\",\n",
        "        \"duygu\", \"psikoloji\", \"terapi\", \"rüya tabiri\", \"motivasyon\",\n",
        "\n",
        "        # Eğitim dışı genel sohbet\n",
        "        \"espri\", \"şaka\", \"fıkra\", \"bilmece\", \"komik\", \"şarkı sözü\", \"hikaye\", \"masal\"\n",
        "    ]\n",
        "\n",
        "    \"\"\"\n",
        "    # Mühendislik alanına özel anahtar kelimeler\n",
        "    engineering_keywords = [\n",
        "        \"sistem\", \"tasarım\", \"analiz\", \"süreç\", \"optimizasyon\",\n",
        "        \"geliştirme\", \"mühendislik\", \"teknik\", \"model\", \"proje\",\n",
        "        \"verimlilik\", \"performans\", \"requirement\", \"gereksinim\",\n",
        "        \"test\", \"kalite\", \"güvenilirlik\", \"lifecycle\", \"yaşam döngüsü\"\n",
        "    ]\n",
        "    \"\"\"\n",
        "\n",
        "    # Prompt ön işleme\n",
        "    clean_prompt = prompt.lower().strip()\n",
        "\n",
        "    # 1. Yasaklı konuların kontrolü\n",
        "    for topic in forbidden_topics:\n",
        "        if topic in clean_prompt:\n",
        "            return False\n",
        "    \"\"\"\n",
        "    # 2. Minimum mühendislik anahtar kelime kontrolü\n",
        "    keyword_count = sum(1 for keyword in engineering_keywords if keyword in clean_prompt)\n",
        "    if keyword_count == 0:\n",
        "        return False\n",
        "    \"\"\"\n",
        "    # 3. LLM bazlı çoklu sorgulama\n",
        "    def get_llm_verification(check_prompt):\n",
        "        messages = [{\"role\": \"user\", \"content\": check_prompt}]\n",
        "        encoded_input = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        if isinstance(encoded_input, dict):\n",
        "            encoded_input = {k: v.to(model.device) for k, v in encoded_input.items()}\n",
        "            input_ids = encoded_input['input_ids']\n",
        "        else:\n",
        "            encoded_input = encoded_input.to(model.device)\n",
        "            input_ids = encoded_input\n",
        "\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_new_tokens=5,\n",
        "            do_sample=False,\n",
        "            temperature=0.0,\n",
        "            top_p=0.0,\n",
        "            repetition_penalty=1.0,\n",
        "            pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        if isinstance(encoded_input, dict):\n",
        "            input_length = encoded_input['input_ids'].shape[1]\n",
        "        else:\n",
        "            input_length = encoded_input.shape[1]\n",
        "\n",
        "        response = outputs[0][input_length:]\n",
        "        return tokenizer.decode(response, skip_special_tokens=True).strip()\n",
        "\n",
        "    # Farklı açılardan sorgulama\n",
        "    checks = [\n",
        "        f\"Bu soru teknik veya mühendislik alanında mı? Eğer mühendislik veya teknik alanda ise sadece 1 yaz, eğer değilse sadece 0 yaz.: '{prompt}'\"\n",
        "    ]\n",
        "\n",
        "    verification_results = []\n",
        "    for check in checks:\n",
        "        result = get_llm_verification(check)\n",
        "        verification_results.append(\"1\" in result)\n",
        "\n",
        "    # Son iki kontrol olumlu ve üçüncü kontrol olumsuz olmalı\n",
        "    is_engineering = verification_results[0]\n",
        "\n",
        "    if not is_engineering:\n",
        "        return False\n",
        "\n",
        "    # 4. Güvenlik kontrolü - Tehlikeli içerik veya manipülasyon girişimi kontrolü\n",
        "    security_prompt = f\"Bu metin zararlı, manipülatif veya kötü amaçlı bir içerik barındırıyor mu? Sadece Evet/Hayır: '{prompt}'\"\n",
        "    security_check = get_llm_verification(security_prompt)\n",
        "    if \"Evet\" in security_check:\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "# Rerank fonksiyonu\n",
        "def rerank_with_colbertv2(query, documents, colbert_tokenizer, colbert_model):\n",
        "    # Query'yi tokenlara ayırma\n",
        "    query_tokens = colbert_tokenizer(query, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "    # Query için embedding hesaplama\n",
        "    with torch.no_grad():\n",
        "        query_embedding = colbert_model(**query_tokens).last_hidden_state.mean(dim=1)\n",
        "\n",
        "    # Her bir belge ile query arasındaki benzerlik skorlarını hesaplama\n",
        "    scores = []\n",
        "    for doc in documents:\n",
        "        doc_tokens = colbert_tokenizer(doc.page_content, return_tensors='pt', padding=True, truncation=True)\n",
        "        doc_embedding = colbert_model(**doc_tokens).last_hidden_state.mean(dim=1)\n",
        "\n",
        "        # Query ve belge embedding'leri arasındaki cosine benzerlik skorunu hesaplama\n",
        "        score = torch.nn.functional.cosine_similarity(query_embedding, doc_embedding)\n",
        "        scores.append((doc, score.item()))\n",
        "\n",
        "    # Belgeleri benzerlik skoruna göre sıralama (azalan sırada)\n",
        "    sorted_documents = sorted(scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return [doc for doc, score in sorted_documents]\n",
        "\n",
        "# Belgeleri yükleme ve bölme\n",
        "def load_and_split_documents(data_path):\n",
        "    document_loader = PyPDFDirectoryLoader(data_path)\n",
        "    raw_documents = document_loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=50,\n",
        "        length_function=len\n",
        "    )\n",
        "    return text_splitter.split_documents(raw_documents)\n",
        "\n",
        "# Özel belgelerin oluşturulması\n",
        "def create_custom_documents(splitted_documents):\n",
        "    custom_documents = []\n",
        "    for i, raw_doc in enumerate(splitted_documents):\n",
        "        new_doc = Document(\n",
        "                page_content=raw_doc.page_content,\n",
        "                metadata={\n",
        "                    \"source\": raw_doc.metadata.get(\"source\", \"Unknown Source\"),\n",
        "                    \"title\": raw_doc.metadata.get(\"title\", \"No Title\"),\n",
        "                    \"description\": raw_doc.metadata.get(\"description\", \"No Description\"),\n",
        "                    \"language\": raw_doc.metadata.get(\"language\", \"Unknown Language\"),\n",
        "                    \"doc_id\": i\n",
        "                }\n",
        "        )\n",
        "        custom_documents.append(new_doc)\n",
        "    return custom_documents\n",
        "\n",
        "# Vectorstore ve Retriever'ın başlatılması\n",
        "def initialize_vectorstore(custom_documents, embeddings_model, persist_directory):\n",
        "    try:\n",
        "        vectorstore = Chroma.from_documents(\n",
        "            documents=custom_documents,\n",
        "            embedding=embeddings_model,  # embeddings_model'i doğrudan kullan\n",
        "            persist_directory=persist_directory\n",
        "        )\n",
        "\n",
        "        return vectorstore.as_retriever(\n",
        "            search_type=\"mmr\",\n",
        "            search_kwargs={'k': 5, 'lambda_mult': 0.40}\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Vectorstore başlatılırken hata oluştu: {str(e)}\")\n",
        "        raise e\n",
        "\n",
        "# İlgili belgelerin getirilmesi\n",
        "def retrieve_relevant_documents(retriever, prompt):\n",
        "    return retriever.get_relevant_documents(prompt)\n",
        "\n",
        "def get_relevant_documents_with_bm25(documents, query):\n",
        "    bm25_retriever = BM25Retriever.from_documents(documents=documents)\n",
        "    bm25_retriever.k = 5\n",
        "\n",
        "    bm25_relevant_documents = bm25_retriever.get_relevant_documents(query=query)\n",
        "\n",
        "    return bm25_relevant_documents, bm25_retriever\n",
        "\n",
        "def get_relevant_documents_for_hybrid_search(query, retriever1, retriever2, weight1, weight2):\n",
        "    ensemble_retriever = EnsembleRetriever(\n",
        "                                retrievers=[retriever1, retriever2],\n",
        "                                weights=[weight1, weight2])\n",
        "\n",
        "    hybrid_relevant_documents = ensemble_retriever.get_relevant_documents(query)\n",
        "\n",
        "    return hybrid_relevant_documents\n",
        "\n",
        "# Final promptun oluşturulması\n",
        "def generate_final_prompt(prompt, context_data, relevant_documents):\n",
        "    #history_prompt = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in chat_history])\n",
        "    metadata_info = \"\\n\".join([f\"Belge {doc.metadata['doc_id']} - Başlık: {doc.metadata['title']}, Kaynak: {doc.metadata['source']}\" for doc in relevant_documents])\n",
        "    return  f\"\"\"\n",
        "###Instruction###\n",
        "You are an expert assistant dedicated to providing answers for beginner systems engineers. Follow these guidelines:\n",
        "\n",
        "1. Deliver clear, concise, and expert-level information in Turkish.\n",
        "2. Use the provided documents (###Context###) and historical conversation (###Previous Conversations###) data to answer questions (###Question###). Reference these documents while interpreting and generating your answers.\n",
        "3. Ensure that your answers are coherent and free from any repetitive or duplicate content.\n",
        "4. Each sentence should provide unique and valuable information.\n",
        "5. You have to answer just in TURKISH.\n",
        "\n",
        "\n",
        "###Question###\n",
        "{prompt}\n",
        "\n",
        "###Context###\n",
        "This is the information we have to answer the question: {context_data}\n",
        "\"\"\", metadata_info\n",
        "\n",
        "# Tekrarları kaldırmak için fonksiyon\n",
        "def remove_repetitions(response, threshold=0.7):\n",
        "    # Cümlelere bölme\n",
        "    sentences = sent_tokenize(response)\n",
        "\n",
        "    # TF-IDF vektörizer oluşturma\n",
        "    vectorizer = TfidfVectorizer().fit_transform(sentences)\n",
        "    vectors = vectorizer.toarray()\n",
        "\n",
        "    # Cosine similarity hesaplama\n",
        "    cosine_matrix = cosine_similarity(vectors)\n",
        "\n",
        "    # Tekrar eden cümleleri tespit etme ve kaldırma\n",
        "    unique_sentences = []\n",
        "    seen_indices = set()\n",
        "\n",
        "    for i in range(len(sentences)):\n",
        "        if i in seen_indices:\n",
        "            continue\n",
        "        unique_sentences.append(sentences[i])\n",
        "        for j in range(i + 1, len(sentences)):\n",
        "            if cosine_matrix[i, j] > threshold:\n",
        "                seen_indices.add(j)\n",
        "\n",
        "    # Tekrarları kaldırılmış yanıtı yeniden oluşturma\n",
        "    cleaned_response = ' '.join(unique_sentences)\n",
        "\n",
        "    return cleaned_response\n",
        "\n",
        "# RAG ile çoklu PDF'ler\n",
        "def rag_with_multiple_pdfs(prompt):\n",
        "    embedding_model = initialize_embeddings()  # Her seferinde yeni bir embeddings nesnesi oluştur\n",
        "    splitted_documents = load_and_split_documents(DATA_PATH)\n",
        "    custom_documents = create_custom_documents(splitted_documents)\n",
        "\n",
        "    chroma_retriever = initialize_vectorstore(custom_documents, embedding_model, CHROMA_PATH)\n",
        "    chroma_relevant_documents = retrieve_relevant_documents(chroma_retriever, prompt)\n",
        "\n",
        "    bm25_documents, bm25retriever = get_relevant_documents_with_bm25(custom_documents,prompt)\n",
        "\n",
        "    weight1 = 0.2\n",
        "    hybrid_search_documents = get_relevant_documents_for_hybrid_search(\n",
        "        query=prompt,\n",
        "        retriever1=bm25retriever,\n",
        "        retriever2=chroma_retriever,\n",
        "        weight1=weight1,\n",
        "        weight2=1-weight1\n",
        "    )\n",
        "\n",
        "    # Hibrit arama sonuçlarını ColBERTv2 ile yeniden sıralama\n",
        "    reranked_documents = rerank_with_colbertv2(prompt, hybrid_search_documents, colbert_tokenizer, colbert_model)\n",
        "\n",
        "    context_data = \" \".join([doc.page_content for doc in reranked_documents])\n",
        "    #cleaned_context_data = remove_repetitions(context_data)\n",
        "    final_prompt, metadata_info = generate_final_prompt(prompt, context_data, reranked_documents)\n",
        "    return final_prompt, metadata_info, chroma_relevant_documents, bm25_documents, reranked_documents\n",
        "\n",
        "def generate_prompt_engineering(prompt, tokenizer, model):\n",
        "    system_message = templates[\"lang_eng\"]\n",
        "    system_message += '\\n' + templates[\"system_multiple\"]\n",
        "\n",
        "    skills = [\"deeper_understanding\", \"task_decomposition\", \"fewshot_prompting\"]\n",
        "    integrated_templates = \"[Prompt Engineering Techniques to Apply]\\n\"\n",
        "\n",
        "    for idx, skill in enumerate(skills):\n",
        "        template = templates[f\"{skill}_simpler\"]\n",
        "        integrated_templates += f\"{idx+1}. {skill}: {template}\\n\"\n",
        "    integrated_templates += \"Based on [Prompt engineering techniques to apply], refine the prompt provided below. Ensure that each technique is fully incorporated to achieve a clear and effective improvement:\\n\\n[original]\\n{prompt}\\n[improved]\\n\"\n",
        "\n",
        "    prompt_template = PromptTemplate.from_template(integrated_templates)\n",
        "    formatted_input = prompt_template.format(prompt=prompt)\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": formatted_input}\n",
        "    ]\n",
        "\n",
        "    # input_ids ve attention_mask alınarak giriş hazırlanır\n",
        "    encoded_input = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # GPU'ya taşıma işlemi\n",
        "    encoded_input = {k: v.to(model.device) for k, v in encoded_input.items()} \\\n",
        "        if isinstance(encoded_input, dict) else encoded_input.to(model.device)\n",
        "\n",
        "    # Model çıktısı üretilir\n",
        "    outputs = model.generate(\n",
        "        input_ids=encoded_input if not isinstance(encoded_input, dict) else encoded_input['input_ids'],\n",
        "        max_new_tokens=1024,\n",
        "        do_sample=True,\n",
        "        temperature=0.75,\n",
        "        top_p=0.90,\n",
        "        repetition_penalty=1.2,\n",
        "        pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Yanıtı decode etme\n",
        "    if isinstance(encoded_input, dict):\n",
        "        input_length = encoded_input['input_ids'].shape[1]\n",
        "    else:\n",
        "        input_length = encoded_input.shape[1]\n",
        "\n",
        "    response = outputs[0][input_length:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "# LLaMA3 yanıtının üretilmesi\n",
        "def generate_llama3_response(prompt_input, tokenizer, model):\n",
        "    SYS_PROMPT = \"\"\"You are an expert assistant dedicated to guiding and empowering beginner systems engineers. Follow these guidelines:\n",
        "1. Provide clear, concise, and expert-level information in Turkish.\n",
        "2. Use the provided documents and historical conversation data to deliver insightful answers. Previous conversation history and document information will be given to you in the user's prompt.\n",
        "3. Ensure that your answers are free from any repetitive or duplicate content and sentences. You have to eliminate repetitive sentences. DON'T REPEAT THE SAME SENTENCES!\n",
        "4. If unsure, confidently state \"Bilmiyorum\" without speculating.\n",
        "5. Your responses should be motivational and tailored to empower beginners in understanding systems engineering principles.\n",
        "6. Only respond in TURKISH.\"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": prompt_input}\n",
        "    ]\n",
        "\n",
        "    # input_ids ve attention_mask alınarak giriş hazırlanır\n",
        "    encoded_input = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # GPU'ya taşıma işlemi\n",
        "    encoded_input = {k: v.to(model.device) for k, v in encoded_input.items()} \\\n",
        "        if isinstance(encoded_input, dict) else encoded_input.to(model.device)\n",
        "\n",
        "    # Model çıktısı üretilir\n",
        "    outputs = model.generate(\n",
        "        input_ids=encoded_input if not isinstance(encoded_input, dict) else encoded_input['input_ids'],\n",
        "        min_new_tokens=512,\n",
        "        max_new_tokens=4096,\n",
        "        do_sample=True,\n",
        "        temperature=0.50,\n",
        "        top_p=0.90,\n",
        "        repetition_penalty=1.2,\n",
        "        pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Yanıtı decode etme\n",
        "    if isinstance(encoded_input, dict):\n",
        "        input_length = encoded_input['input_ids'].shape[1]\n",
        "    else:\n",
        "        input_length = encoded_input.shape[1]\n",
        "\n",
        "    response = outputs[0][input_length:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "def evaluate_answer_relevance(question, answer, tokenizer, model):\n",
        "    \"\"\"\n",
        "    Dil modelini kullanarak cevabın soruya uygun olup olmadığını değerlendirir.\n",
        "    \"\"\"\n",
        "    evaluation_prompt = f\"Soru: {question}\\nCevap: {answer}\\nBu cevap soruyu doğru ve tam olarak yanıtlıyor mu? Lütfen 'Evet' veya 'Hayır' şeklinde cevap ver.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": evaluation_prompt}\n",
        "    ]\n",
        "\n",
        "    # input_ids ve attention_mask alınarak giriş hazırlanır\n",
        "    encoded_input = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # GPU'ya taşıma işlemi\n",
        "    encoded_input = {k: v.to(model.device) for k, v in encoded_input.items()} \\\n",
        "        if isinstance(encoded_input, dict) else encoded_input.to(model.device)\n",
        "\n",
        "    # Model çıktısı üretilir\n",
        "    outputs = model.generate(\n",
        "        input_ids=encoded_input if not isinstance(encoded_input, dict) else encoded_input['input_ids'],\n",
        "        max_new_tokens=50,\n",
        "        do_sample=False,\n",
        "        temperature=0.0,\n",
        "        top_p=0.0,\n",
        "        repetition_penalty=1.0,\n",
        "        pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Yanıtı decode etme\n",
        "    if isinstance(encoded_input, dict):\n",
        "        input_length = encoded_input['input_ids'].shape[1]\n",
        "    else:\n",
        "        input_length = encoded_input.shape[1]\n",
        "\n",
        "    evaluation_response = outputs[0][input_length:]\n",
        "    evaluation_text = tokenizer.decode(evaluation_response, skip_special_tokens=True).strip()\n",
        "\n",
        "    # Cevabı değerlendirme\n",
        "    if \"Evet\" in evaluation_text:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "def select_best_response(question, responses, tokenizer, model):\n",
        "    \"\"\"\n",
        "    Dil modelini kullanarak verilen cevaplar arasından en iyi olanı seçer.\n",
        "\n",
        "    Args:\n",
        "        question (str): Kullanıcının sorusu.\n",
        "        responses (list): Cevapların listesi.\n",
        "        tokenizer: Tokenizer nesnesi.\n",
        "        model: Dil modeli nesnesi.\n",
        "\n",
        "    Returns:\n",
        "        str: En iyi cevap.\n",
        "    \"\"\"\n",
        "    # Cevapları numaralandır\n",
        "    numbered_responses = \"\\n\".join([f\"{idx+1}. {resp}\" for idx, resp in enumerate(responses)])\n",
        "\n",
        "    selection_prompt = f\"Kullanıcının sorusu: {question}\\n\\nVerilen cevaplar:\\n{numbered_responses}\\n\\nBu cevaplar arasından soruya en iyi yanıt veren hangisidir? Lütfen sadece cevabın numarasını yaz.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": selection_prompt}\n",
        "    ]\n",
        "\n",
        "    # input_ids ve attention_mask alınarak giriş hazırlanır\n",
        "    encoded_input = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # GPU'ya taşıma işlemi\n",
        "    encoded_input = {k: v.to(model.device) for k, v in encoded_input.items()} \\\n",
        "        if isinstance(encoded_input, dict) else encoded_input.to(model.device)\n",
        "\n",
        "    # Model çıktısı üretilir\n",
        "    outputs = model.generate(\n",
        "        input_ids=encoded_input if not isinstance(encoded_input, dict) else encoded_input['input_ids'],\n",
        "        max_new_tokens=10,\n",
        "        do_sample=False,\n",
        "        temperature=0.0,\n",
        "        top_p=0.0,\n",
        "        repetition_penalty=1.0,\n",
        "        pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Yanıtı decode etme\n",
        "    if isinstance(encoded_input, dict):\n",
        "        input_length = encoded_input['input_ids'].shape[1]\n",
        "    else:\n",
        "        input_length = encoded_input.shape[1]\n",
        "\n",
        "    selection_response = outputs[0][input_length:]\n",
        "    selection_text = tokenizer.decode(selection_response, skip_special_tokens=True).strip()\n",
        "\n",
        "    # Seçilen numarayı al\n",
        "    try:\n",
        "        selected_index = int(selection_text.split('.')[0].strip()) - 1\n",
        "        if 0 <= selected_index < len(responses):\n",
        "            return responses[selected_index]\n",
        "        else:\n",
        "            # Geçersiz seçim durumunda ilk cevabı döndür\n",
        "            return responses[0]\n",
        "    except ValueError:\n",
        "        # Anlaşılamayan yanıt durumunda ilk cevabı döndür\n",
        "        return responses[0]\n"
      ],
      "metadata": {
        "id": "uC9NIW8q_ZDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hücre 3: Ana Kod Çalıştırma"
      ],
      "metadata": {
        "id": "enICwb7K_jXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.testgorilla.com/blog/system-engineer-interview-questions/\n",
        "\n",
        "bazen iyileştirilmiş promptta ingilizce yapmıyor, bunu çözeceğim.\n",
        "\n",
        "finetuning dosyasını değiştereceğim.\n",
        "\n",
        "ayrıca referans dosyaları göstereceğim, print yaparken.\n",
        "\n",
        "bide kullanıcı 0 girene kadar cevap verme olsun. ayrıca print kısmında paragraflar opsiyonel olsun.\n",
        "\n",
        "forbidden_topics konuları geliştirilecektir."
      ],
      "metadata": {
        "id": "-sIFadnOx47H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Modeli ve tokenizer'ı başlat\n",
        "    # tokenizer, model = initialize_model()\n",
        "\n",
        "    # Sohbet geçmişini başlat\n",
        "    chat_history = []\n",
        "\n",
        "    prompt = input(\"Mesajınızı Giriniz: \")\n",
        "\n",
        "    #while True:\n",
        "    #    # Kullanıcı girdisini al\n",
        "    #    prompt = input(\"Mesajınızı Giriniz: \")\n",
        "\n",
        "    #    # Kullanıcının girdiği promptu kontrol et\n",
        "    #    if not is_prompt_about_engineering(prompt, tokenizer, model):\n",
        "    #        print(\"Girdiğiniz soru sistem mühendisliği alanı ile ilgili değildir. Lütfen sistem mühendisliği ile ilgili bir soru sorunuz.\\n\")\n",
        "    #        continue  # Kullanıcıdan tekrar girdi al\n",
        "    #    else:\n",
        "    #        break  # Döngüden çık ve devam et\n",
        "\n",
        "    # Kullanıcı mesajını sohbet geçmişine ekle\n",
        "    chat_history.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    # Optimize edilmiş promptu oluştur\n",
        "    optimized_prompt = generate_prompt_engineering(prompt, tokenizer, model)\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"OPTİMİZE EDİLMİŞ PROMPT:\")\n",
        "    print(\"=\"*50)\n",
        "    print(optimized_prompt)\n",
        "\n",
        "    # Final promptu oluştur ve belgeleri getir\n",
        "    final_prompt, metadata_info, chroma_relevant_documents, bm25_documents, hybrid_search_documents = rag_with_multiple_pdfs(optimized_prompt)\n",
        "\n",
        "    # BM25 belgelerini yazdır\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"KARAKTER BAZLI ARAMA | BM25:\")\n",
        "    print(\"=\"*50)\n",
        "    for doc in bm25_documents:\n",
        "        print(f\"ID: {doc.metadata['doc_id']} || {doc.page_content}\\n\")\n",
        "\n",
        "    # Hibrit Arama belgelerini yazdır\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"HİBRİT ARAMA:\")\n",
        "    print(\"=\"*50)\n",
        "    for doc in hybrid_search_documents:\n",
        "        print(f\"ID: {doc.metadata['doc_id']} || {doc.page_content}\\n\")\n",
        "\n",
        "    # Semantik Arama belgelerini yazdır (Chroma'dan)\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"SEMANTİK ARAMA:\")\n",
        "    print(\"=\"*50)\n",
        "    for doc in chroma_relevant_documents:\n",
        "        print(f\"ID: {doc.metadata['doc_id']} || {doc.page_content}\\n\")\n",
        "\n",
        "    max_attempts = 1  # Maksimum deneme sayısı\n",
        "    attempts = 0\n",
        "    is_relevant = False\n",
        "    previous_responses = []  # Önceki cevapları tutmak için liste\n",
        "\n",
        "    while attempts < max_attempts and not is_relevant:\n",
        "        # Yanıtı üret\n",
        "        if attempts == 0:\n",
        "            response = generate_llama3_response(final_prompt, tokenizer, model)\n",
        "        else:\n",
        "            # Geri bildirimle yeni prompt oluştur\n",
        "            feedback = f\"Cevabın kullanıcının sorusuna tam olarak yanıt vermedi. Önceki cevabın: '{previous_responses[-1]}'. Lütfen kullanıcının sorusuna odaklanarak daha uygun bir cevap ver.\"\n",
        "            final_prompt_with_feedback = f\"{feedback}\\n\\n{final_prompt}\"\n",
        "            response = generate_llama3_response(final_prompt_with_feedback, tokenizer, model)\n",
        "\n",
        "        # Cevabın uygunluğunu kontrol et\n",
        "        is_relevant = evaluate_answer_relevance(prompt, response, tokenizer, model)\n",
        "\n",
        "        # Önceki cevapları listeye ekle\n",
        "        previous_responses.append(response)\n",
        "        attempts += 1\n",
        "\n",
        "    if not is_relevant:\n",
        "        # Üç cevap arasından en iyisini seç\n",
        "        best_response = select_best_response(prompt, previous_responses, tokenizer, model)\n",
        "        response = best_response\n",
        "\n",
        "    # Uygun cevap sohbet geçmişine eklenir\n",
        "    chat_history.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ASİSTAN YANITI:\")\n",
        "    print(\"=\"*50)\n",
        "    print(response)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "ACv14X5D_hgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "# Uyarıları kapat\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "# Excel dosyasından soruları oku\n",
        "excel_path = \"/content/Deepseek-r1.xlsx\"\n",
        "df_sorular = pd.read_excel(excel_path, usecols=None)  # Tüm sütunları oku\n",
        "\n",
        "# Sütun başlıklarını kontrol et ve ikinci sütunu (sorular) seç\n",
        "if df_sorular.shape[1] < 2:\n",
        "    raise ValueError(\"Excel dosyasında en az iki sütun olmalıdır. Lütfen verileri kontrol edin.\")\n",
        "\n",
        "sorular = df_sorular.iloc[:, 1].dropna().tolist()  # İkinci sütundaki verileri al\n",
        "\n",
        "def main():\n",
        "    # Modeli ve tokenizer'ı başlat\n",
        "    # tokenizer, model = initialize_model()\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for i, soru in enumerate(sorular, start=1):\n",
        "        print(f\"{i}. Soru soruluyor: {soru}\")\n",
        "\n",
        "        # Optimize edilmiş promptu oluştur\n",
        "        optimized_prompt = generate_prompt_engineering(soru, tokenizer, model)\n",
        "\n",
        "        # Final promptu oluştur ve belgeleri getir\n",
        "        final_prompt, metadata_info, chroma_relevant_documents, bm25_documents, hybrid_search_documents = rag_with_multiple_pdfs(optimized_prompt)\n",
        "\n",
        "        # BM25 belgelerini yazdır\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"KARAKTER BAZLI ARAMA | BM25:\")\n",
        "        print(\"=\"*50)\n",
        "        for doc in bm25_documents:\n",
        "            print(f\"ID: {doc.metadata['doc_id']} || {doc.page_content}\\n\")\n",
        "\n",
        "        # Hibrit Arama belgelerini yazdır\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"HİBRİT ARAMA:\")\n",
        "        print(\"=\"*50)\n",
        "        for doc in hybrid_search_documents:\n",
        "            print(f\"ID: {doc.metadata['doc_id']} || {doc.page_content}\\n\")\n",
        "\n",
        "        # Semantik Arama belgelerini yazdır (Chroma'dan)\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"SEMANTİK ARAMA:\")\n",
        "        print(\"=\"*50)\n",
        "        for doc in chroma_relevant_documents:\n",
        "            print(f\"ID: {doc.metadata['doc_id']} || {doc.page_content}\\n\")\n",
        "\n",
        "        max_attempts = 1  # Maksimum deneme sayısı\n",
        "        attempts = 0\n",
        "        is_relevant = False\n",
        "        previous_responses = []  # Önceki cevapları tutmak için liste\n",
        "\n",
        "        while attempts < max_attempts and not is_relevant:\n",
        "            # Yanıtı üret\n",
        "            if attempts == 0:\n",
        "                response = generate_llama3_response(final_prompt, tokenizer, model)\n",
        "            else:\n",
        "                # Geri bildirimle yeni prompt oluştur\n",
        "                feedback = f\"Cevabın kullanıcının sorusuna tam olarak yanıt vermedi. Önceki cevabın: '{previous_responses[-1]}'. Lütfen kullanıcının sorusuna odaklanarak daha uygun bir cevap ver.\"\n",
        "                final_prompt_with_feedback = f\"{feedback}\\n\\n{final_prompt}\"\n",
        "                response = generate_llama3_response(final_prompt_with_feedback, tokenizer, model)\n",
        "\n",
        "            # Cevabın uygunluğunu kontrol et\n",
        "            is_relevant = evaluate_answer_relevance(soru, response, tokenizer, model)\n",
        "\n",
        "            # Önceki cevapları listeye ekle\n",
        "            previous_responses.append(response)\n",
        "            attempts += 1\n",
        "\n",
        "        if not is_relevant:\n",
        "            # Üç cevap arasından en iyisini seç\n",
        "            best_response = select_best_response(soru, previous_responses, tokenizer, model)\n",
        "            response = best_response\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"ASİSTAN YANITI:\")\n",
        "        print(\"=\"*50)\n",
        "        print(response)\n",
        "\n",
        "        # Sonucu kaydet\n",
        "        results.append([soru, response])\n",
        "\n",
        "        # Her sorudan sonra Excel dosyasını güncelle\n",
        "        df_sorular.loc[df_sorular.iloc[:, 1] == soru, \"Cevap\"] = response\n",
        "        output_path = \"sistem_muhendisligi_cevaplar.xlsx\"\n",
        "        df_sorular.to_excel(output_path, index=False)\n",
        "        print(f\"Güncellenmiş sonuçlar kaydedildi: {output_path}\")\n",
        "\n",
        "        # API isteklerini sınırlamak için kısa bir bekleme süresi ekleyelim (gerekirse)\n",
        "        time.sleep(2)\n",
        "\n",
        "    print(\"Tüm sorular tamamlandı.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "_29rKdYptQqo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae3b14ac-eda9-4fe7-c7f6-0aed53d101f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Soru soruluyor: Sistem mühendisliği nedir ve temel amacı nedir?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "KARAKTER BAZLI ARAMA | BM25:\n",
            "==================================================\n",
            "ID: 442 || Any special instructions for operating data recording \n",
            "equipment or other automated test equipment as \n",
            "applicable;\n",
            "Copy of the as-run procedure (may include redlines); and\n",
            "Layouts, schematics, or diagrams showing identification, \n",
            "location, and interconnection of test equipment, test \n",
            "articles, and measuring points and any other associated \n",
            "design or configuration work products;\n",
            "Authentication of test results and authorization of \n",
            "acceptability.\n",
            "Identification of hazardous situations or operations;\n",
            "Precautions and safety instructions to ensure safety of \n",
            "personnel and prevent degradation of test articles and \n",
            "measuring equipment;\n",
            "Environmental and/or other conditions to be maintained \n",
            "with tolerances;\n",
            "Constraints on inspection or testing;\n",
            "Provision or instructions for the recording of verification \n",
            "results and other artifacts;\n",
            "Special instructions for instances of nonconformance and \n",
            "anomalous occurrences or results; and\n",
            "Specifications for facility, equipment maintenance,\n",
            "\n",
            "ID: 0 || TWELVE SYSTEMS ENGINEERING ROLES \n",
            " \n",
            "Sarah A. Sheard \n",
            "Software Productivity Consortium \n",
            "2214 Rock Hill Rd, Herndon VA 22070 \n",
            "sheard@software.org, (703) 742-7106 \n",
            " \n",
            " \n",
            "Abstract. Twel ve rol es are described which are \n",
            "occasionally or frequently assum ed to constitute the \n",
            "practice of systems engineering. Som e roles fit \n",
            "naturally as life-cycle ro les, o thers fit th e Pro gram \n",
            "Management set o f ro les, wh ile still others are not \n",
            "normally thought of i n ei ther group. Int eractions \n",
            "between the roles are discussed, and the system s \n",
            "engineering roles assumed by  t he papers i n t he \n",
            "inaugural issue of Systems Engineering, the Journal of \n",
            "INCOSE, are compared to these categories. \n",
            "INTRODUCTION \n",
            "Since its in ception, INCOSE h as b een attem pting \n",
            "to resolve the question o f wh at, ex actly, is system s \n",
            "engineering. Several dualities h ave b een ex plored, \n",
            "including whet her sy stems engi neers are specialists or \n",
            "generalists, and whether systems engineering is a set of\n",
            "\n",
            "ID: 2 || the roles described in this paper. \n",
            "To deri ve t hese t welve systems engineering roles, \n",
            "papers i n t he i naugural i ssue of Systems Engi neering, \n",
            "the Journal of INCOSE, were reviewed for assumptions \n",
            "                            \n",
            "1  “What is the Value of S ystems Engineering,” at the \n",
            "Washington Metropolitan Area ch apter, October 10, 1995. \n",
            "Panelists included Dorothy  Mc Kinney, Andrew Sage, Dale \n",
            "Langston, and John Snoderly. \n",
            "about rol es t hat sy stems engi neers pl ay. M ore than \n",
            "sixty descri ptions of rol es were col lected and grouped \n",
            "into t he t welve groupi ngs present ed bel ow. Then four \n",
            "years of INCOSE sym posium proceedings were \n",
            "scanned to ensure that m ost of the possible systems \n",
            "engineering rol es were capt ured. The intent was to \n",
            "include roles applicable both to the ty pical DOD and \n",
            "aerospace environm ent and to less standard systems \n",
            "engineering envi ronments such as sm aller program s \n",
            "and commercial co mpanies. Fin ally, th e Washington\n",
            "\n",
            "ID: 82 || National Aeronautics and \n",
            " Space Administration\n",
            "NASA \n",
            "SYSTEMS ENGINEERING \n",
            "HANDBOOK\n",
            "design\n",
            "test\n",
            "integrate\n",
            "fly\n",
            "www.nasa.gov\n",
            "\n",
            "ID: 1232 || 25 \n",
            " \n",
            " \n",
            " \n",
            "2. Develop required Integration Plans,  \n",
            "3. Develop integration test scripts,  \n",
            "4. Develop and implement integration test scenarios,  \n",
            "5. Conduct and document integration tests,  \n",
            "6. Track integration test results and retest status. \n",
            " \n",
            "Example Validation and Verification \n",
            "Validation and verification shall be by collection and analysis of the metrics defined by Systems \n",
            "Integration metrics in Appendix B – Metrics Guide.  \n",
            " \n",
            "2.2.11 Specialty Engineering \n",
            "In the domain of systems engineering, Specialty Engineering is defined as and includes the \n",
            "engineering disciplines that are not typical of the main engineering effort. More common \n",
            "engineering efforts in systems engineering such as hardware, software, and h uman factors \n",
            "engineering may be used as major elements in a majority of systems engineering efforts and \n",
            "therefore are not viewed as \"special\".  Examples of specialty engineering include\n",
            "\n",
            "\n",
            "==================================================\n",
            "HİBRİT ARAMA:\n",
            "==================================================\n",
            "ID: 2 || the roles described in this paper. \n",
            "To deri ve t hese t welve systems engineering roles, \n",
            "papers i n t he i naugural i ssue of Systems Engi neering, \n",
            "the Journal of INCOSE, were reviewed for assumptions \n",
            "                            \n",
            "1  “What is the Value of S ystems Engineering,” at the \n",
            "Washington Metropolitan Area ch apter, October 10, 1995. \n",
            "Panelists included Dorothy  Mc Kinney, Andrew Sage, Dale \n",
            "Langston, and John Snoderly. \n",
            "about rol es t hat sy stems engi neers pl ay. M ore than \n",
            "sixty descri ptions of rol es were col lected and grouped \n",
            "into t he t welve groupi ngs present ed bel ow. Then four \n",
            "years of INCOSE sym posium proceedings were \n",
            "scanned to ensure that m ost of the possible systems \n",
            "engineering rol es were capt ured. The intent was to \n",
            "include roles applicable both to the ty pical DOD and \n",
            "aerospace environm ent and to less standard systems \n",
            "engineering envi ronments such as sm aller program s \n",
            "and commercial co mpanies. Fin ally, th e Washington\n",
            "\n",
            "ID: 0 || TWELVE SYSTEMS ENGINEERING ROLES \n",
            " \n",
            "Sarah A. Sheard \n",
            "Software Productivity Consortium \n",
            "2214 Rock Hill Rd, Herndon VA 22070 \n",
            "sheard@software.org, (703) 742-7106 \n",
            " \n",
            " \n",
            "Abstract. Twel ve rol es are described which are \n",
            "occasionally or frequently assum ed to constitute the \n",
            "practice of systems engineering. Som e roles fit \n",
            "naturally as life-cycle ro les, o thers fit th e Pro gram \n",
            "Management set o f ro les, wh ile still others are not \n",
            "normally thought of i n ei ther group. Int eractions \n",
            "between the roles are discussed, and the system s \n",
            "engineering roles assumed by  t he papers i n t he \n",
            "inaugural issue of Systems Engineering, the Journal of \n",
            "INCOSE, are compared to these categories. \n",
            "INTRODUCTION \n",
            "Since its in ception, INCOSE h as b een attem pting \n",
            "to resolve the question o f wh at, ex actly, is system s \n",
            "engineering. Several dualities h ave b een ex plored, \n",
            "including whet her sy stems engi neers are specialists or \n",
            "generalists, and whether systems engineering is a set of\n",
            "\n",
            "ID: 82 || National Aeronautics and \n",
            " Space Administration\n",
            "NASA \n",
            "SYSTEMS ENGINEERING \n",
            "HANDBOOK\n",
            "design\n",
            "test\n",
            "integrate\n",
            "fly\n",
            "www.nasa.gov\n",
            "\n",
            "ID: 116 || achieving stakeholder functional, physical, and oper-\n",
            "ational performance requirements in the intended \n",
            "use environment over the planned life of the system \n",
            "within cost, schedule, and other constraints. It is a \n",
            "methodology that supports the containment of the \n",
            "life cycle cost of a system. In other words, systems \n",
            "engineering is a logical way of thinking.\n",
            "1 Eberhardt Rechtin, Systems Architecting of Organizations: \n",
            "Why Eagles Can’t Swim.\n",
            "Systems engineering is the art and science of devel -\n",
            "oping an operable system capable of meeting require-\n",
            "ments within often opposed constraints. Systems \n",
            "engineering is a holistic, integrative discipline, \n",
            "wherein the contributions of structural engineers, \n",
            "electrical engineers, mechanism designers, power \n",
            "engineers, human factors engineers, and many more \n",
            "disciplines are evaluated and balanced, one against \n",
            "another, to produce a coherent whole that is not dom-\n",
            "inated by the perspective of a single discipline.2\n",
            "\n",
            "ID: 115 || 3\n",
            "NASA SYSTEMS ENGINEERING HANDBOOK\n",
            "2.0\n",
            "Fundamentals of \n",
            "Systems Engineering\n",
            "A\n",
            "t NASA, “systems engineering” is defined as a \n",
            "methodical, multi-disciplinary approach for the \n",
            "design, realization, technical management, opera -\n",
            "tions, and retirement of a system. A “system” is the \n",
            "combination of elements that function together \n",
            "to produce the capability required to meet a need. \n",
            "The elements include all hardware, software, equip-\n",
            "ment, facilities, personnel, processes, and procedures \n",
            "needed for this purpose; that is, all things required to \n",
            "produce system-level results. The results include sys-\n",
            "tem-level qualities, properties, characteristics, func-\n",
            "tions, behavior, and performance. The value added by \n",
            "the system as a whole, beyond that contributed inde-\n",
            "pendently by the parts, is primarily created by the \n",
            "relationship among the parts; that is, how they are \n",
            "interconnected.1 It is a way of looking at the “big pic-\n",
            "ture” when making technical decisions. It is a way of\n",
            "\n",
            "ID: 1232 || 25 \n",
            " \n",
            " \n",
            " \n",
            "2. Develop required Integration Plans,  \n",
            "3. Develop integration test scripts,  \n",
            "4. Develop and implement integration test scenarios,  \n",
            "5. Conduct and document integration tests,  \n",
            "6. Track integration test results and retest status. \n",
            " \n",
            "Example Validation and Verification \n",
            "Validation and verification shall be by collection and analysis of the metrics defined by Systems \n",
            "Integration metrics in Appendix B – Metrics Guide.  \n",
            " \n",
            "2.2.11 Specialty Engineering \n",
            "In the domain of systems engineering, Specialty Engineering is defined as and includes the \n",
            "engineering disciplines that are not typical of the main engineering effort. More common \n",
            "engineering efforts in systems engineering such as hardware, software, and h uman factors \n",
            "engineering may be used as major elements in a majority of systems engineering efforts and \n",
            "therefore are not viewed as \"special\".  Examples of specialty engineering include\n",
            "\n",
            "ID: 442 || Any special instructions for operating data recording \n",
            "equipment or other automated test equipment as \n",
            "applicable;\n",
            "Copy of the as-run procedure (may include redlines); and\n",
            "Layouts, schematics, or diagrams showing identification, \n",
            "location, and interconnection of test equipment, test \n",
            "articles, and measuring points and any other associated \n",
            "design or configuration work products;\n",
            "Authentication of test results and authorization of \n",
            "acceptability.\n",
            "Identification of hazardous situations or operations;\n",
            "Precautions and safety instructions to ensure safety of \n",
            "personnel and prevent degradation of test articles and \n",
            "measuring equipment;\n",
            "Environmental and/or other conditions to be maintained \n",
            "with tolerances;\n",
            "Constraints on inspection or testing;\n",
            "Provision or instructions for the recording of verification \n",
            "results and other artifacts;\n",
            "Special instructions for instances of nonconformance and \n",
            "anomalous occurrences or results; and\n",
            "Specifications for facility, equipment maintenance,\n",
            "\n",
            "\n",
            "==================================================\n",
            "SEMANTİK ARAMA:\n",
            "==================================================\n",
            "ID: 115 || 3\n",
            "NASA SYSTEMS ENGINEERING HANDBOOK\n",
            "2.0\n",
            "Fundamentals of \n",
            "Systems Engineering\n",
            "A\n",
            "t NASA, “systems engineering” is defined as a \n",
            "methodical, multi-disciplinary approach for the \n",
            "design, realization, technical management, opera -\n",
            "tions, and retirement of a system. A “system” is the \n",
            "combination of elements that function together \n",
            "to produce the capability required to meet a need. \n",
            "The elements include all hardware, software, equip-\n",
            "ment, facilities, personnel, processes, and procedures \n",
            "needed for this purpose; that is, all things required to \n",
            "produce system-level results. The results include sys-\n",
            "tem-level qualities, properties, characteristics, func-\n",
            "tions, behavior, and performance. The value added by \n",
            "the system as a whole, beyond that contributed inde-\n",
            "pendently by the parts, is primarily created by the \n",
            "relationship among the parts; that is, how they are \n",
            "interconnected.1 It is a way of looking at the “big pic-\n",
            "ture” when making technical decisions. It is a way of\n",
            "\n",
            "ID: 115 || 3\n",
            "NASA SYSTEMS ENGINEERING HANDBOOK\n",
            "2.0\n",
            "Fundamentals of \n",
            "Systems Engineering\n",
            "A\n",
            "t NASA, “systems engineering” is defined as a \n",
            "methodical, multi-disciplinary approach for the \n",
            "design, realization, technical management, opera -\n",
            "tions, and retirement of a system. A “system” is the \n",
            "combination of elements that function together \n",
            "to produce the capability required to meet a need. \n",
            "The elements include all hardware, software, equip-\n",
            "ment, facilities, personnel, processes, and procedures \n",
            "needed for this purpose; that is, all things required to \n",
            "produce system-level results. The results include sys-\n",
            "tem-level qualities, properties, characteristics, func-\n",
            "tions, behavior, and performance. The value added by \n",
            "the system as a whole, beyond that contributed inde-\n",
            "pendently by the parts, is primarily created by the \n",
            "relationship among the parts; that is, how they are \n",
            "interconnected.1 It is a way of looking at the “big pic-\n",
            "ture” when making technical decisions. It is a way of\n",
            "\n",
            "ID: 115 || 3\n",
            "NASA SYSTEMS ENGINEERING HANDBOOK\n",
            "2.0\n",
            "Fundamentals of \n",
            "Systems Engineering\n",
            "A\n",
            "t NASA, “systems engineering” is defined as a \n",
            "methodical, multi-disciplinary approach for the \n",
            "design, realization, technical management, opera -\n",
            "tions, and retirement of a system. A “system” is the \n",
            "combination of elements that function together \n",
            "to produce the capability required to meet a need. \n",
            "The elements include all hardware, software, equip-\n",
            "ment, facilities, personnel, processes, and procedures \n",
            "needed for this purpose; that is, all things required to \n",
            "produce system-level results. The results include sys-\n",
            "tem-level qualities, properties, characteristics, func-\n",
            "tions, behavior, and performance. The value added by \n",
            "the system as a whole, beyond that contributed inde-\n",
            "pendently by the parts, is primarily created by the \n",
            "relationship among the parts; that is, how they are \n",
            "interconnected.1 It is a way of looking at the “big pic-\n",
            "ture” when making technical decisions. It is a way of\n",
            "\n",
            "ID: 115 || 3\n",
            "NASA SYSTEMS ENGINEERING HANDBOOK\n",
            "2.0\n",
            "Fundamentals of \n",
            "Systems Engineering\n",
            "A\n",
            "t NASA, “systems engineering” is defined as a \n",
            "methodical, multi-disciplinary approach for the \n",
            "design, realization, technical management, opera -\n",
            "tions, and retirement of a system. A “system” is the \n",
            "combination of elements that function together \n",
            "to produce the capability required to meet a need. \n",
            "The elements include all hardware, software, equip-\n",
            "ment, facilities, personnel, processes, and procedures \n",
            "needed for this purpose; that is, all things required to \n",
            "produce system-level results. The results include sys-\n",
            "tem-level qualities, properties, characteristics, func-\n",
            "tions, behavior, and performance. The value added by \n",
            "the system as a whole, beyond that contributed inde-\n",
            "pendently by the parts, is primarily created by the \n",
            "relationship among the parts; that is, how they are \n",
            "interconnected.1 It is a way of looking at the “big pic-\n",
            "ture” when making technical decisions. It is a way of\n",
            "\n",
            "ID: 116 || achieving stakeholder functional, physical, and oper-\n",
            "ational performance requirements in the intended \n",
            "use environment over the planned life of the system \n",
            "within cost, schedule, and other constraints. It is a \n",
            "methodology that supports the containment of the \n",
            "life cycle cost of a system. In other words, systems \n",
            "engineering is a logical way of thinking.\n",
            "1 Eberhardt Rechtin, Systems Architecting of Organizations: \n",
            "Why Eagles Can’t Swim.\n",
            "Systems engineering is the art and science of devel -\n",
            "oping an operable system capable of meeting require-\n",
            "ments within often opposed constraints. Systems \n",
            "engineering is a holistic, integrative discipline, \n",
            "wherein the contributions of structural engineers, \n",
            "electrical engineers, mechanism designers, power \n",
            "engineers, human factors engineers, and many more \n",
            "disciplines are evaluated and balanced, one against \n",
            "another, to produce a coherent whole that is not dom-\n",
            "inated by the perspective of a single discipline.2\n",
            "\n",
            "\n",
            "==================================================\n",
            "ASİSTAN YANITI:\n",
            "==================================================\n",
            "###Answer###\n",
            "Sistem mühendisliği, genellikle karmaşık sistemler geliştirmeden başlayıp, tasarımı, analizi, uygulamasını, test edilmesini ve bakımını içerir. Bu sürecin başlangıcından sonuna kadar olan yaşam döngüsü boyunca çeşitli roller atar. Örneğin, proje yöneticisi rolü, sistem analisti rolü, yazılım mühendisi rolü, donanım mühendisi rolü gibi birçok farklı rol bulunmaktadır. Bu roller, sistemin gereksinimlerini belirlemek, tasarlamayı yapmak, uygulanmasını sağlamak ve sürekli olarak izlemeyi sağlayabilmek için önemlidir. Ayrıca, bu roller arasında etkileşimler de vardır çünkü her bir rolün diğer rollerle işbirliği yapması gerekmektedir. Bu nedenle, sistem mühendisliğinde farklı disiplinlardan uzmanlar birlikte çalışarak bir bütünü oluşturmaya çalışmaktadır. Sonuç olarak, sistem mühendisliği, karmaşık sistemlerin başarılı şekilde tasarlanmasına ve yönetilmesine yardımcı olurken aynı zamanda maliyetleri azaltmaya da katkı sağlar. Bu sayede daha verimli ve güvenilir sistemler elde edilir. \n",
            "\n",
            "Bu bilgiler doğrultusunda, sistem mühendisliği rolünün önemli olduğunu vurgularız. Bu rolde çalışan uzmanlar, projenin başarısını büyük ölçüde etkilerler. Bu nedenle, doğru rol dağılımının yapılması ve farklı disiplinlardaki uzmanların etkili bir şekilde çalışabilmesi önemlidir. Bu sayede, sistemlerin istenilen performans seviyesine ulaşılmış olur ve sürdürülebilirlik sağlanabilir. Bu düşünceler göz önünde bulundurularak, sistem mühendisliği alanında ilerleyişte sağlanmış teknolojik yeniliklere ve gelişmelere göre hareket etmek önemlidir. Bu da, gelecekte daha karmaşık ve akıllı sistemlerin geliştirilmesine olanak tanıyacaktır. Son olarak, sistem mühendisliği sadece belirli disiplinlerin bir araya gelmesiyle değil, aynı zamanda farklı bakış açılarının birleşerek oluşturulan bir toplulukla da gerçekleşmektedir. Bu nedenle, farklı perspektiflerin birleştirilmesi sayesinde daha zengin ve katılımcı bir sistem mühendisliği pratiği mümkün hale gelir. Bu durum, daha iyi kararlar alınmasına ve daha başarılı sonuçlarla karşılaşılmamıza yardımcı olabilir. Bu sayede, sistem mühendisliği alanı daha geniş bir etki alanına sahip olabilir ve daha fazla insan tarafından benimsenebilir.\n",
            "Güncellenmiş sonuçlar kaydedildi: sistem_muhendisligi_cevaplar.xlsx\n",
            "2. Soru soruluyor: V-Model (V-Şeması) nedir ve hangi aşamaları içerir?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n"
          ]
        }
      ]
    }
  ]
}