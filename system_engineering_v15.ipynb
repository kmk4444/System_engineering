{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "mount_file_id": "https://github.com/kmk4444/System_engineering/blob/main/system_engineering_v15.ipynb",
      "authorship_tag": "ABX9TyNstHEX3LOGaattBvdigqzj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "69f3b3b2538240b186944046d8b42ba6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_75feb93937854a6dae69c8415931ad8c",
              "IPY_MODEL_b497942ec8db464dabd43f62892dba88",
              "IPY_MODEL_ab88731e09f14d62b53bf85d97d4fe5f"
            ],
            "layout": "IPY_MODEL_244fb4b1d60f4626b9971c6d77d65400"
          }
        },
        "75feb93937854a6dae69c8415931ad8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_611bd6afa90842e1be10ee847b6344a0",
            "placeholder": "​",
            "style": "IPY_MODEL_8213d54074c048ffa7fbc60b886bcbc1",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "b497942ec8db464dabd43f62892dba88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd6b987c378643f7ae28b0ef2be3a74c",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_db7fa8ecc15c48d486f737c23a14ec68",
            "value": 4
          }
        },
        "ab88731e09f14d62b53bf85d97d4fe5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc02aa0bcc9348109e174cff46e3665c",
            "placeholder": "​",
            "style": "IPY_MODEL_bb54a00a1b2f4480a995646f95a37ae1",
            "value": " 4/4 [00:04&lt;00:00,  1.00it/s]"
          }
        },
        "244fb4b1d60f4626b9971c6d77d65400": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "611bd6afa90842e1be10ee847b6344a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8213d54074c048ffa7fbc60b886bcbc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd6b987c378643f7ae28b0ef2be3a74c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db7fa8ecc15c48d486f737c23a14ec68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cc02aa0bcc9348109e174cff46e3665c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb54a00a1b2f4480a995646f95a37ae1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmk4444/System_engineering/blob/main/system_engineering_v15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain openai pypdf chroma streamlit langchain_openai langchain_community langchain transformers bitsandbytes accelerate torch"
      ],
      "metadata": {
        "id": "cp9iCXKFtO2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51b2fda6-6d76-4836-9dd5-08858eb718f6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.84.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.6.0)\n",
            "Requirement already satisfied: chroma in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.45.1)\n",
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.46.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.65)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.45)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.5)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.3.0)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.9.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.41.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.25.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U faiss-gpu --no-deps"
      ],
      "metadata": {
        "id": "AQH_UgdMgcMo",
        "outputId": "96a3bc24-43e1-4e33-afa1-d4283db29996",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss-gpu\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain_chroma langchain_experimental sentence-transformers cohere rank_bm25 nltk scikit-learn"
      ],
      "metadata": {
        "id": "KzAEXcnqge2K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc065800-22a2-45e2-dcbf-12cc31d266be"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_chroma in /usr/local/lib/python3.11/dist-packages (0.2.4)\n",
            "Requirement already satisfied: langchain_experimental in /usr/local/lib/python3.11/dist-packages (0.3.4)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: cohere in /usr/local/lib/python3.11/dist-packages (5.15.0)\n",
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.11/dist-packages (0.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: langchain-core>=0.3.60 in /usr/local/lib/python3.11/dist-packages (from langchain_chroma) (0.3.65)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from langchain_chroma) (2.3.0)\n",
            "Requirement already satisfied: chromadb>=1.0.9 in /usr/local/lib/python3.11/dist-packages (from langchain_chroma) (1.0.12)\n",
            "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain_experimental) (0.3.25)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.52.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /usr/local/lib/python3.11/dist-packages (from cohere) (1.11.1)\n",
            "Requirement already satisfied: httpx>=0.21.2 in /usr/local/lib/python3.11/dist-packages (from cohere) (0.28.1)\n",
            "Requirement already satisfied: httpx-sse==0.4.0 in /usr/local/lib/python3.11/dist-packages (from cohere) (0.4.0)\n",
            "Requirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.11/dist-packages (from cohere) (2.11.5)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.11/dist-packages (from cohere) (2.33.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.11/dist-packages (from cohere) (0.21.1)\n",
            "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere) (2.32.4.20250611)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (1.2.2.post1)\n",
            "Requirement already satisfied: fastapi==0.115.9 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (0.115.9)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain_chroma) (0.34.3)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (4.10.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (0.55b1)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (1.34.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (1.72.1)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (0.16.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (33.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (9.1.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (5.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (3.10.18)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=1.0.9->langchain_chroma) (4.24.0)\n",
            "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi==0.115.9->chromadb>=1.0.9->langchain_chroma) (0.45.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.2)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.25 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.3.25)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.0.41)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.11.15)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.9.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.3.45)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3.60->langchain_chroma) (1.33)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9.2->cohere) (0.7.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9.2->cohere) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (2.4.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.20.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb>=1.0.9->langchain_chroma) (1.2.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.3.60->langchain_chroma) (3.0.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain_chroma) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain_chroma) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.9->langchain_chroma) (0.25.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (3.2.2)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (0.10)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.3.8)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (0.23.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain_chroma) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain_chroma) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.9->langchain_chroma) (5.29.5)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb>=1.0.9->langchain_chroma) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain_chroma) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain_chroma) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-proto==1.34.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.9->langchain_chroma) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.55b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=1.0.9->langchain_chroma) (0.55b1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.55b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=1.0.9->langchain_chroma) (0.55b1)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=1.0.9->langchain_chroma) (0.55b1)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.55b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=1.0.9->langchain_chroma) (0.55b1)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.55b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=1.0.9->langchain_chroma) (1.17.2)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-asgi==0.55b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=1.0.9->langchain_chroma) (3.8.1)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb>=1.0.9->langchain_chroma) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb>=1.0.9->langchain_chroma) (1.9.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.1.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb>=1.0.9->langchain_chroma) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb>=1.0.9->langchain_chroma) (2.19.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (3.2.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb>=1.0.9->langchain_chroma) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain_chroma) (0.6.4)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain_chroma) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain_chroma) (1.0.5)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.9->langchain_chroma) (15.0.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=1.0.9->langchain_chroma) (3.22.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=1.0.9->langchain_chroma) (0.1.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain_experimental) (1.1.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb>=1.0.9->langchain_chroma) (10.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.9->langchain_chroma) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade huggingface-hub transformers"
      ],
      "metadata": {
        "id": "xTFQ81pdtOwO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de2865d4-cab8-4648-e41c-84eacec528c3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (0.33.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip uninstall -y numpy transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkiV6YAmXnzf",
        "outputId": "d958a8ce-ff52-46e0-85b2-401cd5845b74"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.3.0\n",
            "Uninstalling numpy-2.3.0:\n",
            "  Successfully uninstalled numpy-2.3.0\n",
            "Found existing installation: transformers 4.52.4\n",
            "Uninstalling transformers-4.52.4:\n",
            "  Successfully uninstalled transformers-4.52.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "id": "Gtx2oAaYYimx",
        "outputId": "81ed8abc-5cf6-4b78-cf96-5c4c7810ce66"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n",
            "  Using cached numpy-2.3.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Using cached numpy-2.3.0-cp311-cp311-manylinux_2_28_x86_64.whl (16.9 MB)\n",
            "Using cached transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
            "Installing collected packages: numpy, transformers\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.0 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.3.0 transformers-4.52.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "978fb5eb0cf742ffbe3c89e8fb5f80e2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import pipeline\n",
        "\n",
        "login(token = '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')"
      ],
      "metadata": {
        "id": "r-6dy_w0tOot"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hücre 1: Gerekli Kütüphaneler ve İndirme Fonksiyonları"
      ],
      "metadata": {
        "id": "2qMStg5E_T3F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IXoDJDdMtJcA",
        "outputId": "d3f78d47-b1f3-43f6-ad5e-8eac2d3c5448",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623,
          "referenced_widgets": [
            "69f3b3b2538240b186944046d8b42ba6",
            "75feb93937854a6dae69c8415931ad8c",
            "b497942ec8db464dabd43f62892dba88",
            "ab88731e09f14d62b53bf85d97d4fe5f",
            "244fb4b1d60f4626b9971c6d77d65400",
            "611bd6afa90842e1be10ee847b6344a0",
            "8213d54074c048ffa7fbc60b886bcbc1",
            "fd6b987c378643f7ae28b0ef2be3a74c",
            "db7fa8ecc15c48d486f737c23a14ec68",
            "cc02aa0bcc9348109e174cff46e3665c",
            "bb54a00a1b2f4480a995646f95a37ae1"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "69f3b3b2538240b186944046d8b42ba6"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Gerekli kütüphanelerin import edilmesi\n",
        "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema.document import Document\n",
        "from langchain.vectorstores.chroma import Chroma\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
        "import torch\n",
        "import os\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "#nltk.download('punkt')\n",
        "#from nltk.tokenize import sent_tokenize\n",
        "\n",
        "templates = {\n",
        "    \"system\": \"You are a professional prompt engineer. Apply the mentioned prompt engineering technique and provide ONLY the improved prompt without any additional commentary or explanations.\",\n",
        "    \"system_multiple\": \"You are a professional prompt engineer. Thoroughly apply EVERY prompt engineering technique listed in the [Prompt Engineering Techniques to Apply] section. Use these techniques to enhance the original prompt provided below, ensuring the enhancement is clear and effective. Provide ONLY the improved version of the prompt without any additional commentary or explanations.\",\n",
        "    \"lang_default\": \"Identify the language of the user's original prompt in the [original] section. You MUST provide the enhanced version of the prompt in the **same language** as the user's original prompt. You'll be penalized if you translate it into another language unless explicitly requested by the user.\",\n",
        "    \"lang_eng\": \"Original prompt is Turkish, first translate it into English before proceeding with the improvement process.It is very important!\",\n",
        "    \"deeper_understanding_simpler\": \"Explain to me as if I’m a beginner in System Engineering. Example: Change \\\"Explain system architecture.\\\" to \\\"Explain system architecture to beginners.\\\"\",\n",
        "    \"task_decomposition_simpler\": \"For complex or multi-step tasks, divide the original prompt into a series of simpler, more manageable sub-prompts. This approach allows the model to focus on one part of the task at a time, generating more detailed and coherent responses for each step.\",\n",
        "    \"fewshot_prompting_simpler\": \"\"\"Improve the original prompt by adding a couple of relevant examples that demonstrate the kind of answer or information being requested. Incorporate those examples smoothly into the prompt to make the desired response clear.\n",
        "\n",
        "Below are examples of how to enhance prompts using this technique, where the original prompt is in Turkish but is translated and improved in English to make it clearer:\n",
        "\n",
        "---\n",
        "\n",
        "Original Prompt (in Turkish): Sistem mühendisliği nedir?\n",
        "Improved Prompt: Explain what systems engineering is to someone who is new to the field. Break down your answer into the following parts:\n",
        "(1) A simple definition,\n",
        "(2) Key goals and responsibilities of a systems engineer,\n",
        "(3) A basic real-life example like building a smart transportation system.\n",
        "\n",
        "---\n",
        "\n",
        "Original Prompt (in Turkish): V-model nedir?\n",
        "Improved Prompt: Describe the V-model in systems engineering for beginners. Divide your explanation into:\n",
        "(1) A simple definition of what the V-model is,\n",
        "(2) How it supports both design and validation phases,\n",
        "(3) A real-world scenario such as developing an aircraft control system where the V-model is applied.\n",
        "\n",
        "---\n",
        "\n",
        "Original Prompt (in Turkish): Doğrulama ve geçerleme farkı nedir?\n",
        "Improved Prompt: Explain the difference between verification and validation in a way that is easy to understand for someone new to systems engineering. Structure your answer as:\n",
        "(1) Clear definitions of verification and validation,\n",
        "(2) The main difference between the two,\n",
        "(3) Simple examples like testing a software module vs. ensuring it meets user needs.\n",
        "\n",
        "---\n",
        "\n",
        "Original Prompt (in Turkish): Gereksinim analizi neden önemlidir?\n",
        "Improved Prompt: Explain why requirements analysis is important in the context of systems engineering, especially for beginners. Organize your answer as:\n",
        "(1) What requirements analysis means,\n",
        "(2) Why it's critical for system success,\n",
        "(3) A real example such as a software project failing due to misunderstood requirements.\n",
        "\"\"\"\n",
        "}\n",
        "\n",
        "\n",
        "# Sabitler\n",
        "DATA_PATH = \"/content/drive/MyDrive/data\"  # PDF dosyalarının bulunduğu dizin\n",
        "CHROMA_PATH = \"chroma\"  # Chroma veritabanının saklanacağı dizin\n",
        "\n",
        "\n",
        "from langchain.embeddings.base import Embeddings\n",
        "from typing import List\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class JinaEmbeddings(Embeddings):\n",
        "    def __init__(self, model_name=\"jinaai/jina-embeddings-v3\"):\n",
        "        from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def _get_embedding(self, text: str) -> List[float]:\n",
        "        # Metni tokenize et\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Embeddingi hesapla\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            embeddings = outputs.last_hidden_state.mean(dim=1).to(torch.float32)\n",
        "\n",
        "        # GPU'dan CPU'ya taşı ve numpy dizisine dönüştür\n",
        "        return embeddings.cpu().numpy()[0].tolist()\n",
        "\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Metinler listesi için embedding vektörleri üretir.\"\"\"\n",
        "        return [self._get_embedding(text) for text in texts]\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        \"\"\"Tek bir metin için embedding vektörü üretir.\"\"\"\n",
        "        return self._get_embedding(text)\n",
        "\n",
        "def initialize_embeddings():\n",
        "    \"\"\"Jina embeddings modelini başlatır.\"\"\"\n",
        "    return JinaEmbeddings(model_name=\"jinaai/jina-embeddings-v3\")\n",
        "\n",
        "embeddings = initialize_embeddings()\n",
        "\n",
        "def initialize_colbertv2_model():\n",
        "    \"\"\"\n",
        "    ColBERTv2 tokenizer ve modelini yükler.\n",
        "\n",
        "    Returns:\n",
        "        colbert_tokenizer: Tokenizer nesnesi\n",
        "        colbert_model: Model nesnesi\n",
        "    \"\"\"\n",
        "    # ColBERTv2 tokenizer ve modelini yükleme\n",
        "    colbert_tokenizer = AutoTokenizer.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "    colbert_model = AutoModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "\n",
        "    return colbert_tokenizer, colbert_model\n",
        "\n",
        "# Kullanım:\n",
        "colbert_tokenizer, colbert_model = initialize_colbertv2_model()\n",
        "\n",
        "# Modelin başlatılması\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "#AutoModelForCausalLM\n",
        "#AutoPeftModelForCausalLM\n",
        "\n",
        "def initialize_model():\n",
        "    # 4-bit quantization için gerekli konfigürasyon\n",
        "    load_in_4bit = False\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"kmk4444/Deepseek-r1-8B-Instruct_syseng_vllm_last\",\n",
        "        load_in_4bit=load_in_4bit,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"kmk4444/Deepseek-r1-8B-Instruct_syseng_vllm_last\")\n",
        "    return tokenizer, model\n",
        "\n",
        "# Modeli ve tokenizer'ı başlat\n",
        "tokenizer, model = initialize_model()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hücre 2: Yardımcı Fonksiyonlar"
      ],
      "metadata": {
        "id": "c9n0F7q6_dbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import torch\n",
        "from transformers import GenerationConfig\n",
        "\n",
        "# Uyarıları kapat\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "def is_prompt_about_engineering(prompt, tokenizer, model):\n",
        "    \"\"\"\n",
        "    Kullanıcının girdiği promptun mühendislikle ilgili olup olmadığını,\n",
        "    System Prompt, Chain-of-Thought ve Few-Shot örnekler kullanarak yüksek doğrulukla değerlendirir.\n",
        "    Bu versiyon, LLM çıktısındaki format farklılıklarına karşı çok daha dayanıklıdır.\n",
        "    \"\"\"\n",
        "    # Yasaklı kelimeler ve konular listesi\n",
        "    forbidden_topics = [\n",
        "        \"film\", \"dizi\", \"sinema\", \"oyuncu\", \"müzik\", \"konser\", \"şarkı\", \"oyun\",\n",
        "        \"ünlü\", \"magazin\", \"dedikodu\", \"influencer\", \"tiktok\", \"instagram\",\n",
        "        \"futbol\", \"basketbol\", \"voleybol\", \"maç\", \"gol\", \"iddaa\", \"bahis\",\n",
        "        \"burç\", \"astroloji\", \"fal\", \"tarot\", \"rüya tabiri\", \"enerji\", \"çakra\",\n",
        "        \"yemek tarifi\", \"nasıl pişirilir\", \"malzemeler\", \"fırın\"\n",
        "    ]\n",
        "\n",
        "    clean_prompt = prompt.lower().strip()\n",
        "\n",
        "    if len(clean_prompt) < 3:\n",
        "        return False\n",
        "\n",
        "    for topic in forbidden_topics:\n",
        "        if topic in clean_prompt:\n",
        "            return False\n",
        "\n",
        "    # System Prompt, Few-Shot ve Chain-of-Thought istemleri\n",
        "    system_prompt_content = \"\"\"Sen, bir metnin sistem mühendisliği, yazılım mühendisliği, proje yönetimi veya genel olarak teknoloji ile ilgili olup olmadığını sınıflandıran bir uzmansın. Görevin, sana verilen soruyu analiz etmek, bir 'Düşünce' süreci oluşturmak ve bu düşünceye dayanarak nihai kararını 'Sonuç:' olarak sadece 'EVET' veya 'HAYIR' şeklinde belirtmektir.\"\"\"\n",
        "\n",
        "    user_prompt_content = f\"\"\"\n",
        "Aşağıdaki örnekleri incele ve en sondaki yeni soruyu aynı mantık ve formatla cevapla.\n",
        "\n",
        "### Örnek 1:\n",
        "**Soru:** \"Konfigürasyon yönetimi ve baseline arasındaki ilişki nedir?\"\n",
        "**Düşünce:** Bu soru, 'konfigürasyon yönetimi' ve 'baseline' gibi net sistem mühendisliği terimleri içeriyor. Bu kesinlikle mühendislik alanına girer.\n",
        "**Sonuç:** EVET\n",
        "\n",
        "### Örnek 2:\n",
        "**Soru:** \"Bu akşamki maç kaç kaç biter?\"\n",
        "**Düşünce:** Bu soru futbol ve spor ile ilgili. Teknik veya mühendislik ile tamamen alakasız.\n",
        "**Sonuç:** HAYIR\n",
        "\n",
        "### Örnek 3:\n",
        "**Soru:** \"Yemek tarifi uygulamasının sistem gereksinimleri nelerdir?\"\n",
        "**Düşünce:** Soru 'yemek tarifi' kelimesini içerse de, asıl odak noktası 'uygulamanın sistem gereksinimleri'dir. 'Sistem gereksinimleri' temel bir yazılım mühendisliği konusudur.\n",
        "**Sonuç:** EVET\n",
        "\n",
        "### Yeni Soru (Cevaplanacak):\n",
        "**Soru:** \"{prompt}\"\n",
        "**Düşünce:**\"\"\"\n",
        "\n",
        "    try:\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt_content},\n",
        "            {\"role\": \"user\", \"content\": user_prompt_content}\n",
        "        ]\n",
        "\n",
        "        generation_config = GenerationConfig(\n",
        "            max_new_tokens=80,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        encoded_input = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(model.device)\n",
        "\n",
        "        outputs = model.generate(\n",
        "            input_ids=encoded_input,\n",
        "            generation_config=generation_config\n",
        "        )\n",
        "\n",
        "        input_length = encoded_input.shape[1]\n",
        "        # Modelin ürettiği çıktıyı alıyoruz (Düşünce'den sonraki kısım)\n",
        "        llm_output = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True).strip()\n",
        "\n",
        "        print(f\"[DEBUG] Konu uygunluk LLM yanıtı:\\n---\\nDüşünce: {llm_output}\\n---\")\n",
        "\n",
        "        # ### EN KARARLI KONTROL ###\n",
        "        # Artık \"Sonuç:\" etiketine veya formata bağımlı değiliz.\n",
        "        # Modelin ürettiği çıktının herhangi bir yerinde \"EVET\" geçmesi yeterli.\n",
        "        # Bu, en sağlam ve hataya en dayanıklı yöntemdir.\n",
        "        is_engineering = \"EVET\" in llm_output.upper()\n",
        "\n",
        "        # Fonksiyon doğrudan True veya False döndürüyor.\n",
        "        print(f\"[is_engineering {is_engineering}\\n---\")\n",
        "        return is_engineering\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[DEBUG] LLM kontrol hatası: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "# Rerank fonksiyonu\n",
        "def rerank_with_colbertv2(query, documents, colbert_tokenizer, colbert_model):\n",
        "    # Query'yi tokenlara ayırma\n",
        "    query_tokens = colbert_tokenizer(query, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "    # Query için embedding hesaplama\n",
        "    with torch.no_grad():\n",
        "        query_embedding = colbert_model(**query_tokens).last_hidden_state.mean(dim=1)\n",
        "\n",
        "    # Her bir belge ile query arasındaki benzerlik skorlarını hesaplama\n",
        "    scores = []\n",
        "    for doc in documents:\n",
        "        doc_tokens = colbert_tokenizer(doc.page_content, return_tensors='pt', padding=True, truncation=True)\n",
        "        doc_embedding = colbert_model(**doc_tokens).last_hidden_state.mean(dim=1)\n",
        "\n",
        "        # Query ve belge embedding'leri arasındaki cosine benzerlik skorunu hesaplama\n",
        "        score = torch.nn.functional.cosine_similarity(query_embedding, doc_embedding)\n",
        "        scores.append((doc, score.item()))\n",
        "\n",
        "    # Belgeleri benzerlik skoruna göre sıralama (azalan sırada)\n",
        "    sorted_documents = sorted(scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return [doc for doc, score in sorted_documents]\n",
        "\n",
        "# Belgeleri yükleme ve bölme\n",
        "def load_and_split_documents(data_path):\n",
        "    document_loader = PyPDFDirectoryLoader(data_path)\n",
        "    raw_documents = document_loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=50,\n",
        "        length_function=len\n",
        "    )\n",
        "    return text_splitter.split_documents(raw_documents)\n",
        "\n",
        "# Özel belgelerin oluşturulması\n",
        "def create_custom_documents(splitted_documents):\n",
        "    custom_documents = []\n",
        "    for i, raw_doc in enumerate(splitted_documents):\n",
        "        new_doc = Document(\n",
        "                page_content=raw_doc.page_content,\n",
        "                metadata={\n",
        "                    \"source\": raw_doc.metadata.get(\"source\", \"Unknown Source\"),\n",
        "                    \"title\": raw_doc.metadata.get(\"title\", \"No Title\"),\n",
        "                    \"description\": raw_doc.metadata.get(\"description\", \"No Description\"),\n",
        "                    \"language\": raw_doc.metadata.get(\"language\", \"Unknown Language\"),\n",
        "                    \"doc_id\": i\n",
        "                }\n",
        "        )\n",
        "        custom_documents.append(new_doc)\n",
        "    return custom_documents\n",
        "\n",
        "# Vectorstore ve Retriever'ın başlatılması\n",
        "def initialize_vectorstore(custom_documents, embeddings_model, persist_directory):\n",
        "    try:\n",
        "        vectorstore = Chroma.from_documents(\n",
        "            documents=custom_documents,\n",
        "            embedding=embeddings_model,  # embeddings_model'i doğrudan kullan\n",
        "            persist_directory=persist_directory\n",
        "        )\n",
        "\n",
        "        return vectorstore.as_retriever(\n",
        "            search_type=\"mmr\",\n",
        "            search_kwargs={'k': 5, 'lambda_mult': 0.40}\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Vectorstore başlatılırken hata oluştu: {str(e)}\")\n",
        "        raise e\n",
        "\n",
        "# İlgili belgelerin getirilmesi\n",
        "def retrieve_relevant_documents(retriever, prompt):\n",
        "    return retriever.get_relevant_documents(prompt)\n",
        "\n",
        "def get_relevant_documents_with_bm25(documents, query):\n",
        "    bm25_retriever = BM25Retriever.from_documents(documents=documents)\n",
        "    bm25_retriever.k = 5\n",
        "\n",
        "    bm25_relevant_documents = bm25_retriever.get_relevant_documents(query=query)\n",
        "\n",
        "    return bm25_relevant_documents, bm25_retriever\n",
        "\n",
        "def get_relevant_documents_for_hybrid_search(query, retriever1, retriever2, weight1, weight2):\n",
        "    ensemble_retriever = EnsembleRetriever(\n",
        "                                retrievers=[retriever1, retriever2],\n",
        "                                weights=[weight1, weight2])\n",
        "\n",
        "    hybrid_relevant_documents = ensemble_retriever.get_relevant_documents(query)\n",
        "\n",
        "    return hybrid_relevant_documents\n",
        "\n",
        "# Final promptun oluşturulması\n",
        "def generate_final_prompt(prompt, context_data, relevant_documents):\n",
        "    #history_prompt = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in chat_history])\n",
        "    metadata_info = \"\\n\".join([f\"Belge {doc.metadata['doc_id']} - Başlık: {doc.metadata['title']}, Kaynak: {doc.metadata['source']}\" for doc in relevant_documents])\n",
        "    return  f\"\"\"\n",
        "###Instruction###\n",
        "You are an expert assistant dedicated to providing answers for beginner systems engineers. Follow these guidelines:\n",
        "\n",
        "1. Deliver clear, concise, and expert-level information in Turkish.\n",
        "2. Use the provided documents (###Context###) and historical conversation (###Previous Conversations###) data to answer questions (###Question###). Reference these documents while interpreting and generating your answers.\n",
        "3. Ensure that your answers are coherent and free from any repetitive or duplicate content.\n",
        "4. Each sentence should provide unique and valuable information.\n",
        "5. You have to answer just in TURKISH.\n",
        "\n",
        "\n",
        "###Question###\n",
        "{prompt}\n",
        "\n",
        "###Context###\n",
        "This is the information we have to answer the question: {context_data}\n",
        "\"\"\", metadata_info\n",
        "\n",
        "# Tekrarları kaldırmak için fonksiyon\n",
        "def remove_repetitions(response, threshold=0.7):\n",
        "    # Cümlelere bölme\n",
        "    sentences = sent_tokenize(response)\n",
        "\n",
        "    # TF-IDF vektörizer oluşturma\n",
        "    vectorizer = TfidfVectorizer().fit_transform(sentences)\n",
        "    vectors = vectorizer.toarray()\n",
        "\n",
        "    # Cosine similarity hesaplama\n",
        "    cosine_matrix = cosine_similarity(vectors)\n",
        "\n",
        "    # Tekrar eden cümleleri tespit etme ve kaldırma\n",
        "    unique_sentences = []\n",
        "    seen_indices = set()\n",
        "\n",
        "    for i in range(len(sentences)):\n",
        "        if i in seen_indices:\n",
        "            continue\n",
        "        unique_sentences.append(sentences[i])\n",
        "        for j in range(i + 1, len(sentences)):\n",
        "            if cosine_matrix[i, j] > threshold:\n",
        "                seen_indices.add(j)\n",
        "\n",
        "    # Tekrarları kaldırılmış yanıtı yeniden oluşturma\n",
        "    cleaned_response = ' '.join(unique_sentences)\n",
        "\n",
        "    return cleaned_response\n",
        "\n",
        "# RAG ile çoklu PDF'ler\n",
        "def rag_with_multiple_pdfs(prompt):\n",
        "    embedding_model = initialize_embeddings()  # Her seferinde yeni bir embeddings nesnesi oluştur\n",
        "    splitted_documents = load_and_split_documents(DATA_PATH)\n",
        "    custom_documents = create_custom_documents(splitted_documents)\n",
        "\n",
        "    chroma_retriever = initialize_vectorstore(custom_documents, embedding_model, CHROMA_PATH)\n",
        "    chroma_relevant_documents = retrieve_relevant_documents(chroma_retriever, prompt)\n",
        "\n",
        "    bm25_documents, bm25retriever = get_relevant_documents_with_bm25(custom_documents,prompt)\n",
        "\n",
        "    weight1 = 0.2\n",
        "    hybrid_search_documents = get_relevant_documents_for_hybrid_search(\n",
        "        query=prompt,\n",
        "        retriever1=bm25retriever,\n",
        "        retriever2=chroma_retriever,\n",
        "        weight1=weight1,\n",
        "        weight2=1-weight1\n",
        "    )\n",
        "\n",
        "    # Hibrit arama sonuçlarını ColBERTv2 ile yeniden sıralama\n",
        "    reranked_documents = rerank_with_colbertv2(prompt, hybrid_search_documents, colbert_tokenizer, colbert_model)\n",
        "\n",
        "    context_data = \" \".join([doc.page_content for doc in reranked_documents])\n",
        "    #cleaned_context_data = remove_repetitions(context_data)\n",
        "    final_prompt, metadata_info = generate_final_prompt(prompt, context_data, reranked_documents)\n",
        "    return final_prompt, metadata_info, chroma_relevant_documents, bm25_documents, reranked_documents\n",
        "\n",
        "def generate_prompt_engineering(prompt, tokenizer, model):\n",
        "    system_message = templates[\"lang_eng\"]\n",
        "    system_message += '\\n' + templates[\"system_multiple\"]\n",
        "\n",
        "    skills = [\"deeper_understanding\", \"task_decomposition\", \"fewshot_prompting\"]\n",
        "    integrated_templates = \"[Prompt Engineering Techniques to Apply]\\n\"\n",
        "\n",
        "    for idx, skill in enumerate(skills):\n",
        "        template = templates[f\"{skill}_simpler\"]\n",
        "        integrated_templates += f\"{idx+1}. {skill}: {template}\\n\"\n",
        "    integrated_templates += \"Based on [Prompt engineering techniques to apply], refine the prompt provided below. Ensure that each technique is fully incorporated to achieve a clear and effective improvement:\\n\\n[original]\\n{prompt}\\n[improved]\\n\"\n",
        "\n",
        "    prompt_template = PromptTemplate.from_template(integrated_templates)\n",
        "    formatted_input = prompt_template.format(prompt=prompt)\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": formatted_input}\n",
        "    ]\n",
        "\n",
        "    # input_ids ve attention_mask alınarak giriş hazırlanır\n",
        "    encoded_input = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # GPU'ya taşıma işlemi\n",
        "    encoded_input = {k: v.to(model.device) for k, v in encoded_input.items()} \\\n",
        "        if isinstance(encoded_input, dict) else encoded_input.to(model.device)\n",
        "\n",
        "    # Model çıktısı üretilir\n",
        "    outputs = model.generate(\n",
        "        input_ids=encoded_input if not isinstance(encoded_input, dict) else encoded_input['input_ids'],\n",
        "        max_new_tokens=1024,\n",
        "        do_sample=True,\n",
        "        temperature=0.75,\n",
        "        top_p=0.90,\n",
        "        repetition_penalty=1.2,\n",
        "        pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Yanıtı decode etme\n",
        "    if isinstance(encoded_input, dict):\n",
        "        input_length = encoded_input['input_ids'].shape[1]\n",
        "    else:\n",
        "        input_length = encoded_input.shape[1]\n",
        "\n",
        "    response = outputs[0][input_length:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "# LLaMA3 yanıtının üretilmesi\n",
        "def generate_llama3_response(prompt_input, tokenizer, model):\n",
        "    SYS_PROMPT = \"\"\"You are an expert assistant dedicated to guiding and empowering beginner systems engineers. Follow these guidelines:\n",
        "1. Provide clear, concise, and expert-level information in Turkish.\n",
        "2. Use the provided documents and historical conversation data to deliver insightful answers. Previous conversation history and document information will be given to you in the user's prompt.\n",
        "3. Ensure that your answers are free from any repetitive or duplicate content and sentences. You have to eliminate repetitive sentences. DON'T REPEAT THE SAME SENTENCES!\n",
        "4. If unsure, confidently state \"Bilmiyorum\" without speculating.\n",
        "5. Your responses should be motivational and tailored to empower beginners in understanding systems engineering principles.\n",
        "6. Only respond in TURKISH.\n",
        "\n",
        "# Few-Shot Examples:\n",
        "User: What is requirements analysis in systems engineering?\n",
        "Assistant: Gereksinim analizi, sistemin ne yapması gerektiğini belirlemek için paydaş ihtiyaçlarının toplanıp teknik terimlere dönüştürülmesidir.\n",
        "\n",
        "User: Is systems engineering a linear or iterative process?\n",
        "Assistant: Sistem mühendisliği iteratif bir süreçtir; her adımda gözden geçirme ve iyileştirme yapılır.\n",
        "\n",
        "User: Why is configuration management important?\n",
        "Assistant: Konfigürasyon yönetimi, sistem bileşenlerinin düzenli ve izlenebilir olmasını sağlar; karmaşayı önler.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": prompt_input}\n",
        "    ]\n",
        "\n",
        "    # input_ids ve attention_mask alınarak giriş hazırlanır\n",
        "    encoded_input = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # GPU'ya taşıma işlemi\n",
        "    encoded_input = {k: v.to(model.device) for k, v in encoded_input.items()} \\\n",
        "        if isinstance(encoded_input, dict) else encoded_input.to(model.device)\n",
        "\n",
        "    # Model çıktısı üretilir\n",
        "    outputs = model.generate(\n",
        "        input_ids=encoded_input if not isinstance(encoded_input, dict) else encoded_input['input_ids'],\n",
        "        min_new_tokens=512,\n",
        "        max_new_tokens=4096,\n",
        "        do_sample=True,\n",
        "        temperature=0.50,\n",
        "        top_p=0.90,\n",
        "        repetition_penalty=1.2,\n",
        "        pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Yanıtı decode etme\n",
        "    if isinstance(encoded_input, dict):\n",
        "        input_length = encoded_input['input_ids'].shape[1]\n",
        "    else:\n",
        "        input_length = encoded_input.shape[1]\n",
        "\n",
        "    response = outputs[0][input_length:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "def evaluate_answer_relevance(question, answer, tokenizer, model):\n",
        "    \"\"\"\n",
        "    Dil modelini kullanarak cevabın soruya uygun olup olmadığını değerlendirir.\n",
        "    \"\"\"\n",
        "    evaluation_prompt = f\"Soru: {question}\\nCevap: {answer}\\nBu cevap soruyu doğru ve tam olarak yanıtlıyor mu? Lütfen 'Evet' veya 'Hayır' şeklinde cevap ver.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": evaluation_prompt}\n",
        "    ]\n",
        "\n",
        "    # input_ids ve attention_mask alınarak giriş hazırlanır\n",
        "    encoded_input = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # GPU'ya taşıma işlemi\n",
        "    encoded_input = {k: v.to(model.device) for k, v in encoded_input.items()} \\\n",
        "        if isinstance(encoded_input, dict) else encoded_input.to(model.device)\n",
        "\n",
        "    # Model çıktısı üretilir\n",
        "    outputs = model.generate(\n",
        "        input_ids=encoded_input if not isinstance(encoded_input, dict) else encoded_input['input_ids'],\n",
        "        max_new_tokens=50,\n",
        "        do_sample=False,\n",
        "        temperature=0.0,\n",
        "        top_p=0.0,\n",
        "        repetition_penalty=1.0,\n",
        "        pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Yanıtı decode etme\n",
        "    if isinstance(encoded_input, dict):\n",
        "        input_length = encoded_input['input_ids'].shape[1]\n",
        "    else:\n",
        "        input_length = encoded_input.shape[1]\n",
        "\n",
        "    evaluation_response = outputs[0][input_length:]\n",
        "    evaluation_text = tokenizer.decode(evaluation_response, skip_special_tokens=True).strip()\n",
        "\n",
        "    # Cevabı değerlendirme\n",
        "    if \"Evet\" in evaluation_text:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "def select_best_response(question, responses, tokenizer, model):\n",
        "    \"\"\"\n",
        "    Dil modelini kullanarak verilen cevaplar arasından en iyi olanı seçer.\n",
        "\n",
        "    Args:\n",
        "        question (str): Kullanıcının sorusu.\n",
        "        responses (list): Cevapların listesi.\n",
        "        tokenizer: Tokenizer nesnesi.\n",
        "        model: Dil modeli nesnesi.\n",
        "\n",
        "    Returns:\n",
        "        str: En iyi cevap.\n",
        "    \"\"\"\n",
        "    # Cevapları numaralandır\n",
        "    numbered_responses = \"\\n\".join([f\"{idx+1}. {resp}\" for idx, resp in enumerate(responses)])\n",
        "\n",
        "    selection_prompt = f\"Kullanıcının sorusu: {question}\\n\\nVerilen cevaplar:\\n{numbered_responses}\\n\\nBu cevaplar arasından soruya en iyi yanıt veren hangisidir? Lütfen sadece cevabın numarasını yaz.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": selection_prompt}\n",
        "    ]\n",
        "\n",
        "    # input_ids ve attention_mask alınarak giriş hazırlanır\n",
        "    encoded_input = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # GPU'ya taşıma işlemi\n",
        "    encoded_input = {k: v.to(model.device) for k, v in encoded_input.items()} \\\n",
        "        if isinstance(encoded_input, dict) else encoded_input.to(model.device)\n",
        "\n",
        "    # Model çıktısı üretilir\n",
        "    outputs = model.generate(\n",
        "        input_ids=encoded_input if not isinstance(encoded_input, dict) else encoded_input['input_ids'],\n",
        "        max_new_tokens=10,\n",
        "        do_sample=False,\n",
        "        temperature=0.0,\n",
        "        top_p=0.0,\n",
        "        repetition_penalty=1.0,\n",
        "        pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Yanıtı decode etme\n",
        "    if isinstance(encoded_input, dict):\n",
        "        input_length = encoded_input['input_ids'].shape[1]\n",
        "    else:\n",
        "        input_length = encoded_input.shape[1]\n",
        "\n",
        "    selection_response = outputs[0][input_length:]\n",
        "    selection_text = tokenizer.decode(selection_response, skip_special_tokens=True).strip()\n",
        "\n",
        "    # Seçilen numarayı al\n",
        "    try:\n",
        "        selected_index = int(selection_text.split('.')[0].strip()) - 1\n",
        "        if 0 <= selected_index < len(responses):\n",
        "            return responses[selected_index]\n",
        "        else:\n",
        "            # Geçersiz seçim durumunda ilk cevabı döndür\n",
        "            return responses[0]\n",
        "    except ValueError:\n",
        "        # Anlaşılamayan yanıt durumunda ilk cevabı döndür\n",
        "        return responses[0]\n"
      ],
      "metadata": {
        "id": "uC9NIW8q_ZDh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hücre 3: Ana Kod Çalıştırma"
      ],
      "metadata": {
        "id": "enICwb7K_jXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.testgorilla.com/blog/system-engineer-interview-questions/\n",
        "\n",
        "bazen iyileştirilmiş promptta ingilizce yapmıyor, bunu çözeceğim.\n",
        "\n",
        "finetuning dosyasını değiştereceğim.\n",
        "\n",
        "ayrıca referans dosyaları göstereceğim, print yaparken.\n",
        "\n",
        "bide kullanıcı 0 girene kadar cevap verme olsun. ayrıca print kısmında paragraflar opsiyonel olsun.\n",
        "\n",
        "forbidden_topics konuları geliştirilecektir."
      ],
      "metadata": {
        "id": "-sIFadnOx47H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Modeli ve tokenizer'ı başlat\n",
        "    # tokenizer, model = initialize_model()\n",
        "\n",
        "    # Sohbet geçmişini başlat\n",
        "    chat_history = []\n",
        "\n",
        "#    prompt = input(\"Mesajınızı Giriniz: \")\n",
        "\n",
        "    while True:\n",
        "        # Kullanıcı girdisini al\n",
        "        prompt = input(\"Mesajınızı Giriniz: \")\n",
        "\n",
        "        # Kullanıcının girdiği promptu kontrol et\n",
        "        if not is_prompt_about_engineering(prompt, tokenizer, model):\n",
        "            print(\"Girdiğiniz soru sistem mühendisliği alanı ile ilgili değildir. Lütfen sistem mühendisliği ile ilgili bir soru sorunuz.\\n\")\n",
        "            continue  # Kullanıcıdan tekrar girdi al\n",
        "        else:\n",
        "            break  # Döngüden çık ve devam et\n",
        "\n",
        "    # Kullanıcı mesajını sohbet geçmişine ekle\n",
        "    #chat_history.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    # Optimize edilmiş promptu oluştur\n",
        "    optimized_prompt = generate_prompt_engineering(prompt, tokenizer, model)\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"OPTİMİZE EDİLMİŞ PROMPT:\")\n",
        "    print(\"=\"*50)\n",
        "    print(optimized_prompt)\n",
        "\n",
        "    # Final promptu oluştur ve belgeleri getir\n",
        "    final_prompt, metadata_info, chroma_relevant_documents, bm25_documents, hybrid_search_documents = rag_with_multiple_pdfs(optimized_prompt)\n",
        "\n",
        "    # BM25 belgelerini yazdır\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"KARAKTER BAZLI ARAMA | BM25:\")\n",
        "    print(\"=\"*50)\n",
        "    for doc in bm25_documents:\n",
        "        print(f\"ID: {doc.metadata['doc_id']} || {doc.page_content}\\n\")\n",
        "\n",
        "    # Hibrit Arama belgelerini yazdır\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"HİBRİT ARAMA:\")\n",
        "    print(\"=\"*50)\n",
        "    for doc in hybrid_search_documents:\n",
        "        print(f\"ID: {doc.metadata['doc_id']} || {doc.page_content}\\n\")\n",
        "\n",
        "    # Semantik Arama belgelerini yazdır (Chroma'dan)\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"SEMANTİK ARAMA:\")\n",
        "    print(\"=\"*50)\n",
        "    for doc in chroma_relevant_documents:\n",
        "        print(f\"ID: {doc.metadata['doc_id']} || {doc.page_content}\\n\")\n",
        "\n",
        "    max_attempts = 1  # Maksimum deneme sayısı\n",
        "    attempts = 0\n",
        "    is_relevant = False\n",
        "    previous_responses = []  # Önceki cevapları tutmak için liste\n",
        "\n",
        "    while attempts < max_attempts and not is_relevant:\n",
        "        # Yanıtı üret\n",
        "        if attempts == 0:\n",
        "            response = generate_llama3_response(final_prompt, tokenizer, model)\n",
        "        else:\n",
        "            # Geri bildirimle yeni prompt oluştur\n",
        "            feedback = f\"Cevabın kullanıcının sorusuna tam olarak yanıt vermedi. Önceki cevabın: '{previous_responses[-1]}'. Lütfen kullanıcının sorusuna odaklanarak daha uygun bir cevap ver.\"\n",
        "            final_prompt_with_feedback = f\"{feedback}\\n\\n{final_prompt}\"\n",
        "            response = generate_llama3_response(final_prompt_with_feedback, tokenizer, model)\n",
        "\n",
        "        # Cevabın uygunluğunu kontrol et\n",
        "        is_relevant = evaluate_answer_relevance(prompt, response, tokenizer, model)\n",
        "\n",
        "        # Önceki cevapları listeye ekle\n",
        "        previous_responses.append(response)\n",
        "        attempts += 1\n",
        "\n",
        "    if not is_relevant:\n",
        "        # Üç cevap arasından en iyisini seç\n",
        "        best_response = select_best_response(prompt, previous_responses, tokenizer, model)\n",
        "        response = best_response\n",
        "\n",
        "    # Uygun cevap sohbet geçmişine eklenir\n",
        "    chat_history.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ASİSTAN YANITI:\")\n",
        "    print(\"=\"*50)\n",
        "    print(response)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "ACv14X5D_hgl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e828bb4-2405-4823-c230-d2f47aa22cd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mesajınızı Giriniz: yemek tarifi verir misin\n",
            "Girdiğiniz soru sistem mühendisliği alanı ile ilgili değildir. Lütfen sistem mühendisliği ile ilgili bir soru sorunuz.\n",
            "\n",
            "Mesajınızı Giriniz: recep tayyip bu sene kazanır mı seçimi\n",
            "[DEBUG] Konu uygunluk LLM yanıtı:\n",
            "---\n",
            "Düşünce: **Sonuç:** HAYIR\n",
            "---\n",
            "[is_engineering False\n",
            "---\n",
            "Girdiğiniz soru sistem mühendisliği alanı ile ilgili değildir. Lütfen sistem mühendisliği ile ilgili bir soru sorunuz.\n",
            "\n",
            "Mesajınızı Giriniz: operasyonel senaryo nedir\n",
            "[DEBUG] Konu uygunluk LLM yanıtı:\n",
            "---\n",
            "Düşünce: **Sonuç:** EVET\n",
            "---\n",
            "[is_engineering True\n",
            "---\n",
            "\n",
            "==================================================\n",
            "OPTİMİZE EDİLMİŞ PROMPT:\n",
            "==================================================\n",
            "### Operasyonel Senaryolar Neden Önemlidir?\n",
            "\n",
            "Operasyonel senaryolar, işletmelerin günlük operasyonlarını simüle ederek olası durumları analiz etmesini sağlar. Bu sayede işletme, riskleri önceden belirleyebilir, kriz durumlarında hızlı bir şekilde tepki verebilir ve verimli bir karar verme sürecini destekler. Ayrıca çalışanların eğitiminde de önemli bir rol oynar.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n",
            "WARNING:transformers_modules.jinaai.xlm-roberta-flash-implementation.2b6bc3f30750b3a9648fe9b63448c09920efe9be.modeling_xlm_roberta:flash_attn is not installed. Using PyTorch native attention implementation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_29rKdYptQqo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}