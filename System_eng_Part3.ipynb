{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMOeqaroVcuqVQYOUPCTT2g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmk4444/System_engineering/blob/main/System_eng_Part3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain openai pypdf chroma streamlit langchain_openai langchain_community langchain transformers bitsandbytes accelerate torch faiss-gpu faiss-cpu langchain_chroma langchain_experimental"
      ],
      "metadata": {
        "id": "pRhoGD70xtE4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adf0c916-32cd-45a8-e68d-10ec9b1105c4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain_chroma) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.22.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb<0.6.0,>=0.4.0->langchain_chroma) (12.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (4.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=7.1,>=6.0->opentelemetry-api>=1.2.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (3.19.2)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb<0.6.0,>=0.4.0->langchain_chroma) (10.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb<0.6.0,>=0.4.0->langchain_chroma) (0.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import pipeline\n",
        "\n",
        "login(token = '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')"
      ],
      "metadata": {
        "id": "9khH96Vv2qLW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "590c7067-944b-4e60-ec9e-ffbe02bafa72"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app_v3.py\n",
        "import streamlit as st\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import FAISS # it is vector database. we will use FAISS database. It is created by Facebook.\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import CohereEmbeddings\n",
        "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import pipeline\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.documents import Document\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# Set up Streamlit page configuration\n",
        "st.set_page_config(page_title=\"ðŸ¦™ðŸ’¬ Sistem MÃ¼hendisliÄŸi GiriÅŸ Chatbot\")\n",
        "\n",
        "# Initialize Embeddings\n",
        "def initialize_embeddings():\n",
        "    return HuggingFaceInferenceAPIEmbeddings(\n",
        "        api_key='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',\n",
        "        model_name=\"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "    )\n",
        "\n",
        "embeddings = initialize_embeddings()\n",
        "\n",
        "# Load and Split Documents\n",
        "def load_and_split_documents(url, embeddings):\n",
        "    loader = WebBaseLoader(url)\n",
        "    raw_documents = loader.load()\n",
        "    text_splitter = SemanticChunker(embeddings, breakpoint_threshold_type=\"percentile\")\n",
        "    return text_splitter.split_documents(raw_documents)\n",
        "\n",
        "# Create Custom Documents\n",
        "def create_custom_documents(splitted_documents):\n",
        "    custom_documents = []\n",
        "    for i, raw_doc in enumerate(splitted_documents):\n",
        "        new_doc = Document(\n",
        "                page_content=raw_doc.page_content,\n",
        "                metadata = {\n",
        "                    \"source\": raw_doc.metadata[\"source\"],\n",
        "                    \"title\" : raw_doc.metadata[\"title\"],\n",
        "                    \"description\" : raw_doc.metadata[\"description\"],\n",
        "                    \"language\" : raw_doc.metadata[\"language\"],\n",
        "                    \"doc_id\" : i\n",
        "                }\n",
        "        )\n",
        "        custom_documents.append(new_doc)\n",
        "    return custom_documents\n",
        "\n",
        "# Initialize Vectorstore and Retriever\n",
        "def initialize_vectorstore(custom_documents, embeddings):\n",
        "    vectorstore = Chroma.from_documents(custom_documents, embeddings)\n",
        "    return vectorstore.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs={'k': 4, 'lambda_mult': 0.25}\n",
        "    )\n",
        "\n",
        "# Retrieve Relevant Documents\n",
        "def retrieve_relevant_documents(retriever, prompt):\n",
        "    return retriever.get_relevant_documents(prompt)\n",
        "\n",
        "# Generate Final Prompt\n",
        "def generate_final_prompt(prompt, context_data, chat_history):\n",
        "    history_prompt = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in chat_history])\n",
        "    return f\"\"\"\n",
        "    Ã–nceki konuÅŸmalarÄ±mÄ±z:\n",
        "    {history_prompt}\n",
        "\n",
        "    ÅžÃ¶yle bir sorum var: {prompt}\n",
        "    Bu soruyu yanÄ±tlamak iÃ§in elimizde ÅŸu bilgiler var: {context_data} .\n",
        "    Bu sorunun yanÄ±tÄ±nÄ± vermek iÃ§in yalnÄ±zca sana burada verdiÄŸim eldeki bilgileri kullan. BunlarÄ±n dÄ±ÅŸÄ±na asla Ã§Ä±kma.\n",
        "    \"\"\"\n",
        "\n",
        "# RAG with URL\n",
        "def rag_with_url(prompt, chat_history):\n",
        "    target_url = \"https://kpmg.com/tr/tr/home/gorusler/2023/12/uretken-yapay-zeka-uygulamalarinin-kurumsallasma-yaklasimi.html\"\n",
        "    splitted_documents = load_and_split_documents(target_url, embeddings)\n",
        "    custom_documents = create_custom_documents(splitted_documents)\n",
        "    retriever = initialize_vectorstore(custom_documents, embeddings)\n",
        "    relevant_documents = retrieve_relevant_documents(retriever, prompt)\n",
        "\n",
        "    context_data = \" \".join([doc.page_content for doc in relevant_documents])\n",
        "    return generate_final_prompt(prompt, context_data, chat_history)\n",
        "\n",
        "# Initialize Model\n",
        "def initialize_model():\n",
        "    model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb_config\n",
        "    )\n",
        "    return tokenizer, model\n",
        "\n",
        "tokenizer, model = initialize_model()\n",
        "\n",
        "# Generate LLaMA2 Response\n",
        "def generate_llama3_response(prompt_input, tokenizer, model):\n",
        "    SYS_PROMPT = \"\"\"You are an assistant for answering questions.\n",
        "    You are given the extracted parts of a long document and a question. Provide a conversational answer.\n",
        "    If you don't know the answer, just say \"I do not know.\" Don't make up an answer.\"\"\"\n",
        "\n",
        "    messages = [{\"role\":\"system\",\"content\":SYS_PROMPT},{\"role\":\"user\",\"content\":prompt_input}]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=1024,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        temperature=0.6,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "# Streamlit App Setup\n",
        "def setup_streamlit():\n",
        "    with st.sidebar:\n",
        "        st.title('ðŸ¦™ðŸ’¬ Sistem MÃ¼hendisliÄŸi GiriÅŸ')\n",
        "        st.subheader('Models and parameters')\n",
        "        temperature = st.sidebar.slider('temperature', min_value=0.01, max_value=5.0, value=0.1, step=0.01)\n",
        "        top_p = st.sidebar.slider('top_p', min_value=0.01, max_value=1.0, value=0.9, step=0.01)\n",
        "        max_length = st.sidebar.slider('max_length', min_value=32, max_value=128, value=120, step=8)\n",
        "        st.markdown('ðŸ“– Learn how to build this app in this [blog](https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/)!')\n",
        "\n",
        "    if \"messages\" not in st.session_state.keys():\n",
        "        st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.write(message[\"content\"])\n",
        "\n",
        "def clear_chat_history():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "st.sidebar.button('Clear Chat History', on_click=clear_chat_history)\n",
        "\n",
        "# Main Function\n",
        "def main():\n",
        "    setup_streamlit()\n",
        "\n",
        "    if prompt := st.chat_input(\"MesajÄ±nÄ±zÄ± Giriniz:\"):\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.write(prompt)\n",
        "\n",
        "    if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            with st.spinner(\"Thinking...\"):\n",
        "                prompt_input = rag_with_url(prompt, st.session_state.messages)\n",
        "                response = generate_llama3_response(prompt_input, tokenizer, model)\n",
        "                placeholder = st.empty()\n",
        "                full_response = ''\n",
        "                for item in response:\n",
        "                    full_response += item\n",
        "                placeholder.markdown(full_response)\n",
        "        message = {\"role\": \"assistant\", \"content\": full_response}\n",
        "        st.session_state.messages.append(message)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "RlxZ69ub2yjS",
        "outputId": "db78fd6a-20b7-418e-b597-953a0902eb0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app_v3.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!streamlit run /content/app_v3.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "tXkPWc1P233O",
        "outputId": "0cee05f7-6969-4dee-ba5c-f8351326ba3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[34;40mnotice\u001b[0m\u001b[35m\u001b[0m created a lockfile as package-lock.json. You should commit this file.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "added 22 packages from 22 contributors and audited 22 packages in 1.576s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 1 \u001b[93mmoderate\u001b[0m severity vulnerability\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 1.149s\n",
            "your url is: https://loose-grapes-bathe.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app_v4.py\n",
        "import streamlit as st\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import FAISS # it is vector database. we will use FAISS database. It is created by Facebook.\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import CohereEmbeddings\n",
        "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import pipeline\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.documents import Document\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# Set up Streamlit page configuration\n",
        "st.set_page_config(page_title=\"ðŸ¦™ðŸ’¬ Sistem MÃ¼hendisliÄŸi GiriÅŸ Chatbot\")\n",
        "\n",
        "# Initialize Embeddings\n",
        "def initialize_embeddings():\n",
        "    return HuggingFaceInferenceAPIEmbeddings(\n",
        "        api_key='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',\n",
        "        model_name=\"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "    )\n",
        "\n",
        "embeddings = initialize_embeddings()\n",
        "\n",
        "# Load and Split Documents\n",
        "def load_and_split_documents(url, embeddings):\n",
        "    loader = WebBaseLoader(url)\n",
        "    raw_documents = loader.load()\n",
        "    text_splitter = SemanticChunker(embeddings, breakpoint_threshold_type=\"percentile\")\n",
        "    return text_splitter.split_documents(raw_documents)\n",
        "\n",
        "# Create Custom Documents\n",
        "def create_custom_documents(splitted_documents):\n",
        "    custom_documents = []\n",
        "    for i, raw_doc in enumerate(splitted_documents):\n",
        "        new_doc = Document(\n",
        "                page_content=raw_doc.page_content,\n",
        "                metadata = {\n",
        "                    \"source\": raw_doc.metadata[\"source\"],\n",
        "                    \"title\" : raw_doc.metadata[\"title\"],\n",
        "                    \"description\" : raw_doc.metadata[\"description\"],\n",
        "                    \"language\" : raw_doc.metadata[\"language\"],\n",
        "                    \"doc_id\" : i\n",
        "                }\n",
        "        )\n",
        "        custom_documents.append(new_doc)\n",
        "    return custom_documents\n",
        "\n",
        "# Initialize Vectorstore and Retriever\n",
        "def initialize_vectorstore(custom_documents, embeddings):\n",
        "    vectorstore = Chroma.from_documents(custom_documents, embeddings)\n",
        "    return vectorstore.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs={'k': 4, 'lambda_mult': 0.25}\n",
        "    )\n",
        "\n",
        "# Retrieve Relevant Documents\n",
        "def retrieve_relevant_documents(retriever, prompt):\n",
        "    return retriever.get_relevant_documents(prompt)\n",
        "\n",
        "# Generate Final Prompt\n",
        "def generate_final_prompt(prompt, context_data, chat_history):\n",
        "    history_prompt = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in chat_history])\n",
        "    return f\"\"\"\n",
        "    Ã–nceki konuÅŸmalarÄ±mÄ±z:\n",
        "    {history_prompt}\n",
        "\n",
        "    ÅžÃ¶yle bir sorum var: {prompt}\n",
        "    Bu soruyu yanÄ±tlamak iÃ§in elimizde ÅŸu bilgiler var: {context_data} .\n",
        "    Bu sorunun yanÄ±tÄ±nÄ± vermek iÃ§in yalnÄ±zca sana burada verdiÄŸim eldeki bilgileri kullan. BunlarÄ±n dÄ±ÅŸÄ±na asla Ã§Ä±kma.\n",
        "    \"\"\"\n",
        "\n",
        "# RAG with URL\n",
        "def rag_with_url(prompt, chat_history):\n",
        "    target_url = \"https://kpmg.com/tr/tr/home/gorusler/2023/12/uretken-yapay-zeka-uygulamalarinin-kurumsallasma-yaklasimi.html\"\n",
        "    splitted_documents = load_and_split_documents(target_url, embeddings)\n",
        "    custom_documents = create_custom_documents(splitted_documents)\n",
        "    retriever = initialize_vectorstore(custom_documents, embeddings)\n",
        "    relevant_documents = retrieve_relevant_documents(retriever, prompt)\n",
        "\n",
        "    context_data = \" \".join([doc.page_content for doc in relevant_documents])\n",
        "    return generate_final_prompt(prompt, context_data, chat_history)\n",
        "\n",
        "# Initialize Model\n",
        "def initialize_model():\n",
        "    model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb_config\n",
        "    )\n",
        "    return tokenizer, model\n",
        "\n",
        "tokenizer, model = initialize_model()\n",
        "\n",
        "# Generate LLaMA2 Response\n",
        "def generate_llama3_response(prompt_input, tokenizer, model):\n",
        "    SYS_PROMPT = \"\"\"You are an assistant for answering questions.\n",
        "    You are given the extracted parts of a long document and a question. Provide a conversational answer.\n",
        "    If you don't know the answer, just say \"I do not know.\" Don't make up an answer.\"\"\"\n",
        "\n",
        "    messages = [{\"role\":\"system\",\"content\":SYS_PROMPT},{\"role\":\"user\",\"content\":prompt_input}]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=1024,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        temperature=0.6,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "# Streamlit App Setup\n",
        "def setup_streamlit():\n",
        "    with st.sidebar:\n",
        "        st.title('ðŸ¦™ðŸ’¬ Sistem MÃ¼hendisliÄŸi GiriÅŸ')\n",
        "        st.subheader('Models and parameters')\n",
        "        temperature = st.sidebar.slider('temperature', min_value=0.01, max_value=5.0, value=0.1, step=0.01)\n",
        "        top_p = st.sidebar.slider('top_p', min_value=0.01, max_value=1.0, value=0.9, step=0.01)\n",
        "        max_length = st.sidebar.slider('max_length', min_value=32, max_value=128, value=120, step=8)\n",
        "        st.markdown('ðŸ“– Learn how to build this app in this [blog](https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/)!')\n",
        "\n",
        "    if \"messages\" not in st.session_state.keys():\n",
        "        st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.write(message[\"content\"])\n",
        "\n",
        "def clear_chat_history():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "st.sidebar.button('Clear Chat History', on_click=clear_chat_history)\n",
        "\n",
        "# Main Function\n",
        "def main():\n",
        "    setup_streamlit()\n",
        "\n",
        "    if prompt := st.chat_input(\"MesajÄ±nÄ±zÄ± Giriniz:\"):\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.write(prompt)\n",
        "\n",
        "    if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            with st.spinner(\"Thinking...\"):\n",
        "                prompt_input = rag_with_url(prompt, st.session_state.messages)\n",
        "                response = generate_llama3_response(prompt_input, tokenizer, model)\n",
        "                placeholder = st.empty()\n",
        "                full_response = ''\n",
        "                for item in response:\n",
        "                    full_response += item\n",
        "                placeholder.markdown(full_response)\n",
        "        message = {\"role\": \"assistant\", \"content\": full_response}\n",
        "        st.session_state.messages.append(message)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "ysyclUONcNRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!streamlit run /content/app_v4.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "UkXERFuxcPyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install get_embedding_function"
      ],
      "metadata": {
        "id": "yceGPuWWp_x0",
        "outputId": "153ebd24-25ba-4315-c76f-7a2145454da9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement get_embedding_function (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for get_embedding_function\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import shutil\n",
        "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.schema.document import Document\n",
        "#from get_embedding_function import get_embedding_function\n",
        "from langchain.vectorstores.chroma import Chroma\n",
        "\n",
        "\n",
        "CHROMA_PATH = \"chroma\"\n",
        "DATA_PATH = \"data\"\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    # Check if the database should be cleared (using the --clear flag).\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--reset\", action=\"store_true\", help=\"Reset the database.\")\n",
        "    args = parser.parse_args()\n",
        "    if args.reset:\n",
        "        print(\"âœ¨ Clearing Database\")\n",
        "        clear_database()\n",
        "\n",
        "    # Create (or update) the data store.\n",
        "    documents = load_documents()\n",
        "    chunks = split_documents(documents)\n",
        "    add_to_chroma(chunks)\n",
        "\n",
        "\n",
        "def load_documents():\n",
        "    document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
        "    return document_loader.load()\n",
        "\n",
        "\n",
        "def split_documents(documents: list[Document]):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=800,\n",
        "        chunk_overlap=80,\n",
        "        length_function=len,\n",
        "        is_separator_regex=False,\n",
        "    )\n",
        "    return text_splitter.split_documents(documents)\n",
        "\n",
        "\n",
        "def add_to_chroma(chunks: list[Document]):\n",
        "    # Load the existing database.\n",
        "    db = Chroma(\n",
        "        persist_directory=CHROMA_PATH, embedding_function=get_embedding_function()\n",
        "    )\n",
        "\n",
        "    # Calculate Page IDs.\n",
        "    chunks_with_ids = calculate_chunk_ids(chunks)\n",
        "\n",
        "    # Add or Update the documents.\n",
        "    existing_items = db.get(include=[])  # IDs are always included by default\n",
        "    existing_ids = set(existing_items[\"ids\"])\n",
        "    print(f\"Number of existing documents in DB: {len(existing_ids)}\")\n",
        "\n",
        "    # Only add documents that don't exist in the DB.\n",
        "    new_chunks = []\n",
        "    for chunk in chunks_with_ids:\n",
        "        if chunk.metadata[\"id\"] not in existing_ids:\n",
        "            new_chunks.append(chunk)\n",
        "\n",
        "    if len(new_chunks):\n",
        "        print(f\"ðŸ‘‰ Adding new documents: {len(new_chunks)}\")\n",
        "        new_chunk_ids = [chunk.metadata[\"id\"] for chunk in new_chunks]\n",
        "        db.add_documents(new_chunks, ids=new_chunk_ids)\n",
        "        db.persist()\n",
        "    else:\n",
        "        print(\"âœ… No new documents to add\")\n",
        "\n",
        "\n",
        "def calculate_chunk_ids(chunks):\n",
        "\n",
        "    # This will create IDs like \"data/monopoly.pdf:6:2\"\n",
        "    # Page Source : Page Number : Chunk Index\n",
        "\n",
        "    last_page_id = None\n",
        "    current_chunk_index = 0\n",
        "\n",
        "    for chunk in chunks:\n",
        "        source = chunk.metadata.get(\"source\")\n",
        "        page = chunk.metadata.get(\"page\")\n",
        "        current_page_id = f\"{source}:{page}\"\n",
        "\n",
        "        # If the page ID is the same as the last one, increment the index.\n",
        "        if current_page_id == last_page_id:\n",
        "            current_chunk_index += 1\n",
        "        else:\n",
        "            current_chunk_index = 0\n",
        "\n",
        "        # Calculate the chunk ID.\n",
        "        chunk_id = f\"{current_page_id}:{current_chunk_index}\"\n",
        "        last_page_id = current_page_id\n",
        "\n",
        "        # Add it to the page meta-data.\n",
        "        chunk.metadata[\"id\"] = chunk_id\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def clear_database():\n",
        "    if os.path.exists(CHROMA_PATH):\n",
        "        shutil.rmtree(CHROMA_PATH)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "SE9AkvFCl00A",
        "outputId": "124a0afe-242f-4337-a4bd-0c82881beb26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [--reset]\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-387f0d8f-0a29-462e-970c-175b176ab960.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence-transformers"
      ],
      "metadata": {
        "id": "R9vTyjDdt79O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app_v4.py\n",
        "import streamlit as st\n",
        "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Set up Streamlit page configuration\n",
        "st.set_page_config(page_title=\"ðŸ¦™ðŸ’¬ Sistem MÃ¼hendisliÄŸi GiriÅŸ Chatbot\")\n",
        "\n",
        "# Constants\n",
        "DATA_PATH = \"data\"\n",
        "CHROMA_PATH = \"chroma\"\n",
        "\n",
        "# Initialize Embeddings\n",
        "def initialize_embeddings():\n",
        "    return HuggingFaceEmbeddings(\n",
        "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "    )\n",
        "\n",
        "embeddings = initialize_embeddings()\n",
        "\n",
        "# Load and Split Documents\n",
        "def load_and_split_documents(data_path, embeddings):\n",
        "    document_loader = PyPDFDirectoryLoader(data_path)\n",
        "    raw_documents = document_loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=800,\n",
        "        chunk_overlap=80,\n",
        "        length_function=len\n",
        "    )\n",
        "    return text_splitter.split_documents(raw_documents)\n",
        "\n",
        "# Create Custom Documents\n",
        "def create_custom_documents(splitted_documents):\n",
        "    custom_documents = []\n",
        "    for i, raw_doc in enumerate(splitted_documents):\n",
        "        new_doc = Document(\n",
        "                page_content=raw_doc.page_content,\n",
        "                metadata = {\n",
        "                    \"source\": raw_doc.metadata.get(\"source\"),\n",
        "                    \"title\" : raw_doc.metadata.get(\"title\"),\n",
        "                    \"description\" : raw_doc.metadata.get(\"description\"),\n",
        "                    \"language\" : raw_doc.metadata.get(\"language\"),\n",
        "                    \"doc_id\" : i\n",
        "                }\n",
        "        )\n",
        "        custom_documents.append(new_doc)\n",
        "    return custom_documents\n",
        "\n",
        "# Initialize Vectorstore and Retriever\n",
        "def initialize_vectorstore(custom_documents, embeddings, persist_directory):\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        custom_documents,\n",
        "        embeddings,\n",
        "        persist_directory=persist_directory\n",
        "    )\n",
        "    return vectorstore.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs={'k': 4, 'lambda_mult': 0.25}\n",
        "    )\n",
        "\n",
        "# Retrieve Relevant Documents\n",
        "def retrieve_relevant_documents(retriever, prompt):\n",
        "    return retriever.get_relevant_documents(prompt)\n",
        "\n",
        "# Generate Final Prompt\n",
        "def generate_final_prompt(prompt, context_data, chat_history, relevant_documents):\n",
        "    history_prompt = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in chat_history])\n",
        "    metadata_info = \"\\n\".join([f\"Belge {doc.metadata['doc_id']} - BaÅŸlÄ±k: {doc.metadata['title']}, Kaynak: {doc.metadata['source']}\" for doc in relevant_documents])\n",
        "    return f\"\"\"\n",
        "    Ã–nceki konuÅŸmalarÄ±mÄ±z:\n",
        "    {history_prompt}\n",
        "\n",
        "    ÅžÃ¶yle bir sorum var: {prompt}\n",
        "    Bu soruyu yanÄ±tlamak iÃ§in elimizde ÅŸu bilgiler var: {context_data}\n",
        "    Belge Bilgileri: {metadata_info}\n",
        "    Bu sorunun yanÄ±tÄ±nÄ± vermek iÃ§in yalnÄ±zca sana burada verdiÄŸim eldeki bilgileri kullan. BunlarÄ±n dÄ±ÅŸÄ±na asla Ã§Ä±kma. Cevap Verirken yukarÄ±da belirttiÄŸim belge bilgilerinide kullan.\n",
        "    \"\"\"\n",
        "\n",
        "# RAG with Multiple PDFs\n",
        "def rag_with_multiple_pdfs(prompt, chat_history):\n",
        "    splitted_documents = load_and_split_documents(DATA_PATH, embeddings)\n",
        "    custom_documents = create_custom_documents(splitted_documents)\n",
        "    retriever = initialize_vectorstore(custom_documents, embeddings, CHROMA_PATH)\n",
        "    relevant_documents = retrieve_relevant_documents(retriever, prompt)\n",
        "\n",
        "    context_data = \" \".join([doc.page_content for doc in relevant_documents])\n",
        "    return generate_final_prompt(prompt, context_data, chat_history, relevant_documents)\n",
        "\n",
        "# Initialize Model\n",
        "def initialize_model():\n",
        "    model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb_config\n",
        "    )\n",
        "    return tokenizer, model\n",
        "\n",
        "tokenizer, model = initialize_model()\n",
        "\n",
        "# Generate LLaMA3 Response\n",
        "def generate_llama3_response(prompt_input, tokenizer, model):\n",
        "    SYS_PROMPT = \"\"\"You are an assistant for answering questions.\n",
        "    You are given the extracted parts of a long document and a question. Provide a conversational answer.\n",
        "    If you don't know the answer, just say \"I do not know.\" Don't make up an answer.\"\"\"\n",
        "\n",
        "    messages = [{\"role\":\"system\",\"content\":SYS_PROMPT},{\"role\":\"user\",\"content\":prompt_input}]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=1024,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        temperature=0.6,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "# Streamlit App Setup\n",
        "def setup_streamlit():\n",
        "    with st.sidebar:\n",
        "        st.title('ðŸ¦™ðŸ’¬ Sistem MÃ¼hendisliÄŸi GiriÅŸ')\n",
        "        st.subheader('Models and parameters')\n",
        "        temperature = st.sidebar.slider('temperature', min_value=0.01, max_value=5.0, value=0.1, step=0.01)\n",
        "        top_p = st.sidebar.slider('top_p', min_value=0.01, max_value=1.0, value=0.9, step=0.01)\n",
        "        max_length = st.sidebar.slider('max_length', min_value=32, max_value=128, value=120, step=8)\n",
        "        st.markdown('ðŸ“– Learn how to build this app in this [blog](https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/)!')\n",
        "\n",
        "    if \"messages\" not in st.session_state.keys():\n",
        "        st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.write(message[\"content\"])\n",
        "\n",
        "def clear_chat_history():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "st.sidebar.button('Clear Chat History', on_click=clear_chat_history)\n",
        "\n",
        "# Main Function\n",
        "def main():\n",
        "    setup_streamlit()\n",
        "\n",
        "    if prompt := st.chat_input(\"MesajÄ±nÄ±zÄ± Giriniz:\"):\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.write(prompt)\n",
        "\n",
        "    if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            with st.spinner(\"Thinking...\"):\n",
        "                prompt_input = rag_with_multiple_pdfs(prompt, st.session_state.messages)\n",
        "                response = generate_llama3_response(prompt_input, tokenizer, model)\n",
        "                placeholder = st.empty()\n",
        "                full_response = ''\n",
        "                for item in response:\n",
        "                    full_response += item\n",
        "                placeholder.markdown(full_response)\n",
        "        message = {\"role\": \"assistant\", \"content\": full_response}\n",
        "        st.session_state.messages.append(message)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "fy5JU_MNsJef",
        "outputId": "c54ade47-6c5e-4960-9142-c2a3d8500ee2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app_v4.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!streamlit run /content/app_v4.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "5nrpXTICsMC0",
        "outputId": "cb47b636-3bb1-49a9-f769-0b90ad479d6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[34;40mnotice\u001b[0m\u001b[35m\u001b[0m created a lockfile as package-lock.json. You should commit this file.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "added 22 packages from 22 contributors and audited 22 packages in 1.466s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 1 \u001b[93mmoderate\u001b[0m severity vulnerability\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 1.072s\n",
            "your url is: https://itchy-rules-take.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}