{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPBfbb1gB9DdPkpANxuzmcc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmk4444/System_engineering/blob/main/System_eng_Part3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain openai pypdf chroma streamlit langchain_openai langchain_community langchain transformers bitsandbytes accelerate torch faiss-gpu faiss-cpu langchain_chroma langchain_experimental"
      ],
      "metadata": {
        "id": "pRhoGD70xtE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import pipeline\n",
        "\n",
        "login(token = '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')"
      ],
      "metadata": {
        "id": "9khH96Vv2qLW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2202504-0af2-48a1-e39f-42056f8068d5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D72PV_cpwfZV",
        "outputId": "7c37359f-4105-4dc0-b242-33c06ca52a31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import FAISS # it is vector database. we will use FAISS database. It is created by Facebook.\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import CohereEmbeddings\n",
        "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import pipeline\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.documents import Document\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
        "    api_key='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',\n",
        "    model_name=\"sentence-transformers/all-MiniLM-l6-v2\"\n",
        ")\n",
        "\n",
        "def rag_with_url(prompt):\n",
        "  from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "  target_url=\"https://kpmg.com/tr/tr/home/gorusler/2023/12/uretken-yapay-zeka-uygulamalarinin-kurumsallasma-yaklasimi.html\"\n",
        "\n",
        "  loader = WebBaseLoader(target_url) # we create a loader object and we assign to this object.\n",
        "  raw_documents = loader.load() # we load document from url. (type is document)#load url\n",
        "\n",
        "\n",
        "  #create chunk using splitters\n",
        "  #below code can run without any parameters but to reach the best answer you should try different parameters.\n",
        "\n",
        "\n",
        "  text_splitter = SemanticChunker(embeddings, breakpoint_threshold_type=\"percentile\")\n",
        "\n",
        "  # we run split_documents method which is inside of text_splitter and save to splitted_document\n",
        "  splitted_document = text_splitter.split_documents(raw_documents)\n",
        "\n",
        "  custom_documents = []\n",
        "\n",
        "  for i, raw_doc in enumerate(splitted_document):\n",
        "    new_doc = Document(\n",
        "            page_content=raw_doc.page_content,\n",
        "            metadata = {\n",
        "                \"source\": raw_doc.metadata[\"source\"],\n",
        "                \"title\" : raw_doc.metadata[\"title\"],\n",
        "                \"description\" : raw_doc.metadata[\"description\"],\n",
        "                \"language\" : raw_doc.metadata[\"language\"],\n",
        "                \"doc_id\" : i\n",
        "            }\n",
        "    )\n",
        "    custom_documents.append(new_doc)\n",
        "\n",
        "  # we need to save splited documents in vector database.\n",
        "  # FAISS create vectors from our documents using our embedding model and save vectordatabase.\n",
        "  #vectorstore = FAISS.from_documents(splitted_document, embeddings)\n",
        "  vectorstore = Chroma.from_documents(custom_documents, embeddings)\n",
        "  # we reach as_retriever method in vectorstore. We convert vector store into retriever.\n",
        "  # retriever won't retrieve all vectors, it retrieves vector considering context similarity among vector and prompt.\n",
        "  #mostly, similarity is estimated by cosine similarity.\n",
        "  retriever = vectorstore.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs={'k': 4, 'lambda_mult': 0.25}\n",
        "  )\n",
        "  relevant_documents = retriever.get_relevant_documents(prompt) # it retrieves the most similar document accordingly our prompt. It can be 3,4 or more documents. Default value is 4.\n",
        "\n",
        "  context_data =\"\"\n",
        "\n",
        "  #Also, we use build function instead of the below code.\n",
        "  for document in relevant_documents:\n",
        "    context_data = context_data + \" \" + document.page_content # document has two features which are metada and page_content.\n",
        "    #because the main data is in context_data.\n",
        "  final_prompt = f\"\"\"ÅžÃ¶yle bir sorum var: {prompt}\n",
        "  Bu soruyu yanÄ±tlamak iÃ§in elimizde ÅŸu bilgiler var: {context_data} .\n",
        "  Bu sorunun yanÄ±tÄ±nÄ± vermek iÃ§in yalnÄ±zca sana burada verdiÄŸim eldeki bilgileri kullan. BunlarÄ±n dÄ±ÅŸÄ±na asla Ã§Ä±kma.\n",
        "  \"\"\"\n",
        "\n",
        "  return final_prompt\n",
        "\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "# use quantization to lower GPU usage\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "terminators = [\n",
        "    tokenizer.eos_token_id,\n",
        "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "\n",
        "\n",
        "# App title\n",
        "st.set_page_config(page_title=\"ðŸ¦™ðŸ’¬ Sistem MÃ¼hendisliÄŸi GiriÅŸ Chatbot\")\n",
        "\n",
        "# Replicate Credentials\n",
        "with st.sidebar:\n",
        "\n",
        "    st.title('ðŸ¦™ðŸ’¬ Sistem MÃ¼hendisliÄŸi GiriÅŸ')\n",
        "\n",
        "    st.subheader('Models and parameters')\n",
        "\n",
        "    temperature = st.sidebar.slider('temperature', min_value=0.01, max_value=5.0, value=0.1, step=0.01)\n",
        "    top_p = st.sidebar.slider('top_p', min_value=0.01, max_value=1.0, value=0.9, step=0.01)\n",
        "    max_length = st.sidebar.slider('max_length', min_value=32, max_value=128, value=120, step=8)\n",
        "    st.markdown('ðŸ“– Learn how to build this app in this [blog](https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/)!')\n",
        "\n",
        "# Store LLM generated responses\n",
        "if \"messages\" not in st.session_state.keys():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "\n",
        "# Display or clear chat messages\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.write(message[\"content\"])\n",
        "\n",
        "def clear_chat_history():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "st.sidebar.button('Clear Chat History', on_click=clear_chat_history)\n",
        "\n",
        "# Function for generating LLaMA2 response. Refactored from https://github.com/a16z-infra/llama2-chatbot\n",
        "def generate_llama2_response(prompt_input):\n",
        "\n",
        "\n",
        "    SYS_PROMPT = \"\"\"You are an assistant for answering questions.\n",
        "    You are given the extracted parts of a long document and a question. Provide a conversational answer.\n",
        "    If you don't know the answer, just say \"I do not know.\" Don't make up an answer.\"\"\"\n",
        "\n",
        "\n",
        "    messages = [{\"role\":\"system\",\"content\":SYS_PROMPT},{\"role\":\"user\",\"content\":prompt_input}]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "      input_ids,\n",
        "      max_new_tokens=1024,\n",
        "      eos_token_id=terminators,\n",
        "      do_sample=True,\n",
        "      temperature=0.6,\n",
        "      top_p=0.9,\n",
        "    )\n",
        "\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "# User-provided prompt\n",
        "if prompt := st.chat_input(\"MesajÄ±nÄ±zÄ± Giriniz:\"):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.write(prompt)\n",
        "\n",
        "# Generate a new response if last message is not from assistant\n",
        "if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            prompt_input=rag_with_url(prompt)\n",
        "            response = generate_llama2_response(prompt_input)\n",
        "            placeholder = st.empty()\n",
        "            full_response = ''\n",
        "            for item in response:\n",
        "                full_response += item\n",
        "                #placeholder.markdown(full_response)\n",
        "            placeholder.markdown(full_response)\n",
        "    message = {\"role\": \"assistant\", \"content\": full_response}\n",
        "    st.session_state.messages.append(message)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!streamlit run /content/app.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "UrD8NWtKxltj",
        "outputId": "86d70b93-e23c-43d0-cf64-ef65b1e6bda0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[34;40mnotice\u001b[0m\u001b[35m\u001b[0m created a lockfile as package-lock.json. You should commit this file.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "added 22 packages from 22 contributors and audited 22 packages in 2.662s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 1 \u001b[93mmoderate\u001b[0m severity vulnerability\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 1.161s\n",
            "your url is: https://hip-cases-refuse.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app_v2.py\n",
        "import streamlit as st\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import FAISS # it is vector database. we will use FAISS database. It is created by Facebook.\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import CohereEmbeddings\n",
        "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import pipeline\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.documents import Document\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
        "    api_key='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',\n",
        "    model_name=\"sentence-transformers/all-MiniLM-l6-v2\"\n",
        ")\n",
        "\n",
        "def rag_with_url(prompt, chat_history):\n",
        "    from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "    target_url = \"https://kpmg.com/tr/tr/home/gorusler/2023/12/uretken-yapay-zeka-uygulamalarinin-kurumsallasma-yaklasimi.html\"\n",
        "\n",
        "    loader = WebBaseLoader(target_url) # we create a loader object and we assign to this object.\n",
        "    raw_documents = loader.load() # we load document from url. (type is document)#load url\n",
        "\n",
        "    #create chunk using splitters\n",
        "    text_splitter = SemanticChunker(embeddings, breakpoint_threshold_type=\"percentile\")\n",
        "\n",
        "    # we run split_documents method which is inside of text_splitter and save to splitted_document\n",
        "    splitted_document = text_splitter.split_documents(raw_documents)\n",
        "\n",
        "    custom_documents = []\n",
        "\n",
        "    for i, raw_doc in enumerate(splitted_document):\n",
        "        new_doc = Document(\n",
        "                page_content=raw_doc.page_content,\n",
        "                metadata = {\n",
        "                    \"source\": raw_doc.metadata[\"source\"],\n",
        "                    \"title\" : raw_doc.metadata[\"title\"],\n",
        "                    \"description\" : raw_doc.metadata[\"description\"],\n",
        "                    \"language\" : raw_doc.metadata[\"language\"],\n",
        "                    \"doc_id\" : i\n",
        "                }\n",
        "        )\n",
        "        custom_documents.append(new_doc)\n",
        "\n",
        "    # we need to save splited documents in vector database.\n",
        "    vectorstore = Chroma.from_documents(custom_documents, embeddings)\n",
        "    retriever = vectorstore.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs={'k': 4, 'lambda_mult': 0.25}\n",
        "    )\n",
        "    relevant_documents = retriever.get_relevant_documents(prompt) # it retrieves the most similar document accordingly our prompt. It can be 3,4 or more documents. Default value is 4.\n",
        "\n",
        "    context_data = \"\"\n",
        "    for document in relevant_documents:\n",
        "        context_data += \" \" + document.page_content # document has two features which are metada and page_content.\n",
        "\n",
        "    # Add chat history to the prompt in a more structured way\n",
        "    history_prompt = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in chat_history])\n",
        "\n",
        "    final_prompt = f\"\"\"\n",
        "    Ã–nceki konuÅŸmalarÄ±mÄ±z:\n",
        "    {history_prompt}\n",
        "\n",
        "    ÅžÃ¶yle bir sorum var: {prompt}\n",
        "    Bu soruyu yanÄ±tlamak iÃ§in elimizde ÅŸu bilgiler var: {context_data} .\n",
        "    Bu sorunun yanÄ±tÄ±nÄ± vermek iÃ§in yalnÄ±zca sana burada verdiÄŸim eldeki bilgileri kullan. BunlarÄ±n dÄ±ÅŸÄ±na asla Ã§Ä±kma.\n",
        "    \"\"\"\n",
        "\n",
        "    return final_prompt\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "# use quantization to lower GPU usage\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "terminators = [\n",
        "    tokenizer.eos_token_id\n",
        "]\n",
        "\n",
        "# App title\n",
        "st.set_page_config(page_title=\"ðŸ¦™ðŸ’¬ Sistem MÃ¼hendisliÄŸi GiriÅŸ Chatbot\")\n",
        "\n",
        "# Replicate Credentials\n",
        "with st.sidebar:\n",
        "    st.title('ðŸ¦™ðŸ’¬ Sistem MÃ¼hendisliÄŸi GiriÅŸ')\n",
        "    st.subheader('Models and parameters')\n",
        "\n",
        "    temperature = st.sidebar.slider('temperature', min_value=0.01, max_value=5.0, value=0.1, step=0.01)\n",
        "    top_p = st.sidebar.slider('top_p', min_value=0.01, max_value=1.0, value=0.9, step=0.01)\n",
        "    max_length = st.sidebar.slider('max_length', min_value=32, max_value=128, value=120, step=8)\n",
        "    st.markdown('ðŸ“– Learn how to build this app in this [blog](https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/)!')\n",
        "\n",
        "# Store LLM generated responses\n",
        "if \"messages\" not in st.session_state.keys():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "\n",
        "# Display or clear chat messages\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.write(message[\"content\"])\n",
        "\n",
        "def clear_chat_history():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "st.sidebar.button('Clear Chat History', on_click=clear_chat_history)\n",
        "\n",
        "# Function for generating LLaMA2 response. Refactored from https://github.com/a16z-infra/llama2-chatbot\n",
        "def generate_llama2_response(prompt_input):\n",
        "    SYS_PROMPT = \"\"\"You are an assistant for answering questions.\n",
        "    You are given the extracted parts of a long document and a question. Provide a conversational answer.\n",
        "    If you don't know the answer, just say \"I do not know.\" Don't make up an answer.\"\"\"\n",
        "\n",
        "    messages = [{\"role\":\"system\",\"content\":SYS_PROMPT},{\"role\":\"user\",\"content\":prompt_input}]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "      input_ids,\n",
        "      max_new_tokens=1024,\n",
        "      eos_token_id=terminators[0],\n",
        "      do_sample=True,\n",
        "      temperature=0.6,\n",
        "      top_p=0.9,\n",
        "    )\n",
        "\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "# User-provided prompt\n",
        "if prompt := st.chat_input(\"MesajÄ±nÄ±zÄ± Giriniz:\"):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.write(prompt)\n",
        "\n",
        "# Generate a new response if last message is not from assistant\n",
        "if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            prompt_input = rag_with_url(prompt, st.session_state.messages)\n",
        "            response = generate_llama2_response(prompt_input)\n",
        "            placeholder = st.empty()\n",
        "            full_response = ''\n",
        "            for item in response:\n",
        "                full_response += item\n",
        "                #placeholder.markdown(full_response)\n",
        "            placeholder.markdown(full_response)\n",
        "    message = {\"role\": \"assistant\", \"content\": full_response}\n",
        "    st.session_state.messages.append(message)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-73cAuVxiAZ",
        "outputId": "ac86f376-3d77-46be-eefd-2e17fe627888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app_v2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!streamlit run /content/app_v2.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ykiyt31xkC_",
        "outputId": "180db9ed-b488-4be6-f91a-a5175cbaafaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25h+ localtunnel@2.0.2\n",
            "updated 1 package and audited 36 packages in 0.477s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 2 \u001b[93mmoderate\u001b[0m severity vulnerabilities\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 1.737s\n",
            "your url is: https://clean-beers-sleep.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app_v3.py\n",
        "import streamlit as st\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import FAISS # it is vector database. we will use FAISS database. It is created by Facebook.\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import CohereEmbeddings\n",
        "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import pipeline\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.documents import Document\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# Set up Streamlit page configuration\n",
        "st.set_page_config(page_title=\"ðŸ¦™ðŸ’¬ Sistem MÃ¼hendisliÄŸi GiriÅŸ Chatbot\")\n",
        "\n",
        "# Initialize Embeddings\n",
        "def initialize_embeddings():\n",
        "    return HuggingFaceInferenceAPIEmbeddings(\n",
        "        api_key='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',\n",
        "        model_name=\"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "    )\n",
        "\n",
        "embeddings = initialize_embeddings()\n",
        "\n",
        "# Load and Split Documents\n",
        "def load_and_split_documents(url, embeddings):\n",
        "    loader = WebBaseLoader(url)\n",
        "    raw_documents = loader.load()\n",
        "    text_splitter = SemanticChunker(embeddings, breakpoint_threshold_type=\"percentile\")\n",
        "    return text_splitter.split_documents(raw_documents)\n",
        "\n",
        "# Create Custom Documents\n",
        "def create_custom_documents(splitted_documents):\n",
        "    custom_documents = []\n",
        "    for i, raw_doc in enumerate(splitted_documents):\n",
        "        new_doc = Document(\n",
        "                page_content=raw_doc.page_content,\n",
        "                metadata = {\n",
        "                    \"source\": raw_doc.metadata[\"source\"],\n",
        "                    \"title\" : raw_doc.metadata[\"title\"],\n",
        "                    \"description\" : raw_doc.metadata[\"description\"],\n",
        "                    \"language\" : raw_doc.metadata[\"language\"],\n",
        "                    \"doc_id\" : i\n",
        "                }\n",
        "        )\n",
        "        custom_documents.append(new_doc)\n",
        "    return custom_documents\n",
        "\n",
        "# Initialize Vectorstore and Retriever\n",
        "def initialize_vectorstore(custom_documents, embeddings):\n",
        "    vectorstore = Chroma.from_documents(custom_documents, embeddings)\n",
        "    return vectorstore.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs={'k': 4, 'lambda_mult': 0.25}\n",
        "    )\n",
        "\n",
        "# Retrieve Relevant Documents\n",
        "def retrieve_relevant_documents(retriever, prompt):\n",
        "    return retriever.get_relevant_documents(prompt)\n",
        "\n",
        "# Generate Final Prompt\n",
        "def generate_final_prompt(prompt, context_data, chat_history):\n",
        "    history_prompt = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in chat_history])\n",
        "    return f\"\"\"\n",
        "    Ã–nceki konuÅŸmalarÄ±mÄ±z:\n",
        "    {history_prompt}\n",
        "\n",
        "    ÅžÃ¶yle bir sorum var: {prompt}\n",
        "    Bu soruyu yanÄ±tlamak iÃ§in elimizde ÅŸu bilgiler var: {context_data} .\n",
        "    Bu sorunun yanÄ±tÄ±nÄ± vermek iÃ§in yalnÄ±zca sana burada verdiÄŸim eldeki bilgileri kullan. BunlarÄ±n dÄ±ÅŸÄ±na asla Ã§Ä±kma.\n",
        "    \"\"\"\n",
        "\n",
        "# RAG with URL\n",
        "def rag_with_url(prompt, chat_history):\n",
        "    target_url = \"https://kpmg.com/tr/tr/home/gorusler/2023/12/uretken-yapay-zeka-uygulamalarinin-kurumsallasma-yaklasimi.html\"\n",
        "    splitted_documents = load_and_split_documents(target_url, embeddings)\n",
        "    custom_documents = create_custom_documents(splitted_documents)\n",
        "    retriever = initialize_vectorstore(custom_documents, embeddings)\n",
        "    relevant_documents = retrieve_relevant_documents(retriever, prompt)\n",
        "\n",
        "    context_data = \" \".join([doc.page_content for doc in relevant_documents])\n",
        "    return generate_final_prompt(prompt, context_data, chat_history)\n",
        "\n",
        "# Initialize Model\n",
        "def initialize_model():\n",
        "    model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb_config\n",
        "    )\n",
        "    return tokenizer, model\n",
        "\n",
        "tokenizer, model = initialize_model()\n",
        "\n",
        "# Generate LLaMA2 Response\n",
        "def generate_llama3_response(prompt_input, tokenizer, model):\n",
        "    SYS_PROMPT = \"\"\"You are an assistant for answering questions.\n",
        "    You are given the extracted parts of a long document and a question. Provide a conversational answer.\n",
        "    If you don't know the answer, just say \"I do not know.\" Don't make up an answer.\"\"\"\n",
        "\n",
        "    messages = [{\"role\":\"system\",\"content\":SYS_PROMPT},{\"role\":\"user\",\"content\":prompt_input}]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=1024,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        temperature=0.6,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "# Streamlit App Setup\n",
        "def setup_streamlit():\n",
        "    with st.sidebar:\n",
        "        st.title('ðŸ¦™ðŸ’¬ Sistem MÃ¼hendisliÄŸi GiriÅŸ')\n",
        "        st.subheader('Models and parameters')\n",
        "        temperature = st.sidebar.slider('temperature', min_value=0.01, max_value=5.0, value=0.1, step=0.01)\n",
        "        top_p = st.sidebar.slider('top_p', min_value=0.01, max_value=1.0, value=0.9, step=0.01)\n",
        "        max_length = st.sidebar.slider('max_length', min_value=32, max_value=128, value=120, step=8)\n",
        "        st.markdown('ðŸ“– Learn how to build this app in this [blog](https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/)!')\n",
        "\n",
        "    if \"messages\" not in st.session_state.keys():\n",
        "        st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.write(message[\"content\"])\n",
        "\n",
        "def clear_chat_history():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "st.sidebar.button('Clear Chat History', on_click=clear_chat_history)\n",
        "\n",
        "# Main Function\n",
        "def main():\n",
        "    setup_streamlit()\n",
        "\n",
        "    if prompt := st.chat_input(\"MesajÄ±nÄ±zÄ± Giriniz:\"):\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.write(prompt)\n",
        "\n",
        "    if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            with st.spinner(\"Thinking...\"):\n",
        "                prompt_input = rag_with_url(prompt, st.session_state.messages)\n",
        "                response = generate_llama3_response(prompt_input, tokenizer, model)\n",
        "                placeholder = st.empty()\n",
        "                full_response = ''\n",
        "                for item in response:\n",
        "                    full_response += item\n",
        "                placeholder.markdown(full_response)\n",
        "        message = {\"role\": \"assistant\", \"content\": full_response}\n",
        "        st.session_state.messages.append(message)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "RlxZ69ub2yjS",
        "outputId": "db78fd6a-20b7-418e-b597-953a0902eb0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app_v3.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!streamlit run /content/app_v3.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "tXkPWc1P233O",
        "outputId": "0cee05f7-6969-4dee-ba5c-f8351326ba3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[34;40mnotice\u001b[0m\u001b[35m\u001b[0m created a lockfile as package-lock.json. You should commit this file.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "added 22 packages from 22 contributors and audited 22 packages in 1.576s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 1 \u001b[93mmoderate\u001b[0m severity vulnerability\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 1.149s\n",
            "your url is: https://loose-grapes-bathe.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}