{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPZsNJtHE+SgQtz46S+6Yof",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmk4444/System_engineering/blob/main/System_eng_Part3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain openai pypdf chroma streamlit langchain_openai langchain_community langchain transformers bitsandbytes accelerate torch faiss-gpu faiss-cpu langchain_chroma langchain_experimental"
      ],
      "metadata": {
        "id": "pRhoGD70xtE4",
        "outputId": "a7c077d2-02a0-4253-b3ec-68d43d5902cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: langchain_experimental\n",
            "Successfully installed langchain_experimental-0.0.61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import pipeline\n",
        "\n",
        "login(token = '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')"
      ],
      "metadata": {
        "id": "9khH96Vv2qLW",
        "outputId": "13a2305f-7d03-42ef-8142-15817c6ff20b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "D72PV_cpwfZV",
        "outputId": "4dc177ee-da73-42bb-9cfb-3504b6e6d3eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import FAISS # it is vector database. we will use FAISS database. It is created by Facebook.\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import CohereEmbeddings\n",
        "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import pipeline\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.documents import Document\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
        "    api_key='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',\n",
        "    model_name=\"sentence-transformers/all-MiniLM-l6-v2\"\n",
        ")\n",
        "\n",
        "def rag_with_url(prompt):\n",
        "  from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "  target_url=\"https://kpmg.com/tr/tr/home/gorusler/2023/12/uretken-yapay-zeka-uygulamalarinin-kurumsallasma-yaklasimi.html\"\n",
        "\n",
        "  loader = WebBaseLoader(target_url) # we create a loader object and we assign to this object.\n",
        "  raw_documents = loader.load() # we load document from url. (type is document)#load url\n",
        "\n",
        "\n",
        "  #create chunk using splitters\n",
        "  #below code can run without any parameters but to reach the best answer you should try different parameters.\n",
        "\n",
        "\n",
        "  text_splitter = SemanticChunker(embeddings, breakpoint_threshold_type=\"percentile\")\n",
        "\n",
        "  # we run split_documents method which is inside of text_splitter and save to splitted_document\n",
        "  splitted_document = text_splitter.split_documents(raw_documents)\n",
        "\n",
        "  custom_documents = []\n",
        "\n",
        "  for i, raw_doc in enumerate(splitted_document):\n",
        "    new_doc = Document(\n",
        "            page_content=raw_doc.page_content,\n",
        "            metadata = {\n",
        "                \"source\": raw_doc.metadata[\"source\"],\n",
        "                \"title\" : raw_doc.metadata[\"title\"],\n",
        "                \"description\" : raw_doc.metadata[\"description\"],\n",
        "                \"language\" : raw_doc.metadata[\"language\"],\n",
        "                \"doc_id\" : i\n",
        "            }\n",
        "    )\n",
        "    custom_documents.append(new_doc)\n",
        "\n",
        "  # we need to save splited documents in vector database.\n",
        "  # FAISS create vectors from our documents using our embedding model and save vectordatabase.\n",
        "  #vectorstore = FAISS.from_documents(splitted_document, embeddings)\n",
        "  vectorstore = Chroma.from_documents(custom_documents, embeddings)\n",
        "  # we reach as_retriever method in vectorstore. We convert vector store into retriever.\n",
        "  # retriever won't retrieve all vectors, it retrieves vector considering context similarity among vector and prompt.\n",
        "  #mostly, similarity is estimated by cosine similarity.\n",
        "  retriever = vectorstore.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs={'k': 4, 'lambda_mult': 0.25}\n",
        "  )\n",
        "  relevant_documents = retriever.get_relevant_documents(prompt) # it retrieves the most similar document accordingly our prompt. It can be 3,4 or more documents. Default value is 4.\n",
        "\n",
        "  context_data =\"\"\n",
        "\n",
        "  #Also, we use build function instead of the below code.\n",
        "  for document in relevant_documents:\n",
        "    context_data = context_data + \" \" + document.page_content # document has two features which are metada and page_content.\n",
        "    #because the main data is in context_data.\n",
        "  final_prompt = f\"\"\"ÅžÃ¶yle bir sorum var: {prompt}\n",
        "  Bu soruyu yanÄ±tlamak iÃ§in elimizde ÅŸu bilgiler var: {context_data} .\n",
        "  Bu sorunun yanÄ±tÄ±nÄ± vermek iÃ§in yalnÄ±zca sana burada verdiÄŸim eldeki bilgileri kullan. BunlarÄ±n dÄ±ÅŸÄ±na asla Ã§Ä±kma.\n",
        "  \"\"\"\n",
        "\n",
        "  return final_prompt\n",
        "\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "# use quantization to lower GPU usage\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "terminators = [\n",
        "    tokenizer.eos_token_id,\n",
        "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "\n",
        "\n",
        "# App title\n",
        "st.set_page_config(page_title=\"ðŸ¦™ðŸ’¬ Sistem MÃ¼hendisliÄŸi GiriÅŸ Chatbot\")\n",
        "\n",
        "# Replicate Credentials\n",
        "with st.sidebar:\n",
        "\n",
        "    st.title('ðŸ¦™ðŸ’¬ Sistem MÃ¼hendisliÄŸi GiriÅŸ')\n",
        "\n",
        "    st.subheader('Models and parameters')\n",
        "\n",
        "    temperature = st.sidebar.slider('temperature', min_value=0.01, max_value=5.0, value=0.1, step=0.01)\n",
        "    top_p = st.sidebar.slider('top_p', min_value=0.01, max_value=1.0, value=0.9, step=0.01)\n",
        "    max_length = st.sidebar.slider('max_length', min_value=32, max_value=128, value=120, step=8)\n",
        "    st.markdown('ðŸ“– Learn how to build this app in this [blog](https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/)!')\n",
        "\n",
        "# Store LLM generated responses\n",
        "if \"messages\" not in st.session_state.keys():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "\n",
        "# Display or clear chat messages\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.write(message[\"content\"])\n",
        "\n",
        "def clear_chat_history():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "st.sidebar.button('Clear Chat History', on_click=clear_chat_history)\n",
        "\n",
        "# Function for generating LLaMA2 response. Refactored from https://github.com/a16z-infra/llama2-chatbot\n",
        "def generate_llama2_response(prompt_input):\n",
        "\n",
        "\n",
        "    SYS_PROMPT = \"\"\"You are an assistant for answering questions.\n",
        "    You are given the extracted parts of a long document and a question. Provide a conversational answer.\n",
        "    If you don't know the answer, just say \"I do not know.\" Don't make up an answer.\"\"\"\n",
        "\n",
        "\n",
        "    messages = [{\"role\":\"system\",\"content\":SYS_PROMPT},{\"role\":\"user\",\"content\":prompt_input}]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "      input_ids,\n",
        "      max_new_tokens=1024,\n",
        "      eos_token_id=terminators,\n",
        "      do_sample=True,\n",
        "      temperature=0.6,\n",
        "      top_p=0.9,\n",
        "    )\n",
        "\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "# User-provided prompt\n",
        "if prompt := st.chat_input(\"MesajÄ±nÄ±zÄ± Giriniz:\"):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.write(prompt)\n",
        "\n",
        "# Generate a new response if last message is not from assistant\n",
        "if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            prompt_input=rag_with_url(prompt)\n",
        "            response = generate_llama2_response(prompt_input)\n",
        "            placeholder = st.empty()\n",
        "            full_response = ''\n",
        "            for item in response:\n",
        "                full_response += item\n",
        "                placeholder.markdown(full_response)\n",
        "            placeholder.markdown(full_response)\n",
        "    message = {\"role\": \"assistant\", \"content\": full_response}\n",
        "    st.session_state.messages.append(message)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!streamlit run /content/app.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "UrD8NWtKxltj",
        "outputId": "063a04f6-960b-4cb2-9819-1e7ef731d7ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25h+ localtunnel@2.0.2\n",
            "updated 1 package and audited 36 packages in 0.435s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 2 \u001b[93mmoderate\u001b[0m severity vulnerabilities\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 1.202s\n",
            "your url is: https://many-walls-press.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "\n",
        "### Construct retriever ###\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "\n",
        "### Contextualize question ###\n",
        "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
        "which might reference context in the chat history, formulate a standalone question \\\n",
        "which can be understood without the chat history. Do NOT answer the question, \\\n",
        "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", contextualize_q_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    llm, retriever, contextualize_q_prompt\n",
        ")\n",
        "\n",
        "\n",
        "### Answer question ###\n",
        "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
        "Use the following pieces of retrieved context to answer the question. \\\n",
        "If you don't know the answer, just say that you don't know. \\\n",
        "Use three sentences maximum and keep the answer concise.\\\n",
        "\n",
        "{context}\"\"\"\n",
        "qa_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", qa_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "\n",
        "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
        "\n",
        "\n",
        "### Statefully manage chat history ###\n",
        "store = {}\n",
        "\n",
        "\n",
        "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ChatMessageHistory()\n",
        "    return store[session_id]\n",
        "\n",
        "\n",
        "conversational_rag_chain = RunnableWithMessageHistory(\n",
        "    rag_chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"chat_history\",\n",
        "    output_messages_key=\"answer\",\n",
        ")"
      ],
      "metadata": {
        "id": "n-73cAuVxiAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9ykiyt31xkC_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}