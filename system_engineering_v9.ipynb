{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "mount_file_id": "https://github.com/kmk4444/System_engineering/blob/main/system_engineering_v8.ipynb",
      "authorship_tag": "ABX9TyN793POjbNMjKfqSC9KJhjj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmk4444/System_engineering/blob/main/system_engineering_v9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain openai pypdf chroma streamlit langchain_openai langchain_community langchain transformers bitsandbytes accelerate torch faiss-gpu faiss-cpu langchain_chroma langchain_experimental sentence-transformers cohere rank_bm25 nltk scikit-learn"
      ],
      "metadata": {
        "id": "cp9iCXKFtO2c",
        "outputId": "836b37fe-274d-4807-a2d0-abb7f1484585",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.4)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.52.2)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting chroma\n",
            "  Downloading Chroma-0.2.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.39.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.2.6-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting langchain_chroma\n",
            "  Downloading langchain_chroma-0.1.4-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting langchain_experimental\n",
            "  Downloading langchain_experimental-0.3.3-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Collecting cohere\n",
            "  Downloading cohere-5.11.3-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.12 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.13)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.0)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.137)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (10.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.3)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog<6,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-5.0.3-py3-none-manylinux2014_x86_64.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.4.0,>=0.3.12 (from langchain)\n",
            "  Downloading langchain_core-0.3.15-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting openai\n",
            "  Downloading openai-1.54.2-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
            "  Downloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.7-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Collecting chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0 (from langchain_chroma)\n",
            "  Downloading chromadb-0.5.18-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting fastapi<1,>=0.95.2 (from langchain_chroma)\n",
            "  Downloading fastapi-0.115.4-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
            "  Downloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting parameterized<0.10.0,>=0.9.0 (from cohere)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.23.4)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
            "  Downloading types_requests-2.32.0.20241016-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Collecting build>=1.0.3 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting posthog>=2.4.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading posthog-3.7.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading onnxruntime-1.20.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.16.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.49b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.16.0)\n",
            "Collecting pypika>=0.48.9 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting overrides>=7.3.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (6.4.5)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.64.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.12.5)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting mmh3>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.10.10)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting starlette<0.42.0,>=0.40.0 (from fastapi<1,>=0.95.2->langchain_chroma)\n",
            "  Downloading starlette-0.41.2-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain) (3.0.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.16.0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.2.2)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (24.3.25)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.2.14)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (75.1.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.65.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.28.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.28.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.28.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading opentelemetry_proto-1.28.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading opentelemetry_sdk-1.28.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting protobuf<6,>=3.20 (from streamlit)\n",
            "  Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.49b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.49b0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting opentelemetry-instrumentation==0.49b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading opentelemetry_instrumentation-0.49b0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.49b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading opentelemetry_semantic_conventions-0.49b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-util-http==0.49b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading opentelemetry_util_http-0.49b0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.49b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.16.0)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.49b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading opentelemetry_api-1.28.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (8.5.0)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (1.5.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading websockets-13.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (4.9)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (3.20.2)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain_chroma) (0.6.1)\n",
            "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading streamlit-1.39.0-py2.py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.2.6-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.54.2-py3-none-any.whl (389 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.3/389.3 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.5-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.3.7-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_chroma-0.1.4-py3-none-any.whl (10 kB)\n",
            "Downloading langchain_experimental-0.3.3-py3-none-any.whl (208 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.0/209.0 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cohere-5.11.3-py3-none-any.whl (248 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.7/248.7 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Downloading chromadb-0.5.18-py3-none-any.whl (615 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.5/615.5 kB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading fastapi-0.115.4-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.15-py3-none-any.whl (408 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.7/408.7 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Downloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n",
            "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m114.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.32.0.20241016-py3-none-any.whl (15 kB)\n",
            "Downloading watchdog-5.0.3-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl (273 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.20.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m113.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.28.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.28.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.28.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.49b0-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.49b0-py3-none-any.whl (30 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.49b0-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.49b0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.2/159.2 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.28.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_util_http-0.49b0-py3-none-any.whl (6.9 kB)\n",
            "Downloading opentelemetry_sdk-1.28.0-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.7.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading starlette-0.41.2-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (425 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-13.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (164 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: chroma, pypika\n",
            "  Building wheel for chroma (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chroma: filename=Chroma-0.2.0-py3-none-any.whl size=7094 sha256=14be08176a338f0bdf2d17f791f9185f2cddcadeed6d93b9531734556270ac3d\n",
            "  Stored in directory: /root/.cache/pip/wheels/58/74/75/a6ab7999ae473ecbe819bc5cae9ccb902429dd6c60795f5112\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=83ad13149d73c69ef615ab3008779faa34b492ce098db48fa7186382c9c05d1c\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built chroma pypika\n",
            "Installing collected packages: pypika, monotonic, faiss-gpu, durationpy, chroma, websockets, watchdog, uvloop, uvicorn, types-requests, SQLAlchemy, rank_bm25, python-dotenv, pyproject_hooks, pypdf, protobuf, parameterized, overrides, opentelemetry-util-http, mypy-extensions, mmh3, marshmallow, humanfriendly, httpx-sse, httptools, fastavro, faiss-cpu, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, typing-inspect, tiktoken, starlette, pydeck, posthog, opentelemetry-proto, opentelemetry-api, coloredlogs, build, pydantic-settings, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, openai, onnxruntime, kubernetes, fastapi, dataclasses-json, bitsandbytes, opentelemetry-sdk, opentelemetry-instrumentation, langchain-core, cohere, streamlit, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, langchain_openai, opentelemetry-instrumentation-fastapi, langchain, langchain_community, chromadb, langchain_experimental, langchain_chroma\n",
            "  Attempting uninstall: SQLAlchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.36\n",
            "    Uninstalling SQLAlchemy-2.0.36:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.36\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.16.0\n",
            "    Uninstalling opentelemetry-api-1.16.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.16.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.37b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.37b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.37b0\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.52.2\n",
            "    Uninstalling openai-1.52.2:\n",
            "      Successfully uninstalled openai-1.52.2\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.16.0\n",
            "    Uninstalling opentelemetry-sdk-1.16.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.16.0\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.13\n",
            "    Uninstalling langchain-core-0.3.13:\n",
            "      Successfully uninstalled langchain-core-0.3.13\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.4\n",
            "    Uninstalling langchain-0.3.4:\n",
            "      Successfully uninstalled langchain-0.3.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-cloud-datastore 2.19.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.28.3 which is incompatible.\n",
            "google-cloud-firestore 2.16.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.28.3 which is incompatible.\n",
            "tensorboard 2.17.0 requires protobuf!=4.24.0,<5.0.0,>=3.19.6, but you have protobuf 5.28.3 which is incompatible.\n",
            "tensorflow 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.28.3 which is incompatible.\n",
            "tensorflow-metadata 1.16.1 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 5.28.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed SQLAlchemy-2.0.35 asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.0 bitsandbytes-0.44.1 build-1.2.2.post1 chroma-0.2.0 chroma-hnswlib-0.7.6 chromadb-0.5.18 cohere-5.11.3 coloredlogs-15.0.1 dataclasses-json-0.6.7 durationpy-0.9 faiss-cpu-1.9.0 faiss-gpu-1.7.2 fastapi-0.115.4 fastavro-1.9.7 httptools-0.6.4 httpx-sse-0.4.0 humanfriendly-10.0 kubernetes-31.0.0 langchain-0.3.7 langchain-core-0.3.15 langchain_chroma-0.1.4 langchain_community-0.3.5 langchain_experimental-0.3.3 langchain_openai-0.2.6 marshmallow-3.23.1 mmh3-5.0.1 monotonic-1.6 mypy-extensions-1.0.0 onnxruntime-1.20.0 openai-1.54.2 opentelemetry-api-1.28.0 opentelemetry-exporter-otlp-proto-common-1.28.0 opentelemetry-exporter-otlp-proto-grpc-1.28.0 opentelemetry-instrumentation-0.49b0 opentelemetry-instrumentation-asgi-0.49b0 opentelemetry-instrumentation-fastapi-0.49b0 opentelemetry-proto-1.28.0 opentelemetry-sdk-1.28.0 opentelemetry-semantic-conventions-0.49b0 opentelemetry-util-http-0.49b0 overrides-7.7.0 parameterized-0.9.0 posthog-3.7.0 protobuf-5.28.3 pydantic-settings-2.6.1 pydeck-0.9.1 pypdf-5.1.0 pypika-0.48.9 pyproject_hooks-1.2.0 python-dotenv-1.0.1 rank_bm25-0.2.2 starlette-0.41.2 streamlit-1.39.0 tiktoken-0.8.0 types-requests-2.32.0.20241016 typing-inspect-0.9.0 uvicorn-0.32.0 uvloop-0.21.0 watchdog-5.0.3 watchfiles-0.24.0 websockets-13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade huggingface-hub transformers"
      ],
      "metadata": {
        "id": "xTFQ81pdtOwO",
        "outputId": "90f5d804-cbba-4bd5-816e-a1e5c9192de8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (0.24.7)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
            "  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2024.8.30)\n",
            "Downloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.46.2-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.24.7\n",
            "    Uninstalling huggingface-hub-0.24.7:\n",
            "      Successfully uninstalled huggingface-hub-0.24.7\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "Successfully installed huggingface-hub-0.26.2 tokenizers-0.20.3 transformers-4.46.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import pipeline\n",
        "\n",
        "login(token = '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')"
      ],
      "metadata": {
        "id": "r-6dy_w0tOot"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hücre 1: Gerekli Kütüphaneler ve İndirme Fonksiyonları"
      ],
      "metadata": {
        "id": "2qMStg5E_T3F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXoDJDdMtJcA"
      },
      "outputs": [],
      "source": [
        "# Gerekli kütüphanelerin import edilmesi\n",
        "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema.document import Document\n",
        "from langchain.vectorstores.chroma import Chroma\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
        "import torch\n",
        "import os\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "templates = {\n",
        "    \"system\": \"You are a professional prompt engineer. Apply the mentioned prompt engineering technique and provide ONLY the improved prompt without any additional commentary or explanations.\",\n",
        "    \"system_multiple\": \"You are a professional prompt engineer. Thoroughly apply EVERY prompt engineering technique listed in the [Prompt Engineering Techniques to Apply] section. Use these techniques to enhance the original prompt provided below, ensuring the enhancement is clear and effective. Provide ONLY the improved version of the prompt without any additional commentary or explanations.\",\n",
        "    \"lang_default\": \"Identify the language of the user's original prompt in the [original] section. You MUST provide the enhanced version of the prompt in the **same language** as the user's original prompt. You'll be penalized if you translate it into another language unless explicitly requested by the user.\",\n",
        "    \"lang_eng\": \"If the original prompt is not in English, first translate it into English before proceeding with the improvement process.\",\n",
        "    \"deeper_understanding_simpler\": \"Explain to me as if I’m a beginner in System Engineering. Example: Change \\\"Explain system architecture.\\\" to \\\"Explain system architecture to beginners.\\\"\",\n",
        "    \"task_decomposition_simpler\": \"For complex or multi-step tasks, divide the original prompt into a series of simpler, more manageable sub-prompts. This approach allows the model to focus on one part of the task at a time, generating more detailed and coherent responses for each step.\",\n",
        "    \"fewshot_prompting_simpler\": \"Improve the original prompt by adding a couple of relevant examples that demonstrate the kind of answer or information being requested. Incorporate those examples smoothly into the prompt to make the desired response clear.\"\n",
        "}\n",
        "\n",
        "# Sabitler\n",
        "DATA_PATH = \"/content/drive/MyDrive/data\"  # PDF dosyalarının bulunduğu dizin\n",
        "CHROMA_PATH = \"chroma\"  # Chroma veritabanının saklanacağı dizin\n",
        "\n",
        "# Embeddings'in başlatılması\n",
        "def initialize_embeddings():\n",
        "    return HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-l6-v2\")\n",
        "\n",
        "embeddings = initialize_embeddings()\n",
        "\n",
        "def initialize_colbertv2_model():\n",
        "    \"\"\"\n",
        "    ColBERTv2 tokenizer ve modelini yükler.\n",
        "\n",
        "    Returns:\n",
        "        colbert_tokenizer: Tokenizer nesnesi\n",
        "        colbert_model: Model nesnesi\n",
        "    \"\"\"\n",
        "    # ColBERTv2 tokenizer ve modelini yükleme\n",
        "    colbert_tokenizer = AutoTokenizer.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "    colbert_model = AutoModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "\n",
        "    return colbert_tokenizer, colbert_model\n",
        "\n",
        "# Kullanım:\n",
        "colbert_tokenizer, colbert_model = initialize_colbertv2_model()\n",
        "\n",
        "# Modelin başlatılması\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "def initialize_model():\n",
        "    # 4-bit quantization için gerekli konfigürasyon\n",
        "    load_in_4bit = False\n",
        "\n",
        "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        \"kmk4444/Llama-3.2-1B-Instruct_fine_sis\",\n",
        "        load_in_4bit=load_in_4bit,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"kmk4444/Llama-3.2-1B-Instruct_fine_sis\")\n",
        "    return tokenizer, model\n",
        "\n",
        "# Modeli ve tokenizer'ı başlat\n",
        "tokenizer, model = initialize_model()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hücre 2: Yardımcı Fonksiyonlar"
      ],
      "metadata": {
        "id": "c9n0F7q6_dbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_prompt_about_engineering(prompt, tokenizer, model):\n",
        "    \"\"\"\n",
        "    Kullanıcının girdiği promptun mühendislikle ilgili olup olmadığını kontrol eder.\n",
        "    Eğer mühendislikle ilgiliyse True, değilse False döndürür.\n",
        "    \"\"\"\n",
        "    check_prompt = f\"Bu metin mühendislik alanı ile ilgili mi? Metin: '{prompt}' Cevabın sadece 'Evet' veya 'Hayır' olsun.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": check_prompt}\n",
        "    ]\n",
        "\n",
        "    # input_ids ve attention_mask alınarak giriş hazırlanır\n",
        "    encoded_input = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # GPU'ya taşıma işlemi ve input_ids'in belirlenmesi\n",
        "    if isinstance(encoded_input, dict):\n",
        "        # Eğer encoded_input bir sözlük ise\n",
        "        encoded_input = {k: v.to(model.device) for k, v in encoded_input.items()}\n",
        "        input_ids = encoded_input['input_ids']\n",
        "    else:\n",
        "        # Eğer encoded_input bir tensor ise\n",
        "        encoded_input = encoded_input.to(model.device)\n",
        "        input_ids = encoded_input\n",
        "\n",
        "    # Model çıktısı üretilir\n",
        "    outputs = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=5,\n",
        "        do_sample=False,\n",
        "        temperature=0.0,\n",
        "        top_p=0.0,\n",
        "        repetition_penalty=1.0,\n",
        "        pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Yanıtı decode etme\n",
        "    if isinstance(encoded_input, dict):\n",
        "        input_length = encoded_input['input_ids'].shape[1]\n",
        "    else:\n",
        "        input_length = encoded_input.shape[1]\n",
        "\n",
        "    response = outputs[0][input_length:]\n",
        "    response_text = tokenizer.decode(response, skip_special_tokens=True).strip()\n",
        "\n",
        "    # Cevabı kontrol etme\n",
        "    if \"Evet\" in response_text:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "\n",
        "# Rerank fonksiyonu\n",
        "def rerank_with_colbertv2(query, documents, colbert_tokenizer, colbert_model):\n",
        "    # Query'yi tokenlara ayırma\n",
        "    query_tokens = colbert_tokenizer(query, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "    # Query için embedding hesaplama\n",
        "    with torch.no_grad():\n",
        "        query_embedding = colbert_model(**query_tokens).last_hidden_state.mean(dim=1)\n",
        "\n",
        "    # Her bir belge ile query arasındaki benzerlik skorlarını hesaplama\n",
        "    scores = []\n",
        "    for doc in documents:\n",
        "        doc_tokens = colbert_tokenizer(doc.page_content, return_tensors='pt', padding=True, truncation=True)\n",
        "        doc_embedding = colbert_model(**doc_tokens).last_hidden_state.mean(dim=1)\n",
        "\n",
        "        # Query ve belge embedding'leri arasındaki cosine benzerlik skorunu hesaplama\n",
        "        score = torch.nn.functional.cosine_similarity(query_embedding, doc_embedding)\n",
        "        scores.append((doc, score.item()))\n",
        "\n",
        "    # Belgeleri benzerlik skoruna göre sıralama (azalan sırada)\n",
        "    sorted_documents = sorted(scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return [doc for doc, score in sorted_documents]\n",
        "\n",
        "# Belgeleri yükleme ve bölme\n",
        "def load_and_split_documents(data_path, embeddings):\n",
        "    document_loader = PyPDFDirectoryLoader(data_path)\n",
        "    raw_documents = document_loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=50,\n",
        "        length_function=len\n",
        "    )\n",
        "    return text_splitter.split_documents(raw_documents)\n",
        "\n",
        "# Özel belgelerin oluşturulması\n",
        "def create_custom_documents(splitted_documents):\n",
        "    custom_documents = []\n",
        "    for i, raw_doc in enumerate(splitted_documents):\n",
        "        new_doc = Document(\n",
        "                page_content=raw_doc.page_content,\n",
        "                metadata={\n",
        "                    \"source\": raw_doc.metadata.get(\"source\", \"Unknown Source\"),\n",
        "                    \"title\": raw_doc.metadata.get(\"title\", \"No Title\"),\n",
        "                    \"description\": raw_doc.metadata.get(\"description\", \"No Description\"),\n",
        "                    \"language\": raw_doc.metadata.get(\"language\", \"Unknown Language\"),\n",
        "                    \"doc_id\": i\n",
        "                }\n",
        "        )\n",
        "        custom_documents.append(new_doc)\n",
        "    return custom_documents\n",
        "\n",
        "# Vectorstore ve Retriever'ın başlatılması\n",
        "def initialize_vectorstore(custom_documents, embeddings, persist_directory):\n",
        "    try:\n",
        "        # Chroma'ya belgeleri ve embeddings'i geçirin\n",
        "        vectorstore = Chroma.from_documents(\n",
        "            custom_documents,\n",
        "            embeddings,\n",
        "            persist_directory=persist_directory\n",
        "        )\n",
        "\n",
        "        return vectorstore.as_retriever(\n",
        "            search_type=\"mmr\",\n",
        "            search_kwargs={'k': 10, 'lambda_mult': 0.40}\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing vectorstore: {e}\")\n",
        "        raise e\n",
        "\n",
        "# İlgili belgelerin getirilmesi\n",
        "def retrieve_relevant_documents(retriever, prompt):\n",
        "    return retriever.get_relevant_documents(prompt)\n",
        "\n",
        "def get_relevant_documents_with_bm25(documents, query):\n",
        "    bm25_retriever = BM25Retriever.from_documents(documents=documents)\n",
        "    bm25_retriever.k = 5\n",
        "\n",
        "    bm25_relevant_documents = bm25_retriever.get_relevant_documents(query=query)\n",
        "\n",
        "    return bm25_relevant_documents, bm25_retriever\n",
        "\n",
        "def get_relevant_documents_for_hybrid_search(query, retriever1, retriever2, weight1, weight2):\n",
        "    ensemble_retriever = EnsembleRetriever(\n",
        "                                retrievers=[retriever1, retriever2],\n",
        "                                weights=[weight1, weight2])\n",
        "\n",
        "    hybrid_relevant_documents = ensemble_retriever.get_relevant_documents(query)\n",
        "\n",
        "    return hybrid_relevant_documents\n",
        "\n",
        "# Final promptun oluşturulması\n",
        "def generate_final_prompt(prompt, context_data, chat_history, relevant_documents):\n",
        "    history_prompt = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in chat_history])\n",
        "    metadata_info = \"\\n\".join([f\"Belge {doc.metadata['doc_id']} - Başlık: {doc.metadata['title']}, Kaynak: {doc.metadata['source']}\" for doc in relevant_documents])\n",
        "    return  f\"\"\"\n",
        "###Instruction###\n",
        "You are an expert assistant dedicated to providing answers for beginner systems engineers. Follow these guidelines:\n",
        "\n",
        "1. Deliver clear, concise, and expert-level information in Turkish.\n",
        "2. Use the provided documents (###Context###) and historical conversation (###Previous Conversations###) data to answer questions (###Question###). Reference these documents while interpreting and generating your answers.\n",
        "3. Ensure that your answers are coherent and free from any repetitive or duplicate content.\n",
        "4. Each sentence should provide unique and valuable information.\n",
        "5. You have to answer just in TURKISH.\n",
        "\n",
        "###Previous Conversations###\n",
        "{history_prompt}\n",
        "\n",
        "###Question###\n",
        "{prompt}\n",
        "\n",
        "###Context###\n",
        "This is the information we have to answer the question: {context_data}\n",
        "\"\"\", metadata_info\n",
        "\n",
        "# Tekrarları kaldırmak için fonksiyon\n",
        "def remove_repetitions(response, threshold=0.7):\n",
        "    # Cümlelere bölme\n",
        "    sentences = sent_tokenize(response)\n",
        "\n",
        "    # TF-IDF vektörizer oluşturma\n",
        "    vectorizer = TfidfVectorizer().fit_transform(sentences)\n",
        "    vectors = vectorizer.toarray()\n",
        "\n",
        "    # Cosine similarity hesaplama\n",
        "    cosine_matrix = cosine_similarity(vectors)\n",
        "\n",
        "    # Tekrar eden cümleleri tespit etme ve kaldırma\n",
        "    unique_sentences = []\n",
        "    seen_indices = set()\n",
        "\n",
        "    for i in range(len(sentences)):\n",
        "        if i in seen_indices:\n",
        "            continue\n",
        "        unique_sentences.append(sentences[i])\n",
        "        for j in range(i + 1, len(sentences)):\n",
        "            if cosine_matrix[i, j] > threshold:\n",
        "                seen_indices.add(j)\n",
        "\n",
        "    # Tekrarları kaldırılmış yanıtı yeniden oluşturma\n",
        "    cleaned_response = ' '.join(unique_sentences)\n",
        "\n",
        "    return cleaned_response\n",
        "\n",
        "# RAG ile çoklu PDF'ler\n",
        "def rag_with_multiple_pdfs(prompt, chat_history):\n",
        "    splitted_documents = load_and_split_documents(DATA_PATH, embeddings)\n",
        "    custom_documents = create_custom_documents(splitted_documents)\n",
        "    chroma_retriever = initialize_vectorstore(custom_documents, embeddings, CHROMA_PATH)\n",
        "    chroma_relevant_documents = retrieve_relevant_documents(chroma_retriever, prompt)\n",
        "\n",
        "    bm25_documents, bm25retriever = get_relevant_documents_with_bm25(custom_documents,prompt)\n",
        "\n",
        "    weight1 = 0.2\n",
        "    hybrid_search_documents = get_relevant_documents_for_hybrid_search(\n",
        "        query=prompt,\n",
        "        retriever1=bm25retriever,\n",
        "        retriever2=chroma_retriever,\n",
        "        weight1=weight1,\n",
        "        weight2=1-weight1\n",
        "    )\n",
        "\n",
        "    # Hibrit arama sonuçlarını ColBERTv2 ile yeniden sıralama\n",
        "    reranked_documents = rerank_with_colbertv2(prompt, hybrid_search_documents, colbert_tokenizer, colbert_model)\n",
        "\n",
        "    context_data = \" \".join([doc.page_content for doc in reranked_documents])\n",
        "    cleaned_context_data = remove_repetitions(context_data)\n",
        "    final_prompt, metadata_info = generate_final_prompt(prompt, cleaned_context_data, chat_history, reranked_documents)\n",
        "    return final_prompt, metadata_info, chroma_relevant_documents, bm25_documents, reranked_documents\n",
        "\n",
        "def generate_prompt_engineering(prompt, tokenizer, model):\n",
        "    system_message = templates[\"system_multiple\"]\n",
        "    system_message += '\\n' + templates[\"lang_eng\"]\n",
        "\n",
        "    skills = [\"deeper_understanding\", \"task_decomposition\", \"fewshot_prompting\"]\n",
        "    integrated_templates = \"[Prompt Engineering Techniques to Apply]\\n\"\n",
        "\n",
        "    for idx, skill in enumerate(skills):\n",
        "        template = templates[f\"{skill}_simpler\"]\n",
        "        integrated_templates += f\"{idx+1}. {skill}: {template}\\n\"\n",
        "    integrated_templates += \"Based on [Prompt engineering techniques to apply], refine the prompt provided below. Ensure that each technique is fully incorporated to achieve a clear and effective improvement:\\n\\n[original]\\n{prompt}\\n[improved]\\n\"\n",
        "\n",
        "    prompt_template = PromptTemplate.from_template(integrated_templates)\n",
        "    formatted_input = prompt_template.format(prompt=prompt)\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": formatted_input}\n",
        "    ]\n",
        "\n",
        "    # input_ids ve attention_mask alınarak giriş hazırlanır\n",
        "    encoded_input = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # GPU'ya taşıma işlemi\n",
        "    encoded_input = {k: v.to(model.device) for k, v in encoded_input.items()} \\\n",
        "        if isinstance(encoded_input, dict) else encoded_input.to(model.device)\n",
        "\n",
        "    # Model çıktısı üretilir\n",
        "    outputs = model.generate(\n",
        "        input_ids=encoded_input if not isinstance(encoded_input, dict) else encoded_input['input_ids'],\n",
        "        max_new_tokens=1024,\n",
        "        do_sample=True,\n",
        "        temperature=0.70,\n",
        "        top_p=0.90,\n",
        "        repetition_penalty=1.2,\n",
        "        pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Yanıtı decode etme\n",
        "    if isinstance(encoded_input, dict):\n",
        "        input_length = encoded_input['input_ids'].shape[1]\n",
        "    else:\n",
        "        input_length = encoded_input.shape[1]\n",
        "\n",
        "    response = outputs[0][input_length:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "# LLaMA3 yanıtının üretilmesi\n",
        "def generate_llama3_response(prompt_input, tokenizer, model):\n",
        "    SYS_PROMPT = \"\"\"You are an expert assistant dedicated to guiding and empowering beginner systems engineers. Follow these guidelines:\n",
        "1. Provide clear, concise, and expert-level information in Turkish.\n",
        "2. Use the provided documents and historical conversation data to deliver insightful answers. Previous conversation history and document information will be given to you in the user's prompt.\n",
        "3. Ensure that your answers are free from any repetitive or duplicate content and sentences. You have to eliminate repetitive sentences. DON'T REPEAT THE SAME SENTENCES!\n",
        "4. If unsure, confidently state \"Bilmiyorum\" without speculating.\n",
        "5. Your responses should be motivational and tailored to empower beginners in understanding systems engineering principles.\n",
        "6. Only respond in TURKISH.\"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": prompt_input}\n",
        "    ]\n",
        "\n",
        "    # input_ids ve attention_mask alınarak giriş hazırlanır\n",
        "    encoded_input = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # GPU'ya taşıma işlemi\n",
        "    encoded_input = {k: v.to(model.device) for k, v in encoded_input.items()} \\\n",
        "        if isinstance(encoded_input, dict) else encoded_input.to(model.device)\n",
        "\n",
        "    # Model çıktısı üretilir\n",
        "    outputs = model.generate(\n",
        "        input_ids=encoded_input if not isinstance(encoded_input, dict) else encoded_input['input_ids'],\n",
        "        max_new_tokens=1024,\n",
        "        do_sample=True,\n",
        "        temperature=0.85,\n",
        "        top_p=0.90,\n",
        "        repetition_penalty=1.2,\n",
        "        pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Yanıtı decode etme\n",
        "    if isinstance(encoded_input, dict):\n",
        "        input_length = encoded_input['input_ids'].shape[1]\n",
        "    else:\n",
        "        input_length = encoded_input.shape[1]\n",
        "\n",
        "    response = outputs[0][input_length:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "def evaluate_answer_relevance(question, answer, tokenizer, model):\n",
        "    \"\"\"\n",
        "    Dil modelini kullanarak cevabın soruya uygun olup olmadığını değerlendirir.\n",
        "    \"\"\"\n",
        "    evaluation_prompt = f\"Soru: {question}\\nCevap: {answer}\\nBu cevap soruyu doğru ve tam olarak yanıtlıyor mu? Lütfen 'Evet' veya 'Hayır' şeklinde cevap ver.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": evaluation_prompt}\n",
        "    ]\n",
        "\n",
        "    # input_ids ve attention_mask alınarak giriş hazırlanır\n",
        "    encoded_input = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # GPU'ya taşıma işlemi\n",
        "    encoded_input = {k: v.to(model.device) for k, v in encoded_input.items()} \\\n",
        "        if isinstance(encoded_input, dict) else encoded_input.to(model.device)\n",
        "\n",
        "    # Model çıktısı üretilir\n",
        "    outputs = model.generate(\n",
        "        input_ids=encoded_input if not isinstance(encoded_input, dict) else encoded_input['input_ids'],\n",
        "        max_new_tokens=50,\n",
        "        do_sample=False,\n",
        "        temperature=0.0,\n",
        "        top_p=0.0,\n",
        "        repetition_penalty=1.0,\n",
        "        pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Yanıtı decode etme\n",
        "    if isinstance(encoded_input, dict):\n",
        "        input_length = encoded_input['input_ids'].shape[1]\n",
        "    else:\n",
        "        input_length = encoded_input.shape[1]\n",
        "\n",
        "    evaluation_response = outputs[0][input_length:]\n",
        "    evaluation_text = tokenizer.decode(evaluation_response, skip_special_tokens=True).strip()\n",
        "\n",
        "    # Cevabı değerlendirme\n",
        "    if \"Evet\" in evaluation_text:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "def select_best_response(question, responses, tokenizer, model):\n",
        "    \"\"\"\n",
        "    Dil modelini kullanarak verilen cevaplar arasından en iyi olanı seçer.\n",
        "\n",
        "    Args:\n",
        "        question (str): Kullanıcının sorusu.\n",
        "        responses (list): Cevapların listesi.\n",
        "        tokenizer: Tokenizer nesnesi.\n",
        "        model: Dil modeli nesnesi.\n",
        "\n",
        "    Returns:\n",
        "        str: En iyi cevap.\n",
        "    \"\"\"\n",
        "    # Cevapları numaralandır\n",
        "    numbered_responses = \"\\n\".join([f\"{idx+1}. {resp}\" for idx, resp in enumerate(responses)])\n",
        "\n",
        "    selection_prompt = f\"Kullanıcının sorusu: {question}\\n\\nVerilen cevaplar:\\n{numbered_responses}\\n\\nBu cevaplar arasından soruya en iyi yanıt veren hangisidir? Lütfen sadece cevabın numarasını yaz.\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": selection_prompt}\n",
        "    ]\n",
        "\n",
        "    # input_ids ve attention_mask alınarak giriş hazırlanır\n",
        "    encoded_input = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # GPU'ya taşıma işlemi\n",
        "    encoded_input = {k: v.to(model.device) for k, v in encoded_input.items()} \\\n",
        "        if isinstance(encoded_input, dict) else encoded_input.to(model.device)\n",
        "\n",
        "    # Model çıktısı üretilir\n",
        "    outputs = model.generate(\n",
        "        input_ids=encoded_input if not isinstance(encoded_input, dict) else encoded_input['input_ids'],\n",
        "        max_new_tokens=10,\n",
        "        do_sample=False,\n",
        "        temperature=0.0,\n",
        "        top_p=0.0,\n",
        "        repetition_penalty=1.0,\n",
        "        pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Yanıtı decode etme\n",
        "    if isinstance(encoded_input, dict):\n",
        "        input_length = encoded_input['input_ids'].shape[1]\n",
        "    else:\n",
        "        input_length = encoded_input.shape[1]\n",
        "\n",
        "    selection_response = outputs[0][input_length:]\n",
        "    selection_text = tokenizer.decode(selection_response, skip_special_tokens=True).strip()\n",
        "\n",
        "    # Seçilen numarayı al\n",
        "    try:\n",
        "        selected_index = int(selection_text.split('.')[0].strip()) - 1\n",
        "        if 0 <= selected_index < len(responses):\n",
        "            return responses[selected_index]\n",
        "        else:\n",
        "            # Geçersiz seçim durumunda ilk cevabı döndür\n",
        "            return responses[0]\n",
        "    except ValueError:\n",
        "        # Anlaşılamayan yanıt durumunda ilk cevabı döndür\n",
        "        return responses[0]\n"
      ],
      "metadata": {
        "id": "uC9NIW8q_ZDh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hücre 3: Ana Kod Çalıştırma"
      ],
      "metadata": {
        "id": "enICwb7K_jXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Modeli ve tokenizer'ı başlat\n",
        "    # tokenizer, model = initialize_model()\n",
        "\n",
        "    # Sohbet geçmişini başlat\n",
        "    chat_history = []\n",
        "\n",
        "    while True:\n",
        "        # Kullanıcı girdisini al\n",
        "        prompt = input(\"Mesajınızı Giriniz: \")\n",
        "\n",
        "        # Kullanıcının girdiği promptu kontrol et\n",
        "        if not is_prompt_about_engineering(prompt, tokenizer, model):\n",
        "            print(\"Girdiğiniz soru sistem mühendisliği alanı ile ilgili değildir. Lütfen sistem mühendisliği ile ilgili bir soru sorunuz.\\n\")\n",
        "            continue  # Kullanıcıdan tekrar girdi al\n",
        "        else:\n",
        "            break  # Döngüden çık ve devam et\n",
        "\n",
        "    # Kullanıcı mesajını sohbet geçmişine ekle\n",
        "    chat_history.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    # Optimize edilmiş promptu oluştur\n",
        "    optimized_prompt = generate_prompt_engineering(prompt, tokenizer, model)\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"OPTİMİZE EDİLMİŞ PROMPT:\")\n",
        "    print(\"=\"*50)\n",
        "    print(optimized_prompt)\n",
        "\n",
        "    # Final promptu oluştur ve belgeleri getir\n",
        "    final_prompt, metadata_info, chroma_relevant_documents, bm25_documents, hybrid_search_documents = rag_with_multiple_pdfs(optimized_prompt, chat_history)\n",
        "\n",
        "    # BM25 belgelerini yazdır\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"KARAKTER BAZLI ARAMA | BM25:\")\n",
        "    print(\"=\"*50)\n",
        "    for doc in bm25_documents:\n",
        "        print(f\"ID: {doc.metadata['doc_id']} || {doc.page_content}\\n\")\n",
        "\n",
        "    # Hibrit Arama belgelerini yazdır\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"HİBRİT ARAMA:\")\n",
        "    print(\"=\"*50)\n",
        "    for doc in hybrid_search_documents:\n",
        "        print(f\"ID: {doc.metadata['doc_id']} || {doc.page_content}\\n\")\n",
        "\n",
        "    # Semantik Arama belgelerini yazdır (Chroma'dan)\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"SEMANTİK ARAMA:\")\n",
        "    print(\"=\"*50)\n",
        "    for doc in chroma_relevant_documents:\n",
        "        print(f\"ID: {doc.metadata['doc_id']} || {doc.page_content}\\n\")\n",
        "\n",
        "    max_attempts = 3  # Maksimum deneme sayısı\n",
        "    attempts = 0\n",
        "    is_relevant = False\n",
        "    previous_responses = []  # Önceki cevapları tutmak için liste\n",
        "\n",
        "    while attempts < max_attempts and not is_relevant:\n",
        "        # Yanıtı üret\n",
        "        if attempts == 0:\n",
        "            response = generate_llama3_response(final_prompt, tokenizer, model)\n",
        "        else:\n",
        "            # Geri bildirimle yeni prompt oluştur\n",
        "            feedback = f\"Cevabın kullanıcının sorusuna tam olarak yanıt vermedi. Önceki cevabın: '{previous_responses[-1]}'. Lütfen kullanıcının sorusuna odaklanarak daha uygun bir cevap ver.\"\n",
        "            final_prompt_with_feedback = f\"{feedback}\\n\\n{final_prompt}\"\n",
        "            response = generate_llama3_response(final_prompt_with_feedback, tokenizer, model)\n",
        "\n",
        "        # Cevabın uygunluğunu kontrol et\n",
        "        is_relevant = evaluate_answer_relevance(prompt, response, tokenizer, model)\n",
        "\n",
        "        # Önceki cevapları listeye ekle\n",
        "        previous_responses.append(response)\n",
        "        attempts += 1\n",
        "\n",
        "    if not is_relevant:\n",
        "        # Üç cevap arasından en iyisini seç\n",
        "        best_response = select_best_response(prompt, previous_responses, tokenizer, model)\n",
        "        response = best_response\n",
        "\n",
        "    # Uygun cevap sohbet geçmişine eklenir\n",
        "    chat_history.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ASİSTAN YANITI:\")\n",
        "    print(\"=\"*50)\n",
        "    print(response)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "ACv14X5D_hgl",
        "outputId": "33305c5f-f767-4410-96f0-8ad0362ce7d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mesajınızı Giriniz: sistem mühendisliği nedir?\n",
            "Girdiğiniz soru sistem mühendisliği alanı ile ilgili değildir. Lütfen sistem mühendisliği ile ilgili bir soru sorunuz.\n",
            "\n",
            "Mesajınızı Giriniz: what is system engineering?\n",
            "Girdiğiniz soru sistem mühendisliği alanı ile ilgili değildir. Lütfen sistem mühendisliği ile ilgili bir soru sorunuz.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_29rKdYptQqo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}