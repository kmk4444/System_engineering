{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "mount_file_id": "https://github.com/kmk4444/System_engineering/blob/main/System_eng_Part4.ipynb",
      "authorship_tag": "ABX9TyNFkSXJNbrqPeqJVS+i9JQP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmk4444/System_engineering/blob/main/System_eng_Part4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain openai pypdf chroma streamlit langchain_openai langchain_community langchain transformers bitsandbytes accelerate torch faiss-gpu faiss-cpu langchain_chroma langchain_experimental"
      ],
      "metadata": {
        "id": "pRhoGD70xtE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence-transformers"
      ],
      "metadata": {
        "id": "R9vTyjDdt79O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install cohere"
      ],
      "metadata": {
        "id": "w74Apoui5vaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import pipeline\n",
        "\n",
        "login(token = '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')"
      ],
      "metadata": {
        "id": "9khH96Vv2qLW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aee57431-26da-47fb-cd1a-15da03b8c521"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app_v3.py\n",
        "import streamlit as st\n",
        "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema.document import Document\n",
        "from langchain.vectorstores.chroma import Chroma\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import os\n",
        "import shutil\n",
        "from langchain_core.documents import Document\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "from langchain_community.embeddings import CohereEmbeddings\n",
        "\n",
        "my_key_cohere=\"RsdlzOlRrQmvf5bxQrUHDpfbeg7Fvfr7WJwGCec1\"\n",
        "\n",
        "# Set up Streamlit page configuration\n",
        "st.set_page_config(page_title=\"ðŸ¦™ðŸ’¬ Sistem MÃ¼hendisliÄŸi GiriÅŸ Chatbot\")\n",
        "\n",
        "# Constants\n",
        "DATA_PATH = \"/content/drive/MyDrive/data\"  # PDF dosyalarÄ±nÄ±n bulunduÄŸu dizin\n",
        "CHROMA_PATH = \"chroma\"  # Chroma veritabanÄ±nÄ±n saklanacaÄŸÄ± dizin\n",
        "\n",
        "# Initialize Embeddings\n",
        "def initialize_embeddings():\n",
        "    embeddings = CohereEmbeddings(cohere_api_key=my_key_cohere,model=\"embed-multilingual-v3.0\")\n",
        "    return embeddings\n",
        "\n",
        "embeddings = initialize_embeddings()\n",
        "\n",
        "# Load and Split Documents\n",
        "def load_and_split_documents(data_path, embeddings):\n",
        "    document_loader = PyPDFDirectoryLoader(data_path)\n",
        "    raw_documents = document_loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=800,\n",
        "        chunk_overlap=80,\n",
        "        length_function=len\n",
        "    )\n",
        "    return text_splitter.split_documents(raw_documents)\n",
        "\n",
        "# Create Custom Documents\n",
        "def create_custom_documents(splitted_documents):\n",
        "    custom_documents = []\n",
        "    for i, raw_doc in enumerate(splitted_documents):\n",
        "        new_doc = Document(\n",
        "                page_content=raw_doc.page_content,\n",
        "                metadata = {\n",
        "                    \"source\": raw_doc.metadata.get(\"source\", \"Unknown Source\"),\n",
        "                    \"title\": raw_doc.metadata.get(\"title\", \"No Title\"),\n",
        "                    \"description\": raw_doc.metadata.get(\"description\", \"No Description\"),\n",
        "                    \"language\": raw_doc.metadata.get(\"language\", \"Unknown Language\"),\n",
        "                    \"doc_id\": i\n",
        "                }\n",
        "        )\n",
        "        custom_documents.append(new_doc)\n",
        "    return custom_documents\n",
        "\n",
        "# Initialize Vectorstore and Retriever\n",
        "def initialize_vectorstore(custom_documents, embeddings, persist_directory):\n",
        "    try:\n",
        "        # Ã–nce belgeleri oluÅŸturun\n",
        "        custom_documents = create_custom_documents(custom_documents)\n",
        "\n",
        "        # CohereEmbeddings boyutunu belirleyin (Ã¶r. 1024 varsayalÄ±m)\n",
        "        embeddings_dimension = 384\n",
        "\n",
        "        # Chroma'ya belgeleri ve embeddings'i geÃ§irin\n",
        "        vectorstore = Chroma.from_documents(\n",
        "            custom_documents,\n",
        "            embeddings,\n",
        "            persist_directory=persist_directory,\n",
        "            embeddings_dimension=embeddings_dimension\n",
        "        )\n",
        "\n",
        "        return vectorstore.as_retriever(\n",
        "            search_type=\"mmr\",\n",
        "            search_kwargs={'k': 4, 'lambda_mult': 0.50}\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing vectorstore: {e}\")\n",
        "        raise e\n",
        "\n",
        "\n",
        "# Retrieve Relevant Documents\n",
        "def retrieve_relevant_documents(retriever, prompt):\n",
        "    return retriever.get_relevant_documents(prompt)\n",
        "\n",
        "# Generate Final Prompt\n",
        "def generate_final_prompt(prompt, context_data, chat_history, relevant_documents):\n",
        "    history_prompt = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in chat_history])\n",
        "    metadata_info = \"\\n\".join([f\"Belge {doc.metadata['doc_id']} - BaÅŸlÄ±k: {doc.metadata['title']}, Kaynak: {doc.metadata['source']}\" for doc in relevant_documents])\n",
        "    return f\"\"\"\n",
        "    Ã–nceki konuÅŸmalarÄ±mÄ±z:\n",
        "    {history_prompt}\n",
        "\n",
        "    ÅžÃ¶yle bir sorum var: {prompt}\n",
        "    Bu soruyu yanÄ±tlamak iÃ§in elimizde ÅŸu bilgiler var: {context_data}\n",
        "    Bu sorunun yanÄ±tÄ±nÄ± vermek iÃ§in yalnÄ±zca sana burada verdiÄŸim eldeki bilgileri kullan. BunlarÄ±n dÄ±ÅŸÄ±na asla Ã§Ä±kma. Sorulara TÃ¼rkÃ§e cevap vereceksin.\n",
        "    \"\"\", metadata_info\n",
        "\n",
        "# RAG with Multiple PDFs\n",
        "def rag_with_multiple_pdfs(prompt, chat_history):\n",
        "    splitted_documents = load_and_split_documents(DATA_PATH, embeddings)\n",
        "    custom_documents = create_custom_documents(splitted_documents)\n",
        "    retriever = initialize_vectorstore(custom_documents, embeddings, CHROMA_PATH)\n",
        "    relevant_documents = retrieve_relevant_documents(retriever, prompt)\n",
        "\n",
        "    context_data = \" \".join([doc.page_content for doc in relevant_documents])\n",
        "    final_prompt, metadata_info = generate_final_prompt(prompt, context_data, chat_history, relevant_documents)\n",
        "    return final_prompt, metadata_info\n",
        "\n",
        "# Initialize Model\n",
        "def initialize_model():\n",
        "    model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb_config\n",
        "    )\n",
        "    return tokenizer, model\n",
        "\n",
        "tokenizer, model = initialize_model()\n",
        "\n",
        "# Generate LLaMA3 Response\n",
        "def generate_llama3_response(prompt_input, tokenizer, model):\n",
        "    SYS_PROMPT = \"\"\"You are an assistant for answering questions.\n",
        "    You are given the extracted parts of a long document and a question. Provide a conversational answer.\n",
        "    If you don't know the answer, just say \"I do not know.\" Don't make up an answer. You have to answer Turkish.\"\"\"\n",
        "\n",
        "    messages = [{\"role\":\"system\",\"content\":SYS_PROMPT},{\"role\":\"user\",\"content\":prompt_input}]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=1024,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "# Streamlit App Setup\n",
        "def setup_streamlit():\n",
        "    with st.sidebar:\n",
        "        st.title('ðŸ¦™ðŸ’¬ Sistem MÃ¼hendisliÄŸi GiriÅŸ')\n",
        "        st.subheader('Models and parameters')\n",
        "        temperature = st.sidebar.slider('temperature', min_value=0.01, max_value=5.0, value=0.1, step=0.01)\n",
        "        top_p = st.sidebar.slider('top_p', min_value=0.01, max_value=1.0, value=0.9, step=0.01)\n",
        "        max_length = st.sidebar.slider('max_length', min_value=32, max_value=128, value=120, step=8)\n",
        "        st.markdown('ðŸ“– Learn how to build this app in this [blog](https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/)!')\n",
        "\n",
        "    if \"messages\" not in st.session_state.keys():\n",
        "        st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.write(message[\"content\"])\n",
        "\n",
        "def clear_chat_history():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "st.sidebar.button('Clear Chat History', on_click=clear_chat_history)\n",
        "\n",
        "# Main Function\n",
        "def main():\n",
        "    setup_streamlit()\n",
        "\n",
        "    if prompt := st.chat_input(\"MesajÄ±nÄ±zÄ± Giriniz:\"):\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.write(prompt)\n",
        "\n",
        "        if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
        "            with st.chat_message(\"assistant\"):\n",
        "                with st.spinner(\"Thinking...\"):\n",
        "                    try:\n",
        "                        prompt_input, metadata_info = rag_with_multiple_pdfs(prompt, st.session_state.messages)\n",
        "                        response = generate_llama3_response(prompt_input, tokenizer, model)\n",
        "                        placeholder = st.empty()\n",
        "                        full_response = response\n",
        "                        placeholder.markdown(full_response)\n",
        "                        message = {\"role\": \"assistant\", \"content\": f\"{full_response}\\n\\nMetadata Bilgileri:\\n{metadata_info}\"}\n",
        "                        st.session_state.messages.append(message)\n",
        "                    except Exception as e:\n",
        "                        st.error(f\"An error occurred: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "fy5JU_MNsJef",
        "outputId": "5196b3a1-7a4b-4fd1-eb42-8b52d34e53ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app_v4.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!streamlit run /content/app_v3.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "5nrpXTICsMC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app_v4.py\n",
        "import streamlit as st\n",
        "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema.document import Document\n",
        "from langchain.vectorstores.chroma import Chroma\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import os\n",
        "import shutil\n",
        "from langchain_core.documents import Document\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "from langchain_community.embeddings import CohereEmbeddings\n",
        "\n",
        "my_key_cohere=\"RsdlzOlRrQmvf5bxQrUHDpfbeg7Fvfr7WJwGCec1\"\n",
        "\n",
        "# Set up Streamlit page configuration\n",
        "st.set_page_config(page_title=\"ðŸ¦™ðŸ’¬ Sistem MÃ¼hendisliÄŸi GiriÅŸ Chatbot\")\n",
        "\n",
        "# Constants\n",
        "DATA_PATH = \"/content/drive/MyDrive/data\"  # PDF dosyalarÄ±nÄ±n bulunduÄŸu dizin\n",
        "CHROMA_PATH = \"chroma\"  # Chroma veritabanÄ±nÄ±n saklanacaÄŸÄ± dizin\n",
        "\n",
        "# Initialize Embeddings\n",
        "def initialize_embeddings():\n",
        "    embeddings = CohereEmbeddings(cohere_api_key=my_key_cohere, model=\"embed-multilingual-v3.0\")\n",
        "    return embeddings\n",
        "\n",
        "embeddings = initialize_embeddings()\n",
        "\n",
        "# Load and Split Documents\n",
        "def load_and_split_documents(data_path, embeddings):\n",
        "    document_loader = PyPDFDirectoryLoader(data_path)\n",
        "    raw_documents = document_loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=800,\n",
        "        chunk_overlap=80,\n",
        "        length_function=len\n",
        "    )\n",
        "    return text_splitter.split_documents(raw_documents)\n",
        "\n",
        "# Create Custom Documents\n",
        "def create_custom_documents(splitted_documents):\n",
        "    custom_documents = []\n",
        "    for i, raw_doc in enumerate(splitted_documents):\n",
        "        new_doc = Document(\n",
        "                page_content=raw_doc.page_content,\n",
        "                metadata={\n",
        "                    \"source\": raw_doc.metadata.get(\"source\", \"Unknown Source\"),\n",
        "                    \"title\": raw_doc.metadata.get(\"title\", \"No Title\"),\n",
        "                    \"description\": raw_doc.metadata.get(\"description\", \"No Description\"),\n",
        "                    \"language\": raw_doc.metadata.get(\"language\", \"Unknown Language\"),\n",
        "                    \"doc_id\": i\n",
        "                }\n",
        "        )\n",
        "        custom_documents.append(new_doc)\n",
        "    return custom_documents\n",
        "\n",
        "# Initialize Vectorstore and Retriever\n",
        "def initialize_vectorstore(custom_documents, embeddings, persist_directory):\n",
        "    try:\n",
        "        # Chroma'ya belgeleri ve embeddings'i geÃ§irin\n",
        "        vectorstore = Chroma.from_documents(\n",
        "            custom_documents,\n",
        "            embeddings,\n",
        "            persist_directory=persist_directory\n",
        "        )\n",
        "\n",
        "        return vectorstore.as_retriever(\n",
        "            search_type=\"mmr\",\n",
        "            search_kwargs={'k': 4, 'lambda_mult': 0.50}\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing vectorstore: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Retrieve Relevant Documents\n",
        "def retrieve_relevant_documents(retriever, prompt):\n",
        "    return retriever.get_relevant_documents(prompt)\n",
        "\n",
        "# Generate Final Prompt\n",
        "def generate_final_prompt(prompt, context_data, chat_history, relevant_documents):\n",
        "    history_prompt = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in chat_history])\n",
        "    metadata_info = \"\\n\".join([f\"Belge {doc.metadata['doc_id']} - BaÅŸlÄ±k: {doc.metadata['title']}, Kaynak: {doc.metadata['source']}\" for doc in relevant_documents])\n",
        "    return f\"\"\"\n",
        "    Ã–nceki konuÅŸmalarÄ±mÄ±z:\n",
        "    {history_prompt}\n",
        "\n",
        "    ÅžÃ¶yle bir sorum var: {prompt}\n",
        "    Bu soruyu yanÄ±tlamak iÃ§in elimizde ÅŸu bilgiler var: {context_data}\n",
        "    Bu sorunun yanÄ±tÄ±nÄ± vermek iÃ§in yalnÄ±zca sana burada verdiÄŸim eldeki bilgileri kullan. BunlarÄ±n dÄ±ÅŸÄ±na asla Ã§Ä±kma. Sorulara TÃ¼rkÃ§e cevap vereceksin.\n",
        "    \"\"\", metadata_info\n",
        "\n",
        "# RAG with Multiple PDFs\n",
        "def rag_with_multiple_pdfs(prompt, chat_history):\n",
        "    splitted_documents = load_and_split_documents(DATA_PATH, embeddings)\n",
        "    custom_documents = create_custom_documents(splitted_documents)\n",
        "    retriever = initialize_vectorstore(custom_documents, embeddings, CHROMA_PATH)\n",
        "    relevant_documents = retrieve_relevant_documents(retriever, prompt)\n",
        "\n",
        "    context_data = \" \".join([doc.page_content for doc in relevant_documents])\n",
        "    final_prompt, metadata_info = generate_final_prompt(prompt, context_data, chat_history, relevant_documents)\n",
        "    return final_prompt, metadata_info\n",
        "\n",
        "# Initialize Model\n",
        "def initialize_model():\n",
        "    model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb_config\n",
        "    )\n",
        "    return tokenizer, model\n",
        "\n",
        "tokenizer, model = initialize_model()\n",
        "\n",
        "# Generate LLaMA3 Response\n",
        "def generate_llama3_response(prompt_input, tokenizer, model):\n",
        "    SYS_PROMPT = \"\"\"You are an assistant for answering questions.\n",
        "    You are given the extracted parts of a long document and a question. Provide a conversational answer.\n",
        "    If you don't know the answer, just say \"I do not know.\" Don't make up an answer. You have to answer Turkish.\"\"\"\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": SYS_PROMPT}, {\"role\": \"user\", \"content\": prompt_input}]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=1024,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "# Streamlit App Setup\n",
        "def setup_streamlit():\n",
        "    with st.sidebar:\n",
        "        st.title('ðŸ¦™ðŸ’¬ Sistem MÃ¼hendisliÄŸi GiriÅŸ')\n",
        "        st.subheader('Models and parameters')\n",
        "        temperature = st.sidebar.slider('temperature', min_value=0.01, max_value=5.0, value=0.1, step=0.01)\n",
        "        top_p = st.sidebar.slider('top_p', min_value=0.01, max_value=1.0, value=0.9, step=0.01)\n",
        "        max_length = st.sidebar.slider('max_length', min_value=32, max_value=128, value=120, step=8)\n",
        "        st.markdown('ðŸ“– Learn how to build this app in this [blog](https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/)!')\n",
        "\n",
        "    if \"messages\" not in st.session_state.keys():\n",
        "        st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.write(message[\"content\"])\n",
        "\n",
        "def clear_chat_history():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "st.sidebar.button('Clear Chat History', on_click=clear_chat_history)\n",
        "\n",
        "# Main Function\n",
        "def main():\n",
        "    setup_streamlit()\n",
        "\n",
        "    if prompt := st.chat_input(\"MesajÄ±nÄ±zÄ± Giriniz:\"):\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.write(prompt)\n",
        "\n",
        "        if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
        "            with st.chat_message(\"assistant\"):\n",
        "                with st.spinner(\"Thinking...\"):\n",
        "                    try:\n",
        "                        prompt_input, metadata_info = rag_with_multiple_pdfs(prompt, st.session_state.messages)\n",
        "                        response = generate_llama3_response(prompt_input, tokenizer, model)\n",
        "                        placeholder = st.empty()\n",
        "                        full_response = response\n",
        "                        placeholder.markdown(full_response)\n",
        "                        message = {\"role\": \"assistant\", \"content\": f\"{full_response}\\n\\nMetadata Bilgileri:\\n{metadata_info}\"}\n",
        "                        st.session_state.messages.append(message)\n",
        "                    except Exception as e:\n",
        "                        st.error(f\"An error occurred: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfFT88sGEWP8",
        "outputId": "cf6910d2-85e3-4f58-ed08-032e0c80136d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app_v4.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!streamlit run /content/app_v4.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "qGiR46jY2Re_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app_v5.py\n",
        "import streamlit as st\n",
        "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema.document import Document\n",
        "from langchain.vectorstores.chroma import Chroma\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import os\n",
        "import shutil\n",
        "from langchain_core.documents import Document\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "from langchain_community.embeddings import CohereEmbeddings\n",
        "\n",
        "my_key_cohere=\"RsdlzOlRrQmvf5bxQrUHDpfbeg7Fvfr7WJwGCec1\"\n",
        "\n",
        "# Set up Streamlit page configuration\n",
        "st.set_page_config(page_title=\"ðŸ¦™ðŸ’¬ Sistem MÃ¼hendisliÄŸi GiriÅŸ Chatbot\")\n",
        "\n",
        "# Constants\n",
        "DATA_PATH = \"/content/drive/MyDrive/data\"  # PDF dosyalarÄ±nÄ±n bulunduÄŸu dizin\n",
        "CHROMA_PATH = \"chroma\"  # Chroma veritabanÄ±nÄ±n saklanacaÄŸÄ± dizin\n",
        "\n",
        "# Initialize Embeddings\n",
        "def initialize_embeddings():\n",
        "    embeddings = CohereEmbeddings(cohere_api_key=my_key_cohere, model=\"embed-multilingual-v3.0\")\n",
        "    return embeddings\n",
        "\n",
        "embeddings = initialize_embeddings()\n",
        "\n",
        "# Load and Split Documents\n",
        "def load_and_split_documents(data_path, embeddings):\n",
        "    document_loader = PyPDFDirectoryLoader(data_path)\n",
        "    raw_documents = document_loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=800,\n",
        "        chunk_overlap=80,\n",
        "        length_function=len\n",
        "    )\n",
        "    return text_splitter.split_documents(raw_documents)\n",
        "\n",
        "# Create Custom Documents\n",
        "def create_custom_documents(splitted_documents):\n",
        "    custom_documents = []\n",
        "    for i, raw_doc in enumerate(splitted_documents):\n",
        "        new_doc = Document(\n",
        "                page_content=raw_doc.page_content,\n",
        "                metadata={\n",
        "                    \"source\": raw_doc.metadata.get(\"source\", \"Unknown Source\"),\n",
        "                    \"title\": raw_doc.metadata.get(\"title\", \"No Title\"),\n",
        "                    \"description\": raw_doc.metadata.get(\"description\", \"No Description\"),\n",
        "                    \"language\": raw_doc.metadata.get(\"language\", \"Unknown Language\"),\n",
        "                    \"doc_id\": i\n",
        "                }\n",
        "        )\n",
        "        custom_documents.append(new_doc)\n",
        "    return custom_documents\n",
        "\n",
        "# Initialize Vectorstore and Retriever\n",
        "def initialize_vectorstore(custom_documents, embeddings, persist_directory):\n",
        "    try:\n",
        "        # Chroma'ya belgeleri ve embeddings'i geÃ§irin\n",
        "        vectorstore = Chroma.from_documents(\n",
        "            custom_documents,\n",
        "            embeddings,\n",
        "            persist_directory=persist_directory\n",
        "        )\n",
        "\n",
        "        return vectorstore.as_retriever(\n",
        "            search_type=\"mmr\",\n",
        "            search_kwargs={'k': 4, 'lambda_mult': 0.50}\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing vectorstore: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Retrieve Relevant Documents\n",
        "def retrieve_relevant_documents(retriever, prompt):\n",
        "    return retriever.get_relevant_documents(prompt)\n",
        "\n",
        "# Generate Final Prompt\n",
        "def generate_final_prompt(prompt, context_data, chat_history, relevant_documents):\n",
        "    history_prompt = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in chat_history])\n",
        "    metadata_info = \"\\n\".join([f\"Belge {doc.metadata['doc_id']} - BaÅŸlÄ±k: {doc.metadata['title']}, Kaynak: {doc.metadata['source']}\" for doc in relevant_documents])\n",
        "    return f\"\"\"\n",
        "    Ã–nceki konuÅŸmalarÄ±mÄ±z:\n",
        "    {history_prompt}\n",
        "\n",
        "    ÅžÃ¶yle bir sorum var: {prompt}\n",
        "    Bu soruyu yanÄ±tlamak iÃ§in elimizde ÅŸu bilgiler var: {context_data}\n",
        "    Bu sorunun yanÄ±tÄ±nÄ± vermek iÃ§in yalnÄ±zca sana burada verdiÄŸim eldeki bilgileri kullan. BunlarÄ±n dÄ±ÅŸÄ±na asla Ã§Ä±kma. Sorulara TÃ¼rkÃ§e cevap vereceksin.\n",
        "    \"\"\", metadata_info\n",
        "\n",
        "# RAG with Multiple PDFs\n",
        "def rag_with_multiple_pdfs(prompt, chat_history):\n",
        "    splitted_documents = load_and_split_documents(DATA_PATH, embeddings)\n",
        "    custom_documents = create_custom_documents(splitted_documents)\n",
        "    retriever = initialize_vectorstore(custom_documents, embeddings, CHROMA_PATH)\n",
        "    relevant_documents = retrieve_relevant_documents(retriever, prompt)\n",
        "\n",
        "    context_data = \" \".join([doc.page_content for doc in relevant_documents])\n",
        "    final_prompt, metadata_info = generate_final_prompt(prompt, context_data, chat_history, relevant_documents)\n",
        "    return final_prompt, metadata_info\n",
        "\n",
        "# Initialize Model\n",
        "def initialize_model():\n",
        "    model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb_config\n",
        "    )\n",
        "    return tokenizer, model\n",
        "\n",
        "tokenizer, model = initialize_model()\n",
        "\n",
        "# Generate LLaMA3 Response\n",
        "def generate_llama3_response(prompt_input, tokenizer, model):\n",
        "    SYS_PROMPT = \"\"\"You are an assistant for answering questions.\n",
        "    You are given the extracted parts of a long document and a question. Provide a conversational answer.\n",
        "    If you don't know the answer, just say \"I do not know.\" Don't make up an answer. You have to answer Turkish.\"\"\"\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": SYS_PROMPT}, {\"role\": \"user\", \"content\": prompt_input}]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=1024,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "# Streamlit App Setup\n",
        "def setup_streamlit():\n",
        "    with st.sidebar:\n",
        "        st.title('ðŸ¦™ðŸ’¬ Sistem MÃ¼hendisliÄŸi GiriÅŸ')\n",
        "        st.subheader('Models and parameters')\n",
        "        temperature = st.sidebar.slider('temperature', min_value=0.01, max_value=5.0, value=0.1, step=0.01)\n",
        "        top_p = st.sidebar.slider('top_p', min_value=0.01, max_value=1.0, value=0.9, step=0.01)\n",
        "        max_length = st.sidebar.slider('max_length', min_value=32, max_value=128, value=120, step=8)\n",
        "        st.markdown('ðŸ“– Learn how to build this app in this [blog](https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/)!')\n",
        "\n",
        "    if \"messages\" not in st.session_state.keys():\n",
        "        st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.write(message[\"content\"])\n",
        "\n",
        "def clear_chat_history():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "st.sidebar.button('Clear Chat History', on_click=clear_chat_history)\n",
        "\n",
        "# Main Function\n",
        "def main():\n",
        "    setup_streamlit()\n",
        "\n",
        "    if prompt := st.chat_input(\"MesajÄ±nÄ±zÄ± Giriniz:\"):\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.write(prompt)\n",
        "\n",
        "        if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
        "            with st.chat_message(\"assistant\"):\n",
        "                with st.spinner(\"Thinking...\"):\n",
        "                    try:\n",
        "                        prompt_input, metadata_info = rag_with_multiple_pdfs(prompt, st.session_state.messages)\n",
        "                        response = generate_llama3_response(prompt_input, tokenizer, model)\n",
        "                        placeholder = st.empty()\n",
        "                        full_response = response\n",
        "                        placeholder.markdown(full_response)\n",
        "                        message = {\"role\": \"assistant\", \"content\": f\"{full_response}\\n\\nMetadata Bilgileri:\\n{metadata_info}\"}\n",
        "                        st.session_state.messages.append(message)\n",
        "                    except Exception as e:\n",
        "                        st.error(f\"An error occurred: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "cCxJ3dUyS0e7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!streamlit run /content/app_v5.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "sNydXxQ2S8Wz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}