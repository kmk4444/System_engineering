{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "mount_file_id": "https://github.com/kmk4444/System_engineering/blob/main/System_eng_Part4.ipynb",
      "authorship_tag": "ABX9TyP8qtxzVO73WEKa2B3fIyxk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmk4444/System_engineering/blob/main/System_eng_Part4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain openai pypdf chroma streamlit langchain_openai langchain_community langchain transformers bitsandbytes accelerate torch faiss-gpu faiss-cpu langchain_chroma langchain_experimental"
      ],
      "metadata": {
        "id": "pRhoGD70xtE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence-transformers"
      ],
      "metadata": {
        "id": "R9vTyjDdt79O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install cohere"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w74Apoui5vaa",
        "outputId": "2231a44a-bc06-45be-e82c-bc5b80f833ec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cohere\n",
            "  Downloading cohere-5.5.8-py3-none-any.whl (173 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m173.8/173.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting boto3<2.0.0,>=1.34.0 (from cohere)\n",
            "  Downloading boto3-1.34.132-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
            "  Downloading fastavro-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.27.0)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from cohere)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Collecting parameterized<0.10.0,>=0.9.0 (from cohere)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.7.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.19.1)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
            "  Downloading types_requests-2.32.0.20240622-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (4.12.2)\n",
            "Collecting botocore<1.35.0,>=1.34.132 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading botocore-1.34.132-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->cohere) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->cohere) (2.18.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (2.0.7)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<1,>=0.15->cohere) (0.23.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.132->boto3<2.0.0,>=1.34.0->cohere) (2.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.15.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (6.0.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (4.66.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.132->boto3<2.0.0,>=1.34.0->cohere) (1.16.0)\n",
            "Installing collected packages: types-requests, parameterized, jmespath, httpx-sse, fastavro, botocore, s3transfer, boto3, cohere\n",
            "Successfully installed boto3-1.34.132 botocore-1.34.132 cohere-5.5.8 fastavro-1.9.4 httpx-sse-0.4.0 jmespath-1.0.1 parameterized-0.9.0 s3transfer-0.10.2 types-requests-2.32.0.20240622\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import pipeline\n",
        "\n",
        "login(token = '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')"
      ],
      "metadata": {
        "id": "9khH96Vv2qLW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7b02565-6092-45e4-8902-181607288a67"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app_v4.py\n",
        "import streamlit as st\n",
        "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema.document import Document\n",
        "from langchain.vectorstores.chroma import Chroma\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import os\n",
        "import shutil\n",
        "from langchain_core.documents import Document\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "from langchain_community.embeddings import CohereEmbeddings\n",
        "\n",
        "my_key_cohere=\"RsdlzOlRrQmvf5bxQrUHDpfbeg7Fvfr7WJwGCec1\"\n",
        "\n",
        "# Set up Streamlit page configuration\n",
        "st.set_page_config(page_title=\"ğŸ¦™ğŸ’¬ Sistem MÃ¼hendisliÄŸi GiriÅŸ Chatbot\")\n",
        "\n",
        "# Constants\n",
        "DATA_PATH = \"/content/drive/MyDrive/data\"  # PDF dosyalarÄ±nÄ±n bulunduÄŸu dizin\n",
        "CHROMA_PATH = \"chroma\"  # Chroma veritabanÄ±nÄ±n saklanacaÄŸÄ± dizin\n",
        "\n",
        "# Initialize Embeddings\n",
        "def initialize_embeddings():\n",
        "    embeddings = CohereEmbeddings(cohere_api_key=my_key_cohere,model=\"embed-multilingual-v3.0\")\n",
        "    return embeddings\n",
        "\n",
        "embeddings = initialize_embeddings()\n",
        "\n",
        "# Load and Split Documents\n",
        "def load_and_split_documents(data_path, embeddings):\n",
        "    document_loader = PyPDFDirectoryLoader(data_path)\n",
        "    raw_documents = document_loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=800,\n",
        "        chunk_overlap=80,\n",
        "        length_function=len\n",
        "    )\n",
        "    return text_splitter.split_documents(raw_documents)\n",
        "\n",
        "# Create Custom Documents\n",
        "def create_custom_documents(splitted_documents):\n",
        "    custom_documents = []\n",
        "    for i, raw_doc in enumerate(splitted_documents):\n",
        "        new_doc = Document(\n",
        "                page_content=raw_doc.page_content,\n",
        "                metadata = {\n",
        "                    \"source\": raw_doc.metadata.get(\"source\", \"Unknown Source\"),\n",
        "                    \"title\": raw_doc.metadata.get(\"title\", \"No Title\"),\n",
        "                    \"description\": raw_doc.metadata.get(\"description\", \"No Description\"),\n",
        "                    \"language\": raw_doc.metadata.get(\"language\", \"Unknown Language\"),\n",
        "                    \"doc_id\": i\n",
        "                }\n",
        "        )\n",
        "        custom_documents.append(new_doc)\n",
        "    return custom_documents\n",
        "\n",
        "# Initialize Vectorstore and Retriever\n",
        "def initialize_vectorstore(custom_documents, embeddings, persist_directory):\n",
        "    try:\n",
        "        # Ã–nce belgeleri oluÅŸturun\n",
        "        custom_documents = create_custom_documents(custom_documents)\n",
        "\n",
        "        # CohereEmbeddings boyutunu belirleyin (Ã¶r. 1024 varsayalÄ±m)\n",
        "        embeddings_dimension = 384\n",
        "\n",
        "        # Chroma'ya belgeleri ve embeddings'i geÃ§irin\n",
        "        vectorstore = Chroma.from_documents(\n",
        "            custom_documents,\n",
        "            embeddings,\n",
        "            persist_directory=persist_directory,\n",
        "            embeddings_dimension=embeddings_dimension\n",
        "        )\n",
        "\n",
        "        return vectorstore.as_retriever(\n",
        "            search_type=\"mmr\",\n",
        "            search_kwargs={'k': 4, 'lambda_mult': 0.50}\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing vectorstore: {e}\")\n",
        "        raise e\n",
        "\n",
        "\n",
        "# Retrieve Relevant Documents\n",
        "def retrieve_relevant_documents(retriever, prompt):\n",
        "    return retriever.get_relevant_documents(prompt)\n",
        "\n",
        "# Generate Final Prompt\n",
        "def generate_final_prompt(prompt, context_data, chat_history, relevant_documents):\n",
        "    history_prompt = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in chat_history])\n",
        "    metadata_info = \"\\n\".join([f\"Belge {doc.metadata['doc_id']} - BaÅŸlÄ±k: {doc.metadata['title']}, Kaynak: {doc.metadata['source']}\" for doc in relevant_documents])\n",
        "    return f\"\"\"\n",
        "    Ã–nceki konuÅŸmalarÄ±mÄ±z:\n",
        "    {history_prompt}\n",
        "\n",
        "    ÅÃ¶yle bir sorum var: {prompt}\n",
        "    Bu soruyu yanÄ±tlamak iÃ§in elimizde ÅŸu bilgiler var: {context_data}\n",
        "    Bu sorunun yanÄ±tÄ±nÄ± vermek iÃ§in yalnÄ±zca sana burada verdiÄŸim eldeki bilgileri kullan. BunlarÄ±n dÄ±ÅŸÄ±na asla Ã§Ä±kma. Sorulara TÃ¼rkÃ§e cevap vereceksin.\n",
        "    \"\"\", metadata_info\n",
        "\n",
        "# RAG with Multiple PDFs\n",
        "def rag_with_multiple_pdfs(prompt, chat_history):\n",
        "    splitted_documents = load_and_split_documents(DATA_PATH, embeddings)\n",
        "    custom_documents = create_custom_documents(splitted_documents)\n",
        "    retriever = initialize_vectorstore(custom_documents, embeddings, CHROMA_PATH)\n",
        "    relevant_documents = retrieve_relevant_documents(retriever, prompt)\n",
        "\n",
        "    context_data = \" \".join([doc.page_content for doc in relevant_documents])\n",
        "    final_prompt, metadata_info = generate_final_prompt(prompt, context_data, chat_history, relevant_documents)\n",
        "    return final_prompt, metadata_info\n",
        "\n",
        "# Initialize Model\n",
        "def initialize_model():\n",
        "    model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb_config\n",
        "    )\n",
        "    return tokenizer, model\n",
        "\n",
        "tokenizer, model = initialize_model()\n",
        "\n",
        "# Generate LLaMA3 Response\n",
        "def generate_llama3_response(prompt_input, tokenizer, model):\n",
        "    SYS_PROMPT = \"\"\"You are an assistant for answering questions.\n",
        "    You are given the extracted parts of a long document and a question. Provide a conversational answer.\n",
        "    If you don't know the answer, just say \"I do not know.\" Don't make up an answer. You have to answer Turkish.\"\"\"\n",
        "\n",
        "    messages = [{\"role\":\"system\",\"content\":SYS_PROMPT},{\"role\":\"user\",\"content\":prompt_input}]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=1024,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "# Streamlit App Setup\n",
        "def setup_streamlit():\n",
        "    with st.sidebar:\n",
        "        st.title('ğŸ¦™ğŸ’¬ Sistem MÃ¼hendisliÄŸi GiriÅŸ')\n",
        "        st.subheader('Models and parameters')\n",
        "        temperature = st.sidebar.slider('temperature', min_value=0.01, max_value=5.0, value=0.1, step=0.01)\n",
        "        top_p = st.sidebar.slider('top_p', min_value=0.01, max_value=1.0, value=0.9, step=0.01)\n",
        "        max_length = st.sidebar.slider('max_length', min_value=32, max_value=128, value=120, step=8)\n",
        "        st.markdown('ğŸ“– Learn how to build this app in this [blog](https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/)!')\n",
        "\n",
        "    if \"messages\" not in st.session_state.keys():\n",
        "        st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.write(message[\"content\"])\n",
        "\n",
        "def clear_chat_history():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "st.sidebar.button('Clear Chat History', on_click=clear_chat_history)\n",
        "\n",
        "# Main Function\n",
        "def main():\n",
        "    setup_streamlit()\n",
        "\n",
        "    if prompt := st.chat_input(\"MesajÄ±nÄ±zÄ± Giriniz:\"):\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.write(prompt)\n",
        "\n",
        "        if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
        "            with st.chat_message(\"assistant\"):\n",
        "                with st.spinner(\"Thinking...\"):\n",
        "                    try:\n",
        "                        prompt_input, metadata_info = rag_with_multiple_pdfs(prompt, st.session_state.messages)\n",
        "                        response = generate_llama3_response(prompt_input, tokenizer, model)\n",
        "                        placeholder = st.empty()\n",
        "                        full_response = response\n",
        "                        placeholder.markdown(full_response)\n",
        "                        message = {\"role\": \"assistant\", \"content\": f\"{full_response}\\n\\nMetadata Bilgileri:\\n{metadata_info}\"}\n",
        "                        st.session_state.messages.append(message)\n",
        "                    except Exception as e:\n",
        "                        st.error(f\"An error occurred: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "fy5JU_MNsJef",
        "outputId": "8d561e0e-6e02-4089-87c4-5b9320d3d009",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app_v4.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!streamlit run /content/app_v4.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "5nrpXTICsMC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app_v4.py\n",
        "import streamlit as st\n",
        "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema.document import Document\n",
        "from langchain.vectorstores.chroma import Chroma\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import os\n",
        "import shutil\n",
        "from langchain_core.documents import Document\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "from langchain_community.embeddings import CohereEmbeddings\n",
        "\n",
        "my_key_cohere=\"RsdlzOlRrQmvf5bxQrUHDpfbeg7Fvfr7WJwGCec1\"\n",
        "\n",
        "# Set up Streamlit page configuration\n",
        "st.set_page_config(page_title=\"ğŸ¦™ğŸ’¬ Sistem MÃ¼hendisliÄŸi GiriÅŸ Chatbot\")\n",
        "\n",
        "# Constants\n",
        "DATA_PATH = \"/content/drive/MyDrive/data\"  # PDF dosyalarÄ±nÄ±n bulunduÄŸu dizin\n",
        "CHROMA_PATH = \"chroma\"  # Chroma veritabanÄ±nÄ±n saklanacaÄŸÄ± dizin\n",
        "\n",
        "# Initialize Embeddings\n",
        "def initialize_embeddings():\n",
        "    embeddings = CohereEmbeddings(cohere_api_key=my_key_cohere, model=\"embed-multilingual-v3.0\")\n",
        "    return embeddings\n",
        "\n",
        "embeddings = initialize_embeddings()\n",
        "\n",
        "# Load and Split Documents\n",
        "def load_and_split_documents(data_path, embeddings):\n",
        "    document_loader = PyPDFDirectoryLoader(data_path)\n",
        "    raw_documents = document_loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=800,\n",
        "        chunk_overlap=80,\n",
        "        length_function=len\n",
        "    )\n",
        "    return text_splitter.split_documents(raw_documents)\n",
        "\n",
        "# Create Custom Documents\n",
        "def create_custom_documents(splitted_documents):\n",
        "    custom_documents = []\n",
        "    for i, raw_doc in enumerate(splitted_documents):\n",
        "        new_doc = Document(\n",
        "                page_content=raw_doc.page_content,\n",
        "                metadata={\n",
        "                    \"source\": raw_doc.metadata.get(\"source\", \"Unknown Source\"),\n",
        "                    \"title\": raw_doc.metadata.get(\"title\", \"No Title\"),\n",
        "                    \"description\": raw_doc.metadata.get(\"description\", \"No Description\"),\n",
        "                    \"language\": raw_doc.metadata.get(\"language\", \"Unknown Language\"),\n",
        "                    \"doc_id\": i\n",
        "                }\n",
        "        )\n",
        "        custom_documents.append(new_doc)\n",
        "    return custom_documents\n",
        "\n",
        "# Initialize Vectorstore and Retriever\n",
        "def initialize_vectorstore(custom_documents, embeddings, persist_directory):\n",
        "    try:\n",
        "        # Chroma'ya belgeleri ve embeddings'i geÃ§irin\n",
        "        vectorstore = Chroma.from_documents(\n",
        "            custom_documents,\n",
        "            embeddings,\n",
        "            persist_directory=persist_directory\n",
        "        )\n",
        "\n",
        "        return vectorstore.as_retriever(\n",
        "            search_type=\"mmr\",\n",
        "            search_kwargs={'k': 4, 'lambda_mult': 0.50}\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing vectorstore: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Retrieve Relevant Documents\n",
        "def retrieve_relevant_documents(retriever, prompt):\n",
        "    return retriever.get_relevant_documents(prompt)\n",
        "\n",
        "# Generate Final Prompt\n",
        "def generate_final_prompt(prompt, context_data, chat_history, relevant_documents):\n",
        "    history_prompt = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in chat_history])\n",
        "    metadata_info = \"\\n\".join([f\"Belge {doc.metadata['doc_id']} - BaÅŸlÄ±k: {doc.metadata['title']}, Kaynak: {doc.metadata['source']}\" for doc in relevant_documents])\n",
        "    return f\"\"\"\n",
        "    Ã–nceki konuÅŸmalarÄ±mÄ±z:\n",
        "    {history_prompt}\n",
        "\n",
        "    ÅÃ¶yle bir sorum var: {prompt}\n",
        "    Bu soruyu yanÄ±tlamak iÃ§in elimizde ÅŸu bilgiler var: {context_data}\n",
        "    Bu sorunun yanÄ±tÄ±nÄ± vermek iÃ§in yalnÄ±zca sana burada verdiÄŸim eldeki bilgileri kullan. BunlarÄ±n dÄ±ÅŸÄ±na asla Ã§Ä±kma. Sorulara TÃ¼rkÃ§e cevap vereceksin.\n",
        "    \"\"\", metadata_info\n",
        "\n",
        "# RAG with Multiple PDFs\n",
        "def rag_with_multiple_pdfs(prompt, chat_history):\n",
        "    splitted_documents = load_and_split_documents(DATA_PATH, embeddings)\n",
        "    custom_documents = create_custom_documents(splitted_documents)\n",
        "    retriever = initialize_vectorstore(custom_documents, embeddings, CHROMA_PATH)\n",
        "    relevant_documents = retrieve_relevant_documents(retriever, prompt)\n",
        "\n",
        "    context_data = \" \".join([doc.page_content for doc in relevant_documents])\n",
        "    final_prompt, metadata_info = generate_final_prompt(prompt, context_data, chat_history, relevant_documents)\n",
        "    return final_prompt, metadata_info\n",
        "\n",
        "# Initialize Model\n",
        "def initialize_model():\n",
        "    model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb_config\n",
        "    )\n",
        "    return tokenizer, model\n",
        "\n",
        "tokenizer, model = initialize_model()\n",
        "\n",
        "# Generate LLaMA3 Response\n",
        "def generate_llama3_response(prompt_input, tokenizer, model):\n",
        "    SYS_PROMPT = \"\"\"You are an assistant for answering questions.\n",
        "    You are given the extracted parts of a long document and a question. Provide a conversational answer.\n",
        "    If you don't know the answer, just say \"I do not know.\" Don't make up an answer. You have to answer Turkish.\"\"\"\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": SYS_PROMPT}, {\"role\": \"user\", \"content\": prompt_input}]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=1024,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "# Streamlit App Setup\n",
        "def setup_streamlit():\n",
        "    with st.sidebar:\n",
        "        st.title('ğŸ¦™ğŸ’¬ Sistem MÃ¼hendisliÄŸi GiriÅŸ')\n",
        "        st.subheader('Models and parameters')\n",
        "        temperature = st.sidebar.slider('temperature', min_value=0.01, max_value=5.0, value=0.1, step=0.01)\n",
        "        top_p = st.sidebar.slider('top_p', min_value=0.01, max_value=1.0, value=0.9, step=0.01)\n",
        "        max_length = st.sidebar.slider('max_length', min_value=32, max_value=128, value=120, step=8)\n",
        "        st.markdown('ğŸ“– Learn how to build this app in this [blog](https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/)!')\n",
        "\n",
        "    if \"messages\" not in st.session_state.keys():\n",
        "        st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.write(message[\"content\"])\n",
        "\n",
        "def clear_chat_history():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "st.sidebar.button('Clear Chat History', on_click=clear_chat_history)\n",
        "\n",
        "# Main Function\n",
        "def main():\n",
        "    setup_streamlit()\n",
        "\n",
        "    if prompt := st.chat_input(\"MesajÄ±nÄ±zÄ± Giriniz:\"):\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.write(prompt)\n",
        "\n",
        "        if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
        "            with st.chat_message(\"assistant\"):\n",
        "                with st.spinner(\"Thinking...\"):\n",
        "                    try:\n",
        "                        prompt_input, metadata_info = rag_with_multiple_pdfs(prompt, st.session_state.messages)\n",
        "                        response = generate_llama3_response(prompt_input, tokenizer, model)\n",
        "                        placeholder = st.empty()\n",
        "                        full_response = response\n",
        "                        placeholder.markdown(full_response)\n",
        "                        message = {\"role\": \"assistant\", \"content\": f\"{full_response}\\n\\nMetadata Bilgileri:\\n{metadata_info}\"}\n",
        "                        st.session_state.messages.append(message)\n",
        "                    except Exception as e:\n",
        "                        st.error(f\"An error occurred: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfFT88sGEWP8",
        "outputId": "cf6910d2-85e3-4f58-ed08-032e0c80136d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app_v4.py\n"
          ]
        }
      ]
    }
  ]
}