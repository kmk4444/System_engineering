{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "mount_file_id": "https://github.com/kmk4444/System_engineering/blob/main/System_eng_Part4.ipynb",
      "authorship_tag": "ABX9TyPE8NcDoqlHtBNzEb4HWFNK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmk4444/System_engineering/blob/main/System_eng_Part4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain openai pypdf chroma streamlit langchain_openai langchain_community langchain transformers bitsandbytes accelerate torch faiss-gpu faiss-cpu langchain_chroma langchain_experimental"
      ],
      "metadata": {
        "id": "pRhoGD70xtE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence-transformers"
      ],
      "metadata": {
        "id": "R9vTyjDdt79O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install cohere"
      ],
      "metadata": {
        "id": "w74Apoui5vaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import pipeline\n",
        "\n",
        "login(token = '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')"
      ],
      "metadata": {
        "id": "9khH96Vv2qLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app_v3.py\n",
        "import streamlit as st\n",
        "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema.document import Document\n",
        "from langchain.vectorstores.chroma import Chroma\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import os\n",
        "import shutil\n",
        "from langchain_core.documents import Document\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "from langchain_community.embeddings import CohereEmbeddings\n",
        "\n",
        "my_key_cohere=\"RsdlzOlRrQmvf5bxQrUHDpfbeg7Fvfr7WJwGCec1\"\n",
        "\n",
        "# Set up Streamlit page configuration\n",
        "st.set_page_config(page_title=\"ðŸ¦™ðŸ’¬ Sistem MÃ¼hendisliÄŸi GiriÅŸ Chatbot\")\n",
        "\n",
        "# Constants\n",
        "DATA_PATH = \"/content/drive/MyDrive/data\"  # PDF dosyalarÄ±nÄ±n bulunduÄŸu dizin\n",
        "CHROMA_PATH = \"chroma\"  # Chroma veritabanÄ±nÄ±n saklanacaÄŸÄ± dizin\n",
        "\n",
        "# Initialize Embeddings\n",
        "def initialize_embeddings():\n",
        "    embeddings = CohereEmbeddings(cohere_api_key=my_key_cohere,model=\"embed-multilingual-v3.0\")\n",
        "    return embeddings\n",
        "\n",
        "embeddings = initialize_embeddings()\n",
        "\n",
        "# Load and Split Documents\n",
        "def load_and_split_documents(data_path, embeddings):\n",
        "    document_loader = PyPDFDirectoryLoader(data_path)\n",
        "    raw_documents = document_loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=800,\n",
        "        chunk_overlap=80,\n",
        "        length_function=len\n",
        "    )\n",
        "    return text_splitter.split_documents(raw_documents)\n",
        "\n",
        "# Create Custom Documents\n",
        "def create_custom_documents(splitted_documents):\n",
        "    custom_documents = []\n",
        "    for i, raw_doc in enumerate(splitted_documents):\n",
        "        new_doc = Document(\n",
        "                page_content=raw_doc.page_content,\n",
        "                metadata = {\n",
        "                    \"source\": raw_doc.metadata.get(\"source\", \"Unknown Source\"),\n",
        "                    \"title\": raw_doc.metadata.get(\"title\", \"No Title\"),\n",
        "                    \"description\": raw_doc.metadata.get(\"description\", \"No Description\"),\n",
        "                    \"language\": raw_doc.metadata.get(\"language\", \"Unknown Language\"),\n",
        "                    \"doc_id\": i\n",
        "                }\n",
        "        )\n",
        "        custom_documents.append(new_doc)\n",
        "    return custom_documents\n",
        "\n",
        "# Initialize Vectorstore and Retriever\n",
        "def initialize_vectorstore(custom_documents, embeddings, persist_directory):\n",
        "    try:\n",
        "        # Ã–nce belgeleri oluÅŸturun\n",
        "        custom_documents = create_custom_documents(custom_documents)\n",
        "\n",
        "        # CohereEmbeddings boyutunu belirleyin (Ã¶r. 1024 varsayalÄ±m)\n",
        "        embeddings_dimension = 384\n",
        "\n",
        "        # Chroma'ya belgeleri ve embeddings'i geÃ§irin\n",
        "        vectorstore = Chroma.from_documents(\n",
        "            custom_documents,\n",
        "            embeddings,\n",
        "            persist_directory=persist_directory,\n",
        "            embeddings_dimension=embeddings_dimension\n",
        "        )\n",
        "\n",
        "        return vectorstore.as_retriever(\n",
        "            search_type=\"mmr\",\n",
        "            search_kwargs={'k': 4, 'lambda_mult': 0.50}\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing vectorstore: {e}\")\n",
        "        raise e\n",
        "\n",
        "\n",
        "# Retrieve Relevant Documents\n",
        "def retrieve_relevant_documents(retriever, prompt):\n",
        "    return retriever.get_relevant_documents(prompt)\n",
        "\n",
        "# Generate Final Prompt\n",
        "def generate_final_prompt(prompt, context_data, chat_history, relevant_documents):\n",
        "    history_prompt = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in chat_history])\n",
        "    metadata_info = \"\\n\".join([f\"Belge {doc.metadata['doc_id']} - BaÅŸlÄ±k: {doc.metadata['title']}, Kaynak: {doc.metadata['source']}\" for doc in relevant_documents])\n",
        "    return f\"\"\"\n",
        "    Ã–nceki konuÅŸmalarÄ±mÄ±z:\n",
        "    {history_prompt}\n",
        "\n",
        "    ÅžÃ¶yle bir sorum var: {prompt}\n",
        "    Bu soruyu yanÄ±tlamak iÃ§in elimizde ÅŸu bilgiler var: {context_data}\n",
        "    Bu sorunun yanÄ±tÄ±nÄ± vermek iÃ§in yalnÄ±zca sana burada verdiÄŸim eldeki bilgileri kullan. BunlarÄ±n dÄ±ÅŸÄ±na asla Ã§Ä±kma. Sorulara TÃ¼rkÃ§e cevap vereceksin.\n",
        "    \"\"\", metadata_info\n",
        "\n",
        "# RAG with Multiple PDFs\n",
        "def rag_with_multiple_pdfs(prompt, chat_history):\n",
        "    splitted_documents = load_and_split_documents(DATA_PATH, embeddings)\n",
        "    custom_documents = create_custom_documents(splitted_documents)\n",
        "    retriever = initialize_vectorstore(custom_documents, embeddings, CHROMA_PATH)\n",
        "    relevant_documents = retrieve_relevant_documents(retriever, prompt)\n",
        "\n",
        "    context_data = \" \".join([doc.page_content for doc in relevant_documents])\n",
        "    final_prompt, metadata_info = generate_final_prompt(prompt, context_data, chat_history, relevant_documents)\n",
        "    return final_prompt, metadata_info\n",
        "\n",
        "# Initialize Model\n",
        "def initialize_model():\n",
        "    model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb_config\n",
        "    )\n",
        "    return tokenizer, model\n",
        "\n",
        "tokenizer, model = initialize_model()\n",
        "\n",
        "# Generate LLaMA3 Response\n",
        "def generate_llama3_response(prompt_input, tokenizer, model):\n",
        "    SYS_PROMPT = \"\"\"You are an assistant for answering questions.\n",
        "    You are given the extracted parts of a long document and a question. Provide a conversational answer.\n",
        "    If you don't know the answer, just say \"I do not know.\" Don't make up an answer. You have to answer Turkish.\"\"\"\n",
        "\n",
        "    messages = [{\"role\":\"system\",\"content\":SYS_PROMPT},{\"role\":\"user\",\"content\":prompt_input}]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=1024,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "# Streamlit App Setup\n",
        "def setup_streamlit():\n",
        "    with st.sidebar:\n",
        "        st.title('ðŸ¦™ðŸ’¬ Sistem MÃ¼hendisliÄŸi GiriÅŸ')\n",
        "        st.subheader('Models and parameters')\n",
        "        temperature = st.sidebar.slider('temperature', min_value=0.01, max_value=5.0, value=0.1, step=0.01)\n",
        "        top_p = st.sidebar.slider('top_p', min_value=0.01, max_value=1.0, value=0.9, step=0.01)\n",
        "        max_length = st.sidebar.slider('max_length', min_value=32, max_value=128, value=120, step=8)\n",
        "        st.markdown('ðŸ“– Learn how to build this app in this [blog](https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/)!')\n",
        "\n",
        "    if \"messages\" not in st.session_state.keys():\n",
        "        st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.write(message[\"content\"])\n",
        "\n",
        "def clear_chat_history():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "st.sidebar.button('Clear Chat History', on_click=clear_chat_history)\n",
        "\n",
        "# Main Function\n",
        "def main():\n",
        "    setup_streamlit()\n",
        "\n",
        "    if prompt := st.chat_input(\"MesajÄ±nÄ±zÄ± Giriniz:\"):\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.write(prompt)\n",
        "\n",
        "        if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
        "            with st.chat_message(\"assistant\"):\n",
        "                with st.spinner(\"Thinking...\"):\n",
        "                    try:\n",
        "                        prompt_input, metadata_info = rag_with_multiple_pdfs(prompt, st.session_state.messages)\n",
        "                        response = generate_llama3_response(prompt_input, tokenizer, model)\n",
        "                        placeholder = st.empty()\n",
        "                        full_response = response\n",
        "                        placeholder.markdown(full_response)\n",
        "                        message = {\"role\": \"assistant\", \"content\": f\"{full_response}\\n\\nMetadata Bilgileri:\\n{metadata_info}\"}\n",
        "                        st.session_state.messages.append(message)\n",
        "                    except Exception as e:\n",
        "                        st.error(f\"An error occurred: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "fy5JU_MNsJef",
        "outputId": "212a548c-d10f-4e54-c159-1370bbb05bef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app_v3.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!streamlit run /content/app_v3.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "5nrpXTICsMC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app_v4.py\n",
        "import streamlit as st\n",
        "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema.document import Document\n",
        "from langchain.vectorstores.chroma import Chroma\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import os\n",
        "import shutil\n",
        "from langchain_core.documents import Document\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "from langchain_community.embeddings import CohereEmbeddings\n",
        "\n",
        "\n",
        "\n",
        "my_key_cohere=\"RsdlzOlRrQmvf5bxQrUHDpfbeg7Fvfr7WJwGCec1\"\n",
        "\n",
        "# Set up Streamlit page configuration\n",
        "st.set_page_config(page_title=\"ðŸ¦™ðŸ’¬ Sistem MÃ¼hendisliÄŸi GiriÅŸ Chatbot\")\n",
        "\n",
        "# Constants\n",
        "DATA_PATH = \"/content/drive/MyDrive/data\"  # PDF dosyalarÄ±nÄ±n bulunduÄŸu dizin\n",
        "CHROMA_PATH = \"chroma\"  # Chroma veritabanÄ±nÄ±n saklanacaÄŸÄ± dizin\n",
        "\n",
        "# Initialize Embeddings\n",
        "def initialize_embeddings():\n",
        "    embeddings = CohereEmbeddings(cohere_api_key=my_key_cohere, model=\"embed-multilingual-v3.0\")\n",
        "    return embeddings\n",
        "\n",
        "embeddings = initialize_embeddings()\n",
        "\n",
        "# Load and Split Documents\n",
        "def load_and_split_documents(data_path, embeddings):\n",
        "    document_loader = PyPDFDirectoryLoader(data_path)\n",
        "    raw_documents = document_loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=800,\n",
        "        chunk_overlap=80,\n",
        "        length_function=len\n",
        "    )\n",
        "    return text_splitter.split_documents(raw_documents)\n",
        "\n",
        "# Create Custom Documents\n",
        "def create_custom_documents(splitted_documents):\n",
        "    custom_documents = []\n",
        "    for i, raw_doc in enumerate(splitted_documents):\n",
        "        new_doc = Document(\n",
        "                page_content=raw_doc.page_content,\n",
        "                metadata={\n",
        "                    \"source\": raw_doc.metadata.get(\"source\", \"Unknown Source\"),\n",
        "                    \"title\": raw_doc.metadata.get(\"title\", \"No Title\"),\n",
        "                    \"description\": raw_doc.metadata.get(\"description\", \"No Description\"),\n",
        "                    \"language\": raw_doc.metadata.get(\"language\", \"Unknown Language\"),\n",
        "                    \"doc_id\": i\n",
        "                }\n",
        "        )\n",
        "        custom_documents.append(new_doc)\n",
        "    return custom_documents\n",
        "\n",
        "# Initialize Vectorstore and Retriever\n",
        "def initialize_vectorstore(custom_documents, embeddings, persist_directory):\n",
        "    try:\n",
        "        # Chroma'ya belgeleri ve embeddings'i geÃ§irin\n",
        "        vectorstore = Chroma.from_documents(\n",
        "            custom_documents,\n",
        "            embeddings,\n",
        "            persist_directory=persist_directory\n",
        "        )\n",
        "\n",
        "        return vectorstore.as_retriever(\n",
        "            search_type=\"mmr\",\n",
        "            search_kwargs={'k': 4, 'lambda_mult': 0.50}\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing vectorstore: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Retrieve Relevant Documents\n",
        "def retrieve_relevant_documents(retriever, prompt):\n",
        "    return retriever.get_relevant_documents(prompt)\n",
        "\n",
        "# Generate Final Prompt\n",
        "def generate_final_prompt(prompt, context_data, chat_history, relevant_documents):\n",
        "    history_prompt = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in chat_history])\n",
        "    metadata_info = \"\\n\".join([f\"Belge {doc.metadata['doc_id']} - BaÅŸlÄ±k: {doc.metadata['title']}, Kaynak: {doc.metadata['source']}\" for doc in relevant_documents])\n",
        "    return f\"\"\"\n",
        "    Ã–nceki konuÅŸmalarÄ±mÄ±z:\n",
        "    {history_prompt}\n",
        "\n",
        "    ÅžÃ¶yle bir sorum var: {prompt}\n",
        "    Bu soruyu yanÄ±tlamak iÃ§in elimizde ÅŸu bilgiler var: {context_data}\n",
        "    Bu sorunun yanÄ±tÄ±nÄ± vermek iÃ§in yalnÄ±zca sana burada verdiÄŸim eldeki bilgileri kullan. BunlarÄ±n dÄ±ÅŸÄ±na asla Ã§Ä±kma. Sorulara TÃ¼rkÃ§e cevap vereceksin.\n",
        "    \"\"\", metadata_info\n",
        "\n",
        "# RAG with Multiple PDFs\n",
        "def rag_with_multiple_pdfs(prompt, chat_history):\n",
        "    splitted_documents = load_and_split_documents(DATA_PATH, embeddings)\n",
        "    custom_documents = create_custom_documents(splitted_documents)\n",
        "    retriever = initialize_vectorstore(custom_documents, embeddings, CHROMA_PATH)\n",
        "    relevant_documents = retrieve_relevant_documents(retriever, prompt)\n",
        "\n",
        "    context_data = \" \".join([doc.page_content for doc in relevant_documents])\n",
        "    final_prompt, metadata_info = generate_final_prompt(prompt, context_data, chat_history, relevant_documents)\n",
        "    return final_prompt, metadata_info\n",
        "\n",
        "# Initialize Model\n",
        "def initialize_model():\n",
        "    model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb_config\n",
        "    )\n",
        "    return tokenizer, model\n",
        "\n",
        "tokenizer, model = initialize_model()\n",
        "\n",
        "# Generate LLaMA3 Response\n",
        "def generate_llama3_response(prompt_input, tokenizer, model):\n",
        "    SYS_PROMPT = \"\"\"You are an assistant for answering questions.\n",
        "    You are given the extracted parts of a long document and a question. Provide a conversational answer.\n",
        "    If you don't know the answer, just say \"I do not know.\" Don't make up an answer. You have to answer Turkish.\"\"\"\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": SYS_PROMPT}, {\"role\": \"user\", \"content\": prompt_input}]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=1024,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "# Streamlit App Setup\n",
        "def setup_streamlit():\n",
        "    with st.sidebar:\n",
        "        st.title('ðŸ¦™ðŸ’¬ Sistem MÃ¼hendisliÄŸi GiriÅŸ')\n",
        "        st.subheader('Models and parameters')\n",
        "        temperature = st.sidebar.slider('temperature', min_value=0.01, max_value=5.0, value=0.1, step=0.01)\n",
        "        top_p = st.sidebar.slider('top_p', min_value=0.01, max_value=1.0, value=0.9, step=0.01)\n",
        "        max_length = st.sidebar.slider('max_length', min_value=32, max_value=128, value=120, step=8)\n",
        "        st.markdown('ðŸ“– Learn how to build this app in this [blog](https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/)!')\n",
        "\n",
        "    if \"messages\" not in st.session_state.keys():\n",
        "        st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.write(message[\"content\"])\n",
        "\n",
        "def clear_chat_history():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "st.sidebar.button('Clear Chat History', on_click=clear_chat_history)\n",
        "\n",
        "# Main Function\n",
        "def main():\n",
        "    setup_streamlit()\n",
        "\n",
        "    if prompt := st.chat_input(\"MesajÄ±nÄ±zÄ± Giriniz:\"):\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.write(prompt)\n",
        "\n",
        "        if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
        "            with st.chat_message(\"assistant\"):\n",
        "                with st.spinner(\"Thinking...\"):\n",
        "                    try:\n",
        "                        prompt_input, metadata_info = rag_with_multiple_pdfs(prompt, st.session_state.messages)\n",
        "                        response = generate_llama3_response(prompt_input, tokenizer, model)\n",
        "                        placeholder = st.empty()\n",
        "                        full_response = response\n",
        "                        placeholder.markdown(full_response)\n",
        "                        message = {\"role\": \"assistant\", \"content\": f\"{full_response}\\n\\nMetadata Bilgileri:\\n{metadata_info}\"}\n",
        "                        st.session_state.messages.append(message)\n",
        "                    except Exception as e:\n",
        "                        st.error(f\"An error occurred: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfFT88sGEWP8",
        "outputId": "cf6910d2-85e3-4f58-ed08-032e0c80136d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app_v4.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!streamlit run /content/app_v4.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "qGiR46jY2Re_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app_v5.py\n",
        "#deneme\n",
        "import streamlit as st\n",
        "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema.document import Document\n",
        "from langchain.vectorstores.chroma import Chroma\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "import os\n",
        "import shutil\n",
        "from langchain_core.documents import Document\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "from langchain_community.embeddings import CohereEmbeddings\n",
        "\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "templates = {\n",
        "    \"system\": \"You are a professional prompt engineer. Apply the mentioned prompt engineering technique and provide ONLY the improved prompt without any additional commentary or explanations.\",\n",
        "\n",
        "    \"system_multiple\": \"You are a professional prompt engineer. Thoroughly apply EVERY prompt engineering technique listed in the [Prompt Engineering Techniques to Apply] section. Use these techniques to enhance the original prompt provided below, ensuring the enhancement is clear and effective. Provide ONLY the improved version of the prompt without any additional commentary or explanations.\",\n",
        "\n",
        "    \"lang_default\": \"Identify the language of the user's original prompt in the [original] section. You MUST provide the enhanced version of the prompt in the **same language** as the user's original prompt. You'll be penalized if you translate it into another language unless explicitly requested by the user.\",\n",
        "\n",
        "    \"lang_eng\": \"If the original prompt is not in English, first translate it into English before proceeding with the improvement process.\",\n",
        "\n",
        "    \"no_politeness\": \"Improve the original prompt to be more direct and concise, removing any unnecessary polite or indirect phrases such as \\\"please\\\", \\\"if you donâ€™t mind\\\", \\\"thank you\\\", or \\\"I would like to\\\". Get straight to the point of the task.\\n\\nHere are examples demonstrating how to apply this principle:\\n\\n[original]\\nPlease, can you list the benefits of engaging in regular exercise? \\n[improved]\\nList the benefits of regular exercise.\\n\\n[original]\\nWould you mind calculating 2+6? \\n[improved]\\nCalculate 2+6.\\n\\n========\\nBased on this approach, refine the following prompt to ensure it is straightforward and focused on the task:\\n\\n[original]\\n{prompt}\\n[improved]\\n\",\n",
        "\n",
        "    \"no_politeness_simpler\": \"Remove polite phrases to be more direct.\\nExample: \\Change \\\"Would you mind recommending sci-fi movies like The Matrix?\\\" to \\\"Recommend sci-fi movies like The Matrix.\\\"\",\n",
        "\n",
        "    \"affirmative_sentencing\": \"When crafting the prompt, focus on using positive, action-oriented instructions like \\\"do\\\", \\\"use\\\", or \\\"include\\\" while avoiding negative language such as \\\"don't\\\", \\\"avoid\\\" or \\\"never\\\". This encourages the model to concentrate on what it should do rather than what it shouldn't.\\n\\nHere are examples demonstrating how to apply this principle:\\n\\n[original]\\nPrepare three pieces of advice to help high school students achieve high grades. Don't use complex words or vocabulary and don't add details.\\n[improved]\\nPrepare three pieces of advice to help high school students achieve high grades. Use only simple words and make each piece of advice as short as you can.\\n\\n========\\nBased on this approach, refine the following prompt.\\n\\n[original]\\n{prompt}\\n[improved]\\n\",\n",
        "\n",
        "    \"affirmative_sentencing_simpler\": \"Use positive, action-oriented language.\\nExample: Replace \\\"Don't forget to create a table from the text.\\\" with \\\"Remember to  create a table from the text.\\\"\",\n",
        "\n",
        "    \"deeper_understanding\" : \"When you need clarity or a deeper understanding of a topic, idea, or any piece of information, utilize the following prompts: - Explain [insert specific topic] in simple terms. - Explain to me like I'm 11 years old. - Explain to me as if I'm a beginner in [field]. - Explain to me as if I'm an expert in [field]. - -Write the [essay/text/paragraph] using simple English like you're explaining something to a 5-year-old.\",\n",
        "\n",
        "    \"deeper_understanding_simpler\": \"Explain to me as if Iâ€™m a beginner in System Engineering. Example: Change \\\"Explain system architecture.\\\" to \\\"Explain system architecture to beginners.\\\"\",\n",
        "\n",
        "    \"audience_integration\": \"Integrate the intended audience in the original prompt.\\n\\nHere are examples demonstrating how to apply this principle:\\n\\n[original]\\nDescribe the function of the heart in the human body.\\n[improved]\\nDescribe the function of the heart in the human body. Assume your audience is a first-year medical student.\\n\\n[original]\\nWrite the opening paragraph of a fantasy story.\\n[improved]\\nWrite the opening paragraph of a middle-grade fantasy story, capturing the attention of a young reader around 11 years old.\\n\\n========\\nBased on this approach, refine the following prompt.\\n\\n[original]\\n{prompt}\\n[improved]\\n\",\n",
        "\n",
        "    \"audience_integration_simpler\": \"Specify the audience in the prompt.\\nExample: Change \\\"Explain how a film camera works.\\\" to \\\"Explain how a film camera works to beginners.\\\"\",\n",
        "\n",
        "    \"role_assignment\": \"To better guide the model's response, assign a specific role or persona to the model within the original prompt. By defining the model's role, such as an expert in a particular field or a character with specific knowledge, model can generate responses that are more focused, relevant, and aligned with the desired perspective or domain.\\n\\nHere are examples demonstrating how to apply this principle:\\n\\n[original]\\nCan you discuss the effects of deforestation on biodiversity?\\n[improved]\\nAct as an environmental scientist. Discuss the effects of deforestation on biodiversity.\\n\\n========\\nBased on this approach, refine the following prompt:\\n\\n[original]\\n{prompt}\\n[improved]\\n\",\n",
        "\n",
        "    \"role_assignment_simpler\": \"Assign a role to the model.\\nExample: Change \\\"Explain how scents affect people's emotions.\\\" to \\\"As a psychologist, explain how scents affect people's emotions.\\\"\",\n",
        "\n",
        "    \"penalty_warning\": \"Include phrases like \\\"You will be penalized\\\" or similar penalty-related language in the original prompt to stress the importance of adhering to specific requirements or including crucial information. This type of phrasing emphasizes the significance of certain elements and discourages the model from omitting them in its response.\\n\\nHere are examples demonstrating how to apply this principle:\\n\\n[original] \\nExplain the difference between day and night. cover the rotation of the Earth.\\n[improved]\\nExplain the difference between day and night. You will be penalized if you omit to cover the rotation of the Earth.\\n\\n========\\nBased on this approach, refine the following prompt:\\n\\n[original]\\n{prompt}\\n[improved]\\n\",\n",
        "\n",
        "    \"penalty_warning_simpler\": \"Emphasize the importance of details with penalty warnings.\\nExample: Change \\\"Include all steps.\\\" to \\\"Include all steps. You will be penalized for any omissions.\\\"\",\n",
        "\n",
        "    \"imperative_task\": \"Improve the original prompt by identifying the main task or objective and emphasizing it using phrases such as \\\"Your task is\\\" or \\\"you MUST\\\".\\n\\nHere are examples demonstrating how to apply this principle:\\n\\n[original]\\nMention the device used to measure temperature and state the units it measures in. \\n[improved]\\nYour task is to mention the device used to measure temperature. You MUST also state the units it measures in.\\n\\n========\\nBased on this approach, refine the following prompt to ensure it is straightforward and focused on the task:\\n\\n[original]\\n{prompt}\\n[improved]\\n\",\n",
        "\n",
        "    \"imperative_task_simpler\": \"State the task explicitly by adding the phrases like \\\"Your task is\\\" or \\\"you MUST\\\". Example: Change \\\"Utilize Bing Search.\\\" to \\\"You MUST utilize Bing Search.\\\"\",\n",
        "\n",
        "    \"guideline_indicators\": \"Refine the original prompt by clearly stating the requirements that the model must follow to produce the desired content. Include specific keywords, regulations, hints, or instructions that define the expected output format, style, length, or any other relevant constraints. This will guide the model to generate content that aligns with your intended outcome.\\n\\nHere are examples demonstrating how to apply this principle:\\n\\n[original]\\nExplain why we need to sleep.\\n[improved]\\nExplain why we need to sleep.\\nKeywords: rest, energy, body, mind.\\nFormat: A few simple sentences.\\n\\n========\\nBased on this approach, refine the following prompt.\\n\\n[original]\\n{prompt}\\n[improved]\\n\",\n",
        "\n",
        "    \"guideline_indicators_simpler\": \"Refine the original prompt by clearly stating the requirements that the model must follow to produce the desired content. Include specific keywords, regulations, hints, or instructions that define the expected output format, style, length, or any other relevant constraints. This will guide the model to generate content that aligns with your intended outcome.\",\n",
        "\n",
        "    \"task_decomposition\": \"For complex or multi-step tasks, divide the original prompt into a series of simpler, more manageable sub-prompts. This approach allows the model to focus on one part of the task at a time, generating more detailed and coherent responses for each step.\\n\\nHere are examples demonstrating how to apply this principle:\\n\\n[original]\\nCreate a short story about a character who discovers an old, mysterious book that grants them extraordinary powers.\\n[improved]\\n1. Create a short story about a character who discovers an old, mysterious book that grants them extraordinary powers. first: Introduce the protagonist, ordinary life, and setting.\\n2. Describe discovering the book and the character's growing powers.\\n3. present challenges from powers and character's growth.Resolve conflicts, and show the character's reflection on the journey.\\n4. write the short story.\\n\\n========\\nBased on this approach, refine the following prompt to enable its breakdown into a series of simpler, step-by-step tasks:\\n\\n[original]\\n{prompt}\\n[improved]\\n\",\n",
        "\n",
        "    \"task_decomposition_simpler\": \"For complex or multi-step tasks, divide the original prompt into a series of simpler, more manageable sub-prompts. This approach allows the model to focus on one part of the task at a time, generating more detailed and coherent responses for each step.\",\n",
        "\n",
        "    \"fewshot_prompting\": \"Improve the original prompt by adding a couple of relevant examples that demonstrate the kind of answer or information being requested. Incorporate those examples smoothly into the prompt to make the desired response clear.\\n\\nHere are examples demonstrating how to apply this principle:\\n\\n[original]\\nClassify the emotion of the following text as positive, negative, or neutral. \\n\\\"It's a good day to try something fun.\\\" \\n[improved]\\nClassify the emotion of the following text as positive, negative, or neutral. \\nExample1: \\\"This music is awesome.\\\" (Positive) \\nExample2: \\\"I don't like spicy flavors.\\\" (Negative)\\nExample3: \\\"Every flower blooms at a different pace.\\\" (Neutral)\\nQuestion: \\\"It's a good day to try something fun.\\\" \\n\\n========\\nBased on this approach, refine the following prompt.\\n\\n[original]\\n{prompt}\\n[improved]\\n\",\n",
        "\n",
        "    \"fewshot_prompting_simpler\": \"Improve the original prompt by adding a couple of relevant examples that demonstrate the kind of answer or information being requested. Incorporate those examples smoothly into the prompt to make the desired response clear.\",\n",
        "\n",
        "    \"echo_directive\": \"Identify the central theme or topic of the original prompt and choose a key word or phrase that represents it. Strategically repeat this word or phrase multiple times throughout the prompt to emphasize its importance and help the model focus on the main idea.\\n\\nHere are examples demonstrating how to apply this principle:\\n\\n[original]\\nWhat are some unique adaptations seen in desert-dwelling animals, specifically in this subset of animals?\\n[improved]\\nAnimals, both wild and domesticated animals, offer a window into nature What are some unique adaptations seen in desert-dwelling animals, specifically in this subset of animals?\\n\\n========\\nBased on this approach, refine the following prompt:\\n\\n[original]\\n{prompt}\\n[improved]\\n\",\n",
        "\n",
        "    \"echo_directive_simpler\": \"Identify the central theme or topic of the original prompt and choose a key word or phrase that represents it. Strategically repeat this word or phrase multiple times throughout the prompt to emphasize its importance and help the model focus on the main idea.\",\n",
        "\n",
        "    \"delimiters\": \"Incorporate delimiters such as <>, <tag></tag>, or '' in the original prompt to highlight key concepts, tasks, or entities. This helps to visually distinguish important elements and provides clarity to the model about the specific focus of the prompt.\\n\\nHere are examples demonstrating how to apply this principle:\\n\\n[original]\\nCreate a step-by-step guide on how to meditate for beginners, highlighting its benefits for mental well-being.\\n[improved]\\nCreate a step-by-step guide on how to <meditate> for beginners, highlighting its benefits for mental well-being.\\n\\n========\\nBased on this approach, refine the following prompt.\\n\\n[original]\\n{prompt}\\n[improved]\\n\",\n",
        "\n",
        "    \"delimiters_simpler\": \"Incorporate delimiters such as <>, <tag></tag>, or '' in the original prompt to highlight key concepts, tasks, or entities. This helps to visually distinguish important elements and provides clarity to the model about the specific focus of the prompt.\",\n",
        "\n",
        "    \"formatted_prompt\": \"Structure the original prompt by beginning with the '###Instruction###' tag, followed by '###Example###' or '###Question###' tags if applicable. After the tags, provide the relevant content for each section. Utilize line breaks to clearly separate the instructions, examples, questions, context, and input data, ensuring that each part is distinct and easily identifiable.\\n\\nHere are examples demonstrating how to apply this principle:\\n\\n[original]\\nProvide a synonym for a given adjective. Whatâ€™s another word for \\\"happy\\\"?\\n[improved]\\n###Instruction###\\nProvide a synonym for a given adjective.\\n### Question ###\\nWhat's another word for \\\"happy\\\"?\\n\\n========\\nBased on this approach, refine the following prompt.\\n\\n[original]\\n{prompt}\\n[improved]\\n\",\n",
        "\n",
        "    \"formatted_prompt_simpler\": \"Structure the original prompt by beginning with the '###Instruction###' tag, followed by '###Example###' or '###Question###' tags if applicable. After the tags, provide the relevant content for each section. Utilize line breaks to clearly separate the instructions, examples, questions, context, and input data, ensuring that each part is distinct and easily identifiable.\",\n",
        "\n",
        "    \"output_primers\": \"Incorporate output primers into the original prompt by ending it with the beginning of the desired response. This technique guides the model to generate output that follows a specific structure or format.\\n\\nHere are examples demonstrating how to apply this principle:\\n\\n[original]\\nWhat are the primary reasons for the decline in bee populations worldwide and why is this concerning?\\n[improved]\\nWhat are the primary reasons for the decline in bee populations worldwide and why is this concerning? Analysis:\\n\\n========\\nBased on this approach, refine the following prompt.\\n\\n[original]\\n{prompt}\\n[improved]\\n\",\n",
        "\n",
        "    \"output_primers_simpler\": \"Incorporate output primers into the original prompt by ending it with the beginning of the desired response. This technique guides the model to generate output that follows a specific structure or format. Change \\\"Can you make a list of what to pack in my bag for my trip to Seoul?\\\" to \\\"Can you make a list of what to pack in my bag for my trip to Seoul? List items:\\\"\",\n",
        "}\n",
        "\n",
        "my_key_cohere=\"RsdlzOlRrQmvf5bxQrUHDpfbeg7Fvfr7WJwGCec1\"\n",
        "\n",
        "# Set up Streamlit page configuration\n",
        "st.set_page_config(page_title=\"ðŸ¦™ðŸ’¬ Sistem MÃ¼hendisliÄŸi GiriÅŸ Chatbot\")\n",
        "\n",
        "# Constants\n",
        "DATA_PATH = \"/content/drive/MyDrive/data\"  # PDF dosyalarÄ±nÄ±n bulunduÄŸu dizin\n",
        "CHROMA_PATH = \"chroma\"  # Chroma veritabanÄ±nÄ±n saklanacaÄŸÄ± dizin\n",
        "\n",
        "# Initialize Embeddings\n",
        "def initialize_embeddings():\n",
        "    embeddings = CohereEmbeddings(cohere_api_key=my_key_cohere, model=\"embed-multilingual-v3.0\")\n",
        "    return embeddings\n",
        "\n",
        "embeddings = initialize_embeddings()\n",
        "\n",
        "# Load and Split Documents\n",
        "def load_and_split_documents(data_path, embeddings):\n",
        "    document_loader = PyPDFDirectoryLoader(data_path)\n",
        "    raw_documents = document_loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=800,\n",
        "        chunk_overlap=80,\n",
        "        length_function=len\n",
        "    )\n",
        "    return text_splitter.split_documents(raw_documents)\n",
        "\n",
        "# Create Custom Documents\n",
        "def create_custom_documents(splitted_documents):\n",
        "    custom_documents = []\n",
        "    for i, raw_doc in enumerate(splitted_documents):\n",
        "        new_doc = Document(\n",
        "                page_content=raw_doc.page_content,\n",
        "                metadata={\n",
        "                    \"source\": raw_doc.metadata.get(\"source\", \"Unknown Source\"),\n",
        "                    \"title\": raw_doc.metadata.get(\"title\", \"No Title\"),\n",
        "                    \"description\": raw_doc.metadata.get(\"description\", \"No Description\"),\n",
        "                    \"language\": raw_doc.metadata.get(\"language\", \"Unknown Language\"),\n",
        "                    \"doc_id\": i\n",
        "                }\n",
        "        )\n",
        "        custom_documents.append(new_doc)\n",
        "    return custom_documents\n",
        "\n",
        "# Initialize Vectorstore and Retriever\n",
        "def initialize_vectorstore(custom_documents, embeddings, persist_directory):\n",
        "    try:\n",
        "        # Chroma'ya belgeleri ve embeddings'i geÃ§irin\n",
        "        vectorstore = Chroma.from_documents(\n",
        "            custom_documents,\n",
        "            embeddings,\n",
        "            persist_directory=persist_directory\n",
        "        )\n",
        "\n",
        "        return vectorstore.as_retriever(\n",
        "            search_type=\"mmr\",\n",
        "            search_kwargs={'k': 4, 'lambda_mult': 0.50}\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing vectorstore: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Retrieve Relevant Documents\n",
        "def retrieve_relevant_documents(retriever, prompt):\n",
        "    return retriever.get_relevant_documents(prompt)\n",
        "\n",
        "# Generate Final Prompt\n",
        "def generate_final_prompt(prompt, context_data, chat_history, relevant_documents):\n",
        "    history_prompt = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in chat_history])\n",
        "    metadata_info = \"\\n\".join([f\"Belge {doc.metadata['doc_id']} - BaÅŸlÄ±k: {doc.metadata['title']}, Kaynak: {doc.metadata['source']}\" for doc in relevant_documents])\n",
        "    return  f\"\"\"\n",
        "    ###Instruction###\n",
        "    You are an expert assistant dedicated to providing answers for beginner systems engineers. Your primary role is to deliver clear, concise, and expert-level information in Turkish. Use the provided documents and historical conversation data to answer questions. Do not speculate or go beyond the given data.\n",
        "\n",
        "    ###Previous Conversations###\n",
        "    {history_prompt}\n",
        "\n",
        "    ###Question###\n",
        "    {prompt}\n",
        "\n",
        "    ###Context###\n",
        "    This is the information we have to answer the question: {context_data}\n",
        "    \"\"\", metadata_info\n",
        "\n",
        "# RAG with Multiple PDFs\n",
        "def rag_with_multiple_pdfs(prompt, chat_history):\n",
        "    splitted_documents = load_and_split_documents(DATA_PATH, embeddings)\n",
        "    custom_documents = create_custom_documents(splitted_documents)\n",
        "    retriever = initialize_vectorstore(custom_documents, embeddings, CHROMA_PATH)\n",
        "    relevant_documents = retrieve_relevant_documents(retriever, prompt)\n",
        "\n",
        "    context_data = \" \".join([doc.page_content for doc in relevant_documents])\n",
        "    final_prompt, metadata_info = generate_final_prompt(prompt, context_data, chat_history, relevant_documents)\n",
        "    return final_prompt, metadata_info\n",
        "\n",
        "# Initialize Model\n",
        "def initialize_model():\n",
        "    model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb_config\n",
        "    )\n",
        "    return tokenizer, model\n",
        "\n",
        "tokenizer, model = initialize_model()\n",
        "\n",
        "\n",
        "def generate_prompt_engineering(prompt, tokenizer, model):\n",
        "    system_message = templates[\"system_multiple\"]\n",
        "    system_message += '\\n' + templates[\"lang_eng\"]\n",
        "\n",
        "\n",
        "    skills = [\"deeper_understanding\",\"task_decomposition\",\"fewshot_prompting\"]\n",
        "    integrated_templates = \"[Prompt Engineering Techniques to Apply]\\n\"\n",
        "\n",
        "    for idx, skill in enumerate(skills):\n",
        "        template = templates[f\"{skill}_simpler\"]\n",
        "        integrated_templates += f\"{idx+1}. {skill}: {template}\\n\"\n",
        "    integrated_templates += \"Based on [Prompt engineering techniques to apply], refine the prompt provided below. Ensure that each technique is fully incorporated to achieve a clear and effective improvement:\\n\\n[original]\\n{prompt}\\n[improved]\\n\"\n",
        "\n",
        "    prompt_template = PromptTemplate.from_template(integrated_templates)\n",
        "    formatted_input = prompt_template.format(prompt=prompt)\n",
        "\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": system_message}, {\"role\": \"user\", \"content\": formatted_input}]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=1024,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "\n",
        "# Generate LLaMA3 Response\n",
        "def generate_llama3_response(prompt_input, tokenizer, model):\n",
        "    SYS_PROMPT = \"\"\"You are an expert assistant dedicated to guiding and empowering beginner systems engineers.\n",
        "    Your role is crucial in providing clear, concise, and expert-level information in Turkish.\n",
        "    Utilize the provided documents and historical conversation data to deliver insightful answers.\n",
        "    If unsure, confidently state \"Bilmiyorum\" without speculating.\n",
        "    Your responses should be motivational and tailored to empower beginners in understanding systems engineering principles.\"\"\"\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": SYS_PROMPT}, {\"role\": \"user\", \"content\": prompt_input}]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=1024,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)\n",
        "\n",
        "# Streamlit App Setup\n",
        "def setup_streamlit():\n",
        "    with st.sidebar:\n",
        "        st.title('ðŸ¦™ðŸ’¬ Sistem MÃ¼hendisliÄŸi GiriÅŸ')\n",
        "        st.subheader('Models and parameters')\n",
        "        temperature = st.sidebar.slider('temperature', min_value=0.01, max_value=5.0, value=0.1, step=0.01)\n",
        "        top_p = st.sidebar.slider('top_p', min_value=0.01, max_value=1.0, value=0.9, step=0.01)\n",
        "        max_length = st.sidebar.slider('max_length', min_value=32, max_value=128, value=120, step=8)\n",
        "        st.markdown('ðŸ“– Learn how to build this app in this [blog](https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/)!')\n",
        "\n",
        "    if \"messages\" not in st.session_state.keys():\n",
        "        st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.write(message[\"content\"])\n",
        "\n",
        "def clear_chat_history():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
        "st.sidebar.button('Clear Chat History', on_click=clear_chat_history)\n",
        "\n",
        "# Main Function\n",
        "def main():\n",
        "    setup_streamlit()\n",
        "\n",
        "    if prompt := st.chat_input(\"MesajÄ±nÄ±zÄ± Giriniz:\"):\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "        prompt_v2 = generate_llama3_response(prompt, tokenizer, model)\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.write(prompt_v2)\n",
        "\n",
        "        if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
        "            with st.chat_message(\"assistant\"):\n",
        "                with st.spinner(\"Thinking...\"):\n",
        "                    try:\n",
        "                        prompt_input, metadata_info = rag_with_multiple_pdfs(prompt, st.session_state.messages)\n",
        "                        response = generate_llama3_response(prompt_input, tokenizer, model)\n",
        "                        placeholder = st.empty()\n",
        "                        full_response = response\n",
        "                        placeholder.markdown(full_response)\n",
        "                        message = {\"role\": \"assistant\", \"content\": f\"{full_response}\\n\\nMetadata Bilgileri:\\n{metadata_info}\"}\n",
        "                        st.session_state.messages.append(message)\n",
        "                    except Exception as e:\n",
        "                        st.error(f\"An error occurred: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCxJ3dUyS0e7",
        "outputId": "2e5fbc72-b035-4543-e1e2-cd8f9d113567"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app_v5.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!streamlit run /content/app_v5.py &>/content/logs.txt &\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNydXxQ2S8Wz",
        "outputId": "1af0a7c2-f4ef-42f8-e0f7-8d6df4b6f856"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25h+ localtunnel@2.0.2\n",
            "updated 1 package and audited 36 packages in 0.436s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 2 \u001b[93mmoderate\u001b[0m severity vulnerabilities\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 1.28s\n",
            "your url is: https://flat-comics-boil.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}